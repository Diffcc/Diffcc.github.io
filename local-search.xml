<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>大模型量化技术</title>
    <link href="/Diffcc/Diffcc.github.io/2025/07/12/20250712/"/>
    <url>/Diffcc/Diffcc.github.io/2025/07/12/20250712/</url>
    
    <content type="html"><![CDATA[<p>现有的基于Transformer架构的大模型，参数量规模极大，往往一个普通的模型动不动就几个G占用，对于边缘端设备来说，内存和计算能力都相对较低的情况下，压缩模型，同时尽量保证较高的推理性能尤为重要。</p><p>DeepSeek模型详解：<a href="https://cloud.tencent.com/developer/article/2497217">https://cloud.tencent.com/developer/article/2497217</a></p><p>Qwen3模型优化：<a href="https://www.zhihu.com/question/1914286810902827620/answer/1914361315604031301">https://www.zhihu.com/question/1914286810902827620/answer/1914361315604031301</a></p><span id="more"></span><p>本文主要涉及模型压缩中的量化技术（Quantization），对于其他压缩技术，例如剪枝（Pruning）、知识蒸馏（Knowledge Distillation）、低秩分解（Low-Rank Factorization）等，读者可自行了解。</p><h3 id="量化原理">量化原理</h3><p>常见的数据类型有如下几种（Float32，TF32，FP16，BF16）等，其中后缀数字表明1个符号位 + 指数位 + 尾数位，通俗的理解，指数位代表该数的量级，也就是范围，而尾数位则代表了该数的有效尾数，所以我们在比较Float类型的数字是否相等时，往往取二者的差值小于某个小数阈值，而不同的尾数个数，对于阈值的要求也不一样。</p><p>而模型量化则是将模型的参数从高精度的数据类型（例如Float32）转换为低精度的数据类型（如INT8，FP8），进而减少模型的参数量大小。</p><p><img src="%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F.png" alt="数据格式"></p><h4 id="量化对象">量化对象</h4><p>模型量化的对象包括以下几个方面：</p><ul><li>权重（weight）：weight的量化最为常见，权重的减小可以减少模型的内存占用，权重由于在训练完成之后固定，其数值范围和数值无关，可以离线完成量化，通常比较简单。</li><li>激活（Activation）：activation在整个模型中占据了内存的大头，所以量化激活可以更近一步减少内存占用，同时结合权重量化可以充分利用整数计算获得模型推理性能的提升，但是由于激活输出会随着输入的改变而改变，所以需要统计数据的动态范围，更难量化。</li><li>KV Cache：KV Cache在大模型中会消耗许多内存，因此，量化KV Cache对提高模型长序列生成的吞吐量至关重要。</li><li>梯度（Gradients）：梯度量化主要用于训练场景，量化梯度可以减少浮点数梯度在分布式计算中的通信开销，同时减少反向传播的开销。</li></ul><h4 id="量化形式">量化形式</h4><p>根据量化数表示的原始数据是否均匀，可以将量化方法划分为对称量化和非对称量化，实际网络中的权重和激活往往是不均匀的，理应采用非对称量化减少精度损失，但是由于非对称量化的计算复杂度较高，通常采用线性量化：</p><p>$q = clip(round(r/s) + z, q_{min},q_{max})$</p><p>原始浮点数$r$ 除以量化间隔$s$, 得到缩放后的值$r/s$, 这一步将$r$映射到一个更小的范围，便于后续的整数表示，加上偏置$z$, 调整数据零点的位置，如果$z = 0$, 表示堆成量化，数据的零点居中；反之，则为非对称量化，对称量化可以避免量化算子在推理中计算$z$相关的部分，降低推理时的时间复杂度；非对称量化可以根据实际数据的分布确定最大值和最大值，更加充分利用量化数据信息，使得量化导致的损失更低。$round(·)$表示取整操作，得到最接近的整数值，$clip(·)$表示截断操作，将取整之后的结果限制在$[q_{min}, q_{max}]$范围内，防止超出量化之后的数值表示范围。</p><p><img src="%E5%AF%B9%E7%A7%B0%E9%87%8F%E5%8C%96%E5%92%8C%E9%9D%9E%E5%AF%B9%E7%A7%B0%E9%87%8F%E5%8C%96.png" alt="对称量化和非对称量化"></p><h4 id="量化粒度">量化粒度</h4><p>根据量化参数$s$和$z$的共享范围（即量化粒度），量化方法可以按照如下粒度划分：</p><ul><li>per-tensor(per-layer)量化：每层或每个张量只有一个缩放因子，张量内的所有值都被这个缩放因子量化。</li><li>per-channel量化：卷积核中的每个通道都有不同的缩放因子。</li><li>per-token量化：针对激活而言，针对每一行进行量化。在LLM中，通常和per-channel量化配合使用，如：逐token量化激活，逐channel量化权重。</li><li>per-group/group-wise: 分组量化，以组为单位，分组量化的特殊情况是，将每个密集的矩阵都视为一个组，每个矩阵都有自己的量化范围。而更为普遍的情况是将每个密集矩阵按照输出神经元进行分割，每个连续的N输出神经元为一个组。比如：GPTQ、AWQ中使用128个元素为一组进行量化。有些地方也称为子通道分组（Sub-channel-wise）量化，即将通道划分为更小的子组，以实现更细粒度的精度控制。它的粒度处于 per-tensor 和 per-channel 之间。比如：group_size=128对应一个量化系数，共有 ⌊T/group_size⌋ * ⌊C0/group_size⌋ 个。当 group_size=1 时，逐组量化与逐层量化等价；当 group_size=num_filters（如：dw（Depthwise）将卷积核变成单通道）时，逐组量化与逐通道量化等价。</li></ul><p><img src="%E4%B8%8D%E5%90%8C%E9%87%8F%E5%8C%96%E6%96%B9%E5%BC%8F.jpg" alt="不同量化方式"></p><p>下图中$X$表示激活Tensor，$W$表示权重Tensor。</p><p><img src="%E9%87%8F%E5%8C%96%E7%B2%92%E5%BA%A6.png" alt="量化粒度"></p><h4 id="静态量化和动态量化">静态量化和动态量化</h4><p><strong>对于激活而言， 量化有动态量化和静态量化之分。</strong></p><p>通常，<strong>对于激活而言</strong>，静态量化是指如果采用具有代表性的校准数据集来为其生成缩放因子和零点，这些参数在模型的整个生命周期中保持不变。静态量化的优点在于推理时的计算效率较高，因为它不需要在运行时动态计算量化参数。然而，由于量化参数是固定的，静态量化可能会引入一些量化误差，从而影响模型的精度</p><p>而动态量化是指在每次前向传递期间计算激活的最小值和最大值，以提供动态的缩放因子以实现高精度。动态量化的优点在于它可以更准确地表示模型的激活值，因为它考虑了运行时的实际数据分布。然而，这种方法的缺点是可能会增加计算开销，因为需要在运行时计算量化参数。动态量化适合于那些对模型精度要求较高的应用场景，尤其是当模型的输入数据分布变化较大时。</p><p>目前，常见的是对激活<strong>使用静态量化</strong>，其中最小/最大范围是在离线校准阶段计算的。但由于LLM中激活范围差异巨大，将<strong>导致准确度显著下降</strong>。</p><h4 id="离线量化和在线量化">离线量化和在线量化</h4><p>离线量化是指模型上线前进行量化并生成缩放因子，而在线量化是指模型运行时进行量化。</p><p>动态与静态量化的区别在于是否使用校准集，而离线与在线量化的区别则是量化的时机不同。简单理解就是说<strong>离线静态量化</strong>是指在模型上线推理前使用校准集生成缩放因子，对权重和激活进行量化。<strong>在线动态量化</strong>是指在模型上线推理时，在每次前向传播过程中实时生成缩放因子，对模型对权重和激活进行量化。 而<strong>离线动态量化</strong>通常是指<strong>对权重在运行前先进行量化，对激活在运行时进行动态量化。</strong></p><h4 id="量化阶段">量化阶段</h4><p>根据应用量化压缩模型的阶段，可以将模型量化分为：</p><ul><li>量化感知训练（Quantization Aware Training, QAT）：在模型训练过程中加入伪量化算子，通过训练时统计输入输出的数据范围可以提升量化后模型的精度，适用于对模型精度要求较高的场景；其量化目标无缝地集成到模型的训练过程中。这种方法使LLM在训练过程中适应低精度表示，增强其处理由量化引起的精度损失的能力。这种适应旨在量化过程之后保持更高性能。</li><li>量化感知微调（Quantization-Aware Fine-tuning，<a href="https://zhida.zhihu.com/search?content_id=251400333&amp;content_type=Article&amp;match_order=1&amp;q=QAF&amp;zhida_source=entity">QAF</a>）：在微调过程中对LLM进行量化。主要目标是确保经过微调的LLM在量化为较低位宽后仍保持性能。通过将量化感知整合到微调中，以在模型压缩和保持性能之间取得平衡。</li><li>训练后量化（Post Training Quantization, PTQ）：在LLM训练完成后对其参数进行量化，只需要少量校准数据，适用于追求高易用性和缺乏训练资源的场景。主要目标是减少LLM的存储和计算复杂性，而无需对LLM架构进行修改或进行重新训练。PTQ的主要优势在于其简单性和高效性。但PTQ可能会在量化过程中引入一定程度的精度损失。</li></ul><h5 id="量化感知训练">量化感知训练</h5><p>量化感知训练是在训练过程中模拟量化，<strong>利用伪量化算子将量化带来的精度损失计入训练误差，使得优化器能在训练过程中尽量减少量化误差</strong>，得到更高的模型精度。量化感知训练的具体流程如下：</p><ul><li><strong>初始化</strong>：设置权重和激活值的范围$q_{min}$和$q_{max}$的初始值；</li><li><strong>构建模拟量化网络</strong>：在需要量化的权重和激活值后插入伪量化算子；</li><li><strong>量化训练</strong>：重复执行以下步骤直到网络收敛，<strong>计算量化网络层的权重和激活值的范围</strong>$q_{min}$和$q_{max}$，并<strong>根据该范围将量化损失带入到前向推理和后向参数更新的过程中</strong>；</li><li><strong>导出量化网络</strong>：获取$q_{min}$和$q_{max}$，并计算量化参数$s$和$z$；<strong>将量化参数代入量化公式中，转换网络中的权重为量化整数值；删除伪量化算子，在量化网络层前后分别插入量化和反量化算子</strong>。</li></ul><p>伪量化算子的数学形式与实际量化公式类似，但增加了梯度传播机制。例如，对权重或激活值 x<em>x</em> 的伪量化过程如下:</p><p>$x_{fake_quant} = clip(round{x/s + z} - z) * s$</p><p>在前向传播时，$round()$和$clip()$模拟量化， 对输入值进行缩放、取整、截断，再反缩放回原始范围。</p><p>在反向传播时，梯度直接传递（STE），忽略$round()$和$clip()$的不可微性：</p><p>$dL/dx = dL/d{x_{fake_quant}}$</p><h5 id="训练后量化">训练后量化</h5><p>训练后量化也可以分成两种，权重量化和全量化。</p><ul><li>权重量化<strong>仅量化模型的权重</strong>以压缩模型的大小，在推理时将权重反量化为原始的float32数据，后续推理流程与普通的float32模型一致。权重量化的好处是不需要校准数据集，不需要实现量化算子，且模型的精度误差较小，由于实际推理使用的仍然是float32算子，所以推理性能不会提高。</li><li>全量化<strong>不仅会量化模型的权重，还会量化模型的激活值</strong>，在模型推理时执行量化算子来加快模型的推理速度。为了量化激活值，需要用户提供一定数量的校准数据集用于统计每一层激活值的分布，并对量化后的算子做校准。<strong>校准数据集可以来自训练数据集或者真实场景的输入数据，需要数量通常非常小</strong>。</li></ul><p>在量化激活值时会<strong>以校准数据集为输入，执行推理流程然后统计每层激活值的数据分布并得到相应的量化参数</strong>。具体的操作流程如下：</p><ul><li>使用直方图统计的方式<strong>得到原始float32数据的统计分布</strong> $P_f$；</li><li>在给定的搜索空间中选取若干个$q_{min}$和$q_{max}$分别<strong>对激活值量化</strong>，得到量化后的数据$Q_q$；</li><li>使用直方图统计得到$Q_q$的<strong>统计分布</strong>;</li><li>计算每个$Q_a$与$P_f$的统计分布差异，并<strong>找到差异性最低的一个</strong>对应的 $q_{min}$和$q_{max}$来计算相应的量化参数，常见的用于度量分布差异的指标包括KL散度(Kullback-Leibler Divergence)、对称KL散度(Symmetric Kullback-Leibler Divergence)和JS散度(Jenson-Shannon Divergence)</li></ul><p>除此之外，由于量化存在固有误差，还需要<strong>校正量化误差</strong>。以矩阵乘为例$a =  \sum_{i=1}^{N} w_ix_i + b$，$w$表示权重，$x$表示激活值，$b$表示偏置。</p><p>首先需要对量化的均值做校正，对float32算子和量化算子输出的每个通道求平均，假设某个通道i的float32算子输出均值为$a_i$，量化算子反量化输出均值为$a_{qi}$，将这个通道两个均值的差$a_i - a_{qi}$加到对应的通道上即可使得最终的输出均值和float32一致。</p><p>另外，还需要<strong>保证量化后的分布和量化前是一致的</strong>，设某个通道权重数据的均值、方差为$E(w_c)$、$||w_c - E(w_c)||$，量化后的均值和方差为$E(\hat{w_c})$、$||\hat{w_c} - E(\hat{w_c})||$，对权重如下校正：</p><p>$\hat{w}_c \leftarrow \zeta_c (\hat{w}_c + u_c) $</p><p>其中：</p><p>$u_c = E(w_c) - E(\hat{w}_c) $</p><p>$\zeta_c = \frac{|w_c - E(w_c)|}{|\hat{w}_c - E(\hat{w}_c)|} $</p><h3 id="量化方法">量化方法</h3>]]></content>
    
    
    <categories>
      
      <category>高性能计算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>CUDA基础</title>
    <link href="/Diffcc/Diffcc.github.io/2025/06/11/20250611/"/>
    <url>/Diffcc/Diffcc.github.io/2025/06/11/20250611/</url>
    
    <content type="html"><![CDATA[<p>可编程处理单元**（Graphic Processor Unit）<strong>日益发展成为高并行、多线程、多核处理的计算利器，2006年11月，NVIDIA推出了</strong>CUDA**——一种通用并行计算平台和编程模型，通过NVIDIA GPU中的并行计算引擎，以比CPU更高效的方式解决复杂的计算问题。</p><p>本文基于CUDA官方文档：<a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html">1. Preface — CUDA C++ Best Practices Guide 12.9 documentation</a></p><span id="more"></span><h2 id="GPU结构">GPU结构</h2><p>CPU和GPU在计算单元上存在以下的不同：</p><table><thead><tr><th style="text-align:left"><strong>组件</strong></th><th style="text-align:left"><strong>CPU</strong></th><th style="text-align:left"><strong>GPU</strong></th></tr></thead><tbody><tr><td style="text-align:left"><strong>控制单元</strong></td><td style="text-align:left">1个强大的控制单元（Control）</td><td style="text-align:left">极简控制逻辑（图中未单独标注）</td></tr><tr><td style="text-align:left"><strong>ALU数量</strong></td><td style="text-align:left">少量复杂ALU（通常4-8核）</td><td style="text-align:left">大量简单ALU（数千个流处理器）</td></tr><tr><td style="text-align:left"><strong>线程能力</strong></td><td style="text-align:left">支持少量并行线程（超线程优化）</td><td style="text-align:left">支持数万并发线程</td></tr></tbody></table><p>CPU通过单个控制单元管理少量的ALU，GPU无集中标注的控制单元，突出多ALU（<strong>Arithmetic Logic Unit，算术逻辑单元</strong>）并行；此外CPU利用大容量多级缓存（L1/L2/L3）实现不同层级的信息缓存，GPU的共享缓存则很少量，两者都连接DRAM（<strong>Dynamic Random-Access Memory，动态随机存取存储器，用于临时存储计算机运行时所需的程序和数据，断电后数据丢失</strong>），但GPU的显存带宽通常是CPU的5-10倍。</p><p><img src="CPU%E5%92%8CGPU%E7%9A%84%E5%AF%B9%E6%AF%94.png" alt="CPU和GPU的对比"></p><p>总的来说，GPU通过<strong>高度并行运算</strong>，使用大量ALU组成流式多处理器（SM）来隐藏内存访问延迟，而不是像CPU一样利用大量的数据缓存和流控制来确保低延迟。</p><p>在CUDA并行编程中，有三个关键抽象为开发者提供高效的并行计算能力：</p><ol><li><strong>线程组层次结构（Hierarchy of Thread Groups）</strong></li><li><strong>共享内存（Shared Memory）</strong></li><li><strong>屏障同步（Barrier Synchronization）</strong></li></ol><p>线程组通过网格（Grid）、线程块（Block）、线程（Thread）的分层结构，利用共享内存<code>__shared__</code>进行快速数据交换，同时通过<code>__syncthreads()</code>确保数据的一致性，即块内所有的线程必须执行到当前点才能继续。</p><p><img src="%E5%85%B3%E9%94%AE%E6%8A%BD%E8%B1%A1%E7%9A%84%E5%85%B3%E7%B3%BB.png" alt="关键抽象的关系"></p><h2 id="CUDA编程模型">CUDA编程模型</h2><p>首先需要明白CUDA 编程中的<em><strong>kernels</strong></em>的概念，<em><strong>kernel</strong></em>可以理解为CUDA扩展的下的函数实现，以C++为例（本文都采用C++作为示例），常规C++函数往往只能按序执行一次，而在<em><strong>kernels</strong></em>下，这些函数调用由N个不同的CUDA线程并行执行N次。</p><p>内核使用 <code>__global__</code>说明符定义，使用<code>&lt;&lt;&lt;block数、每个block的thread数&gt;&gt;&gt;</code>指定总的CUDA线程数（总线程数：numBlocks * threadPerBlock）,执行<em><strong>kernel</strong></em>的每个线程都有一个唯一的线程ID，即<code>threadIdx</code>。</p><p><strong>threadIdx</strong>是一个可以包含三个维度的向量，可以通过下表中的计算公式获得：</p><table><thead><tr><th style="text-align:left"><strong>维度</strong></th><th style="text-align:left"><strong>全局索引公式</strong></th></tr></thead><tbody><tr><td style="text-align:left">1D</td><td style="text-align:left"><code>tid = blockIdx.x * blockDim.x + threadIdx.x</code></td></tr><tr><td style="text-align:left">2D</td><td style="text-align:left"><code>row = blockIdx.y * blockDim.y + threadIdx.y</code> <code>col = blockIdx.x * blockDim.x + threadIdx.x</code></td></tr><tr><td style="text-align:left">3D</td><td style="text-align:left"><code>x = blockIdx.x * blockDim.x + threadIdx.x</code> <code>y = blockIdx.y * blockDim.y + threadIdx.y</code> <code>z = blockIdx.z * blockDim.z + threadIdx.z</code></td></tr></tbody></table><ul><li><code>threadIdx.x</code>：线程在块内的 x 维索引（从 0 开始）</li><li><code>blockIdx.x</code>：块在网格中的 x 维索引</li><li><code>blockDim.x</code>：每个块的 x 维线程数</li><li><code>gridDim.x</code>：网格中 x 维的块数</li></ul><p>对于一维块，线程索引和ID是相同的;对于大小为 （Dx， Dy） 的二维块，索引为 （x， y） 的线程 ID 为 （x + y Dx）;对于大小为 （Dx， Dy， Dz） 的三维块，索引为 （x， y， z） 的线程ID 为 （x + y Dx + z Dx Dy），可通过上表联立计算threadIdx得到。</p><p><img src="Grid_Block_Thread.png" alt="Grid_Block_Thread"></p><p>同样的，通过<code>blockIdx</code>确定block所在的索引（可以理解为行），<code>blockDim</code>确定对应的维度，值得注意的是</p><p><code>&lt;&lt;&lt;…&gt;&gt;&gt;</code>可以被指定为int 或者是 <code>dim3</code>，用于表示二维块或者网格。</p><p>以下看几个具体的编程示例:</p><p><strong>两个一维的数组相加</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">VecAdd</span><span class="hljs-params">(<span class="hljs-type">float</span>* A, <span class="hljs-type">float</span>* B, <span class="hljs-type">float</span>* C)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> i = theadIdx.x; <span class="hljs-comment">//应为blockIdx.x + blockDim.x + threadIdx.x 但是此处block数为1</span><br>    C[i] = A[i] + B[i];<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    ...<br>    VecAdd&lt;&lt;&lt;<span class="hljs-number">1</span>, N&gt;&gt;&gt;(A, B, C);    <br>    ...<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>两个二维数组相加</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatAdd</span><span class="hljs-params">(<span class="hljs-type">float</span> A[N][N], <span class="hljs-type">float</span> B[N][N], <span class="hljs-type">float</span> C[N][N])</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> i = threadIdx.x; <span class="hljs-comment">//此处应为blockIdx.x * blockDim.x + threadIdx.x</span><br>    <span class="hljs-type">int</span> j = threadIdx.y; <span class="hljs-comment">//此处应为blockIdx.y * blockDim.y + threadIdx.y 但是block为1</span><br>    c[i][j] = A[i][j] + B[i][j];<br>&#125;<br><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> numBlocks = <span class="hljs-number">1</span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">threadsPerBlock</span><span class="hljs-params">(N,N)</span></span>;<br>    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B. C);<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>更为一般的情况</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatAdd</span><span class="hljs-params">(<span class="hljs-type">float</span> A[N][N], <span class="hljs-type">float</span> B[N][N], <span class="hljs-type">float</span> C[N][N])</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">int</span> j = blockIdx.y * blockDim.y + threadIdx.y;<br>    <span class="hljs-keyword">if</span>(i &lt; N ** j &lt; N)<br>    c[i][j] = A[i][j] + B[i][j];<br>&#125;<br><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-function">dim3 <span class="hljs-title">threadsPerBlock</span><span class="hljs-params">(<span class="hljs-number">16</span>,<span class="hljs-number">16</span>)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">numBlocks</span><span class="hljs-params">(N / threadsPerBlock.x, N / threadPerBlock.y)</span></span>;<br>    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B. C);<br>&#125;<br></code></pre></td></tr></table></figure><p>在边缘端设备Jetson Orin Nano 8G、Cuda-11.4运行<code>hello.cu</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span><span class="hljs-comment">//它声明了你在 CUDA kernel 函数中会使用的一些 内置变量</span></span><br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">()</span> </span>&#123;<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;hello world&quot;</span>);<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>kernel &lt;&lt;&lt;<span class="hljs-number">1</span>, <span class="hljs-number">1</span>&gt;&gt;&gt; ();<br>cudaError_t err = <span class="hljs-built_in">cudaDeviceSynchronize</span>();<br><span class="hljs-keyword">if</span> (err != cudaSuccess) &#123;<br>std::cerr &lt;&lt; <span class="hljs-string">&quot;CUDA Error: &quot;</span> &lt;&lt; <span class="hljs-built_in">cudaGetErrorString</span>(err) &lt;&lt; std::endl;<br>&#125;<br> <br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvcc hello.cu  -o hello<br><br>./hello<br>hello world<br></code></pre></td></tr></table></figure><h2 id="CUDA内存结构">CUDA内存结构</h2><p>CUDA线程在执行期间会访问到来自多个内存空间的数据，值得注意的是：</p><ul><li>每个线程都有自己独立的<strong>私有内存</strong>（类似于进程中线程的独立栈空间）</li><li>同一个线程块中含有<strong>共享内存（shared memory）</strong>，对该块中的所有线程可见，同时与该块具有相同的生命周期</li><li>除此之外，所有线程都可以访问相同的**全局内存（global memory）**以及只读的内存空间：<strong>常量内存（constant memory）<strong>和</strong>纹理内存（texture memory）</strong></li></ul><p><img src="%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84.png" alt="内存结构"></p><p>当执行一个C++程序时，其中的线性化流程会在<strong>Host（主设备）<strong>上运行，其余的可以并行的程序将会在一个个独立的</strong>Device（从设备）<strong>上运行（例如</strong>kernel</strong>），<em><strong>CUDA runtime</strong></em>用于管理<strong>kernel</strong>可见的全局内存、常量内存以及纹理内存，同时负责device的内存分配和释放，以及<strong>host</strong>与<strong>device</strong>之间的数据传输。</p><p>不同的GPU设备具备不同的计算能力，称之为“SM version”, NVIDIA GPU架构全览如下：</p><table><thead><tr><th style="text-align:left"><strong>架构名称</strong></th><th style="text-align:left"><strong>发布时间</strong></th><th style="text-align:left"><strong>计算能力 (SM)</strong></th><th style="text-align:left"><strong>代表产品</strong></th><th style="text-align:left"><strong>制程工艺</strong></th><th style="text-align:left"><strong>核心创新</strong></th></tr></thead><tbody><tr><td style="text-align:left"><strong>Tesla</strong></td><td style="text-align:left">2006</td><td style="text-align:left">SM 1.x</td><td style="text-align:left">GeForce 8800 GTX</td><td style="text-align:left">90nm</td><td style="text-align:left">首款支持CUDA的架构</td></tr><tr><td style="text-align:left"><strong>Fermi</strong></td><td style="text-align:left">2010</td><td style="text-align:left">SM 2.x</td><td style="text-align:left">GTX 480, Tesla C2050</td><td style="text-align:left">40nm</td><td style="text-align:left">首次支持FP64、ECC显存、L1/L2缓存</td></tr><tr><td style="text-align:left"><strong>Kepler</strong></td><td style="text-align:left">2012</td><td style="text-align:left">SM 3.x</td><td style="text-align:left">GTX 680, Tesla K80</td><td style="text-align:left">28nm</td><td style="text-align:left">动态并行、Hyper-Q、GPU Boost</td></tr><tr><td style="text-align:left"><strong>Maxwell</strong></td><td style="text-align:left">2014</td><td style="text-align:left">SM 5.x</td><td style="text-align:left">GTX 980, Tesla M40</td><td style="text-align:left">28nm</td><td style="text-align:left">SMM设计、能效比提升2倍</td></tr><tr><td style="text-align:left"><strong>Pascal</strong></td><td style="text-align:left">2016</td><td style="text-align:left">SM 6.x</td><td style="text-align:left">GTX 1080 Ti, Tesla P100</td><td style="text-align:left">16nm</td><td style="text-align:left">NVLink 1.0、FP16支持、HBM2显存</td></tr><tr><td style="text-align:left"><strong>Volta</strong></td><td style="text-align:left">2017</td><td style="text-align:left">SM 7.0/7.2</td><td style="text-align:left">Tesla V100, Titan V</td><td style="text-align:left">12nm</td><td style="text-align:left">首代Tensor Core、独立线程调度</td></tr><tr><td style="text-align:left"><strong>Turing</strong></td><td style="text-align:left">2018</td><td style="text-align:left">SM 7.5</td><td style="text-align:left">RTX 2080 Ti, Tesla T4</td><td style="text-align:left">12nm</td><td style="text-align:left">RT Core、第二代Tensor Core（INT8/INT4）、GDDR6</td></tr><tr><td style="text-align:left"><strong>Ampere</strong></td><td style="text-align:left">2020</td><td style="text-align:left">SM 8.x</td><td style="text-align:left">RTX 3090, Tesla A100</td><td style="text-align:left">7nm/8nm</td><td style="text-align:left">第三代Tensor Core（TF32）、MIG、结构化稀疏</td></tr><tr><td style="text-align:left"><strong>Hopper</strong></td><td style="text-align:left">2022</td><td style="text-align:left">SM 9.x</td><td style="text-align:left">H100</td><td style="text-align:left">4nm</td><td style="text-align:left">第四代Tensor Core（FP8）、Transformer引擎、机密计算</td></tr><tr><td style="text-align:left"><strong>Ada Lovelace</strong></td><td style="text-align:left">2022</td><td style="text-align:left">SM 8.9</td><td style="text-align:left">RTX 4090</td><td style="text-align:left">5nm</td><td style="text-align:left">DLSS 3、光追性能翻倍、AV1编码</td></tr><tr><td style="text-align:left"><strong>Blackwell</strong></td><td style="text-align:left">2024*</td><td style="text-align:left">SM 10.x*</td><td style="text-align:left">B100 (预计)</td><td style="text-align:left">3nm*</td><td style="text-align:left">FP6支持、新一代NVLink*（*为预测特性）</td></tr></tbody></table><p>可以通过在代码中嵌入如下命令显示对应的 “SM_version”</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C++">cudaDeviceProp prop;<br><span class="hljs-built_in">cudaGetDeviceProperties</span>(&amp;prop, <span class="hljs-number">0</span>);<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Arch: SM_%d%d\n&quot;</span>, prop.major, prop.minor);<br><br><span class="hljs-comment">//Jetson Orin Nano 8G</span><br>jetson@unbutu:~/mhj/CUDA_ws$ ./hello <br>Arch: SM_87<br></code></pre></td></tr></table></figure><h2 id="CUDA编程接口">CUDA编程接口</h2><p>CUDA C++由<strong>C++ Language Extensions</strong> 和 <strong>CUDA Runtime</strong>组成，其中<strong>C++ Language Extensions</strong>即C++形式下的CUDA编程模型，例如C++形式下的<strong>kernels</strong>，而<strong>CUDA Runtime</strong> 则用于更底层的功能，运行在更低级的API上，例如在<strong>host</strong> 和 device之间传输数据，分配和释放内存，管理多个<strong>device</strong>等，往往使用C 和 C++混合实现。</p><h3 id="NVCC编译">NVCC编译</h3><p>NVCC的编译过程分为两个主要阶段：</p><ul><li><strong>主机代码(Host Code)处理</strong>：处理CPU端的C++代码</li><li><strong>设备代码(Device Code)处理</strong>：处理GPU端的CUDA代码</li></ul><p>主要分为以下几个阶段：</p><ul><li><strong>预处理阶段</strong>：首先调用C/C++预处理器处理<code>.cu</code>文件，处理所有<code>#include</code>、<code>#define</code>等预处理指令，展开宏定义。</li><li><strong>代码分离</strong>：NVCC将代码分离为主机代码和设备代码，主机代码即普通的C++代码，由CPU执行，设备代码，即标记有<code>__global__</code>、<code>__device__</code>等限定符的函数，由GPU执行。</li><li><strong>设备代码编译</strong>：使用NVIDIA的专有编译器前端和后端将设备代码编译为PTX(Parallel Thread eXecution)虚拟汇编代码，或者是直接编译为cubin二进制格式(使用<code>-cubin</code>选项)。</li><li><strong>主机代码编译</strong>：生成修改过的主机代码，其中包含对CUDA Runtime API的调用，传递给系统的C++编译器(如gcc、clang等)</li><li><strong>链接阶段</strong>：将所有对象文件(主机和设备)链接在一起，链接CUDA运行时库，生成最终的可执行文件。</li></ul><p>编译时有如下选项：</p><ul><li><code>--cuda</code>：只生成主机代码，不进行设备代码编译</li><li><code>-ptx</code>：只生成PTX代码</li><li><code>-cubin</code>：生成cubin二进制文件</li><li><code>-fatbin</code>：生成fatbin文件(包含多种架构的二进制)</li><li><code>-keep</code>：保留中间文件用于调试</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvcc -<span class="hljs-built_in">arch</span>=sm_75 -o my_program myprogram.cu<br><br>-<span class="hljs-built_in">arch</span>=sm_75 是 -gencode <span class="hljs-built_in">arch</span>=compute_35,code= \&#x27;compute_35,sm_35\&#x27; 的简写<br>也是 -gencode <span class="hljs-built_in">arch</span>=compute_35,code=sm_35的简写<br></code></pre></td></tr></table></figure><p><strong>Just-In-Time (JIT)</strong> 编译是一种动态编译技术，它在程序运行时(而非之前)将代码编译为机器指令。这种技术结合了解释执行的灵活性和预先编译(AOT, Ahead-Of-Time)的高效性。CUDA中的JIT编译将PTX代码在运行时编译为特定GPU的机器码，允许代码在不同代GPU上运行。</p><p>在CUDA中，JIT编译通常发生在：</p><ol><li>应用程序加载PTX代码时</li><li>驱动程序将PTX即时编译为当前GPU的特定机器码</li><li>缓存编译结果供后续使用</li></ol><h3 id="CUDA-Runtime">CUDA Runtime</h3><p>CUDA Runtime由<code>cudart</code>库实现，cudart库通过<code>cudart.lib</code>和<code>libcudart.a</code>静态链接，或者由<code>cudart.dll</code>或<code>libcudart.so</code>动态链接。</p><p>CUDA Runtime在第一次调用Runtime function时进行初始化，不显式初始化，在初始化期间，Runtime会为系统中的每个设备创建一个CUDA上下文，CUDA 上下文是指GPU 上的虚拟执行环境，包含所有 GPU 状态（内存、模块、流等）的容器，类似于 CPU 编程中的进程概念。</p><p>使用Runtime API隐式的创建上下文</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-built_in">cudaMalloc</span>(&amp;devPtr, size); <span class="hljs-comment">// 第一次调用Runtime API时会自动创建上下文</span><br></code></pre></td></tr></table></figure><h3 id="Device-Memory">Device Memory</h3><p>每个Host和Device都有自己独立的内存，Runtime提供分配释放Host memory和Device memory以及二者之间数据的传输。</p><p>Device memory 可以进一步分配为 linear memory 和 CUDA arrays，CUDA arrays是不透明的内存布局，而linear memory则被分配到一个统一的地址空间中。</p><p>linear memory通过 <code>cudaMalloc()</code>分配，<code>cudaFree()</code>释放，利用<code>cudaMemcpy()</code>进行host memory和device memory之间的数据传输。</p><p>下列命令实现了在Device 空间做运算，最后将device空间的数据拷贝到host空间</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span><span class="hljs-comment">//它声明了你在 CUDA kernel 函数中会使用的一些 内置变量</span></span><br><br><span class="hljs-comment">//device code</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">vecAdd</span><span class="hljs-params">(<span class="hljs-type">float</span>* A, <span class="hljs-type">float</span>* B, <span class="hljs-type">float</span>* C, <span class="hljs-type">int</span> N)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;<br>    <span class="hljs-keyword">if</span>( i &lt; N)<br>        C[i] = A[i] + B[i];<br>&#125; <br><br><span class="hljs-comment">//host code</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> N = <span class="hljs-number">16</span>;<br>    <span class="hljs-type">size_t</span> size = N * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br><br>    <span class="hljs-comment">//host memory </span><br>    <span class="hljs-type">float</span>* h_A = (<span class="hljs-type">float</span>*)<span class="hljs-built_in">malloc</span>(size);<br>    <span class="hljs-type">float</span>* h_B = (<span class="hljs-type">float</span>*)<span class="hljs-built_in">malloc</span>(size);<br>    <span class="hljs-type">float</span>* h_C = (<span class="hljs-type">float</span>*)<span class="hljs-built_in">malloc</span>(size);<br>    <span class="hljs-comment">/******</span><br><span class="hljs-comment">     * 初始化输入矩阵</span><br><span class="hljs-comment">     */</span><br><br>    <span class="hljs-comment">//device memory</span><br>    <span class="hljs-type">float</span>* d_A;<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_A, size);<br>    <span class="hljs-type">float</span>* d_B;<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_B, size);<br>    <span class="hljs-type">float</span>* d_C;<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_C, size);<br><br>    <span class="hljs-comment">//copy data from host to device</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(d_A, h_A, size, cudaMemcpyHostToDevice);<br>    <span class="hljs-built_in">cudaMemcpy</span>(d_B, h_B, size, cudaMemcpyHostToDevice);<br><br>    <span class="hljs-comment">//kernel</span><br>    <span class="hljs-type">int</span> threadsPerBlock = <span class="hljs-number">256</span>;<br>    <span class="hljs-type">int</span> blocksPerGrid = <br>    (N + threadsPerBlock - <span class="hljs-number">1</span>) / threadsPerBlock;<br><br>    vecAdd&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(d_A, d_B, d_C, N);<br><br>    <span class="hljs-comment">//copy data from device to host</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(h_C, d_C, size, cudaMemcpyDeviceToHost);<br><br>    <span class="hljs-comment">//free device memory</span><br>    <span class="hljs-built_in">cudaFree</span>(d_A);<br>    <span class="hljs-built_in">cudaFree</span>(d_B);<br>    <span class="hljs-built_in">cudaFree</span>(d_C);<br><br>    <span class="hljs-comment">/***</span><br><span class="hljs-comment">     * free host memory</span><br><span class="hljs-comment">     */</span><br><br>&#125;<br></code></pre></td></tr></table></figure><p>进一步的，可以通过<code>cudaMallocPitch()</code> 和 <code>cudaMalloc3D()</code> 分配2D和3D的数组（对应的数据复制可以使用<code>cudaMemcpy2D()</code> 和 <code>cudaMemcpy3D()</code>）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaMallocPitch</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">void</span>** devPtr,      <span class="hljs-comment">// 输出参数，返回分配的内存地址指针</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span>* pitch,      <span class="hljs-comment">// 输出参数，返回实际分配的行间距（字节数）</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> width,       <span class="hljs-comment">// 请求分配的每行宽度（字节数）</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> height,      <span class="hljs-comment">// 请求分配的行数</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> elementSize  <span class="hljs-comment">// 可选参数，元素大小（默认为1，CUDA 12.0+新增）</span></span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span>;<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaMalloc3D</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    cudaPitchedPtr* pitchedDevPtr,  <span class="hljs-comment">// 输出参数，返回分配的内存信息</span></span></span><br><span class="hljs-params"><span class="hljs-function">    cudaExtent extent               <span class="hljs-comment">// 请求分配的3D空间范围</span></span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span>;<br><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaExtent</span> &#123;<br>    <span class="hljs-type">size_t</span> width;   <span class="hljs-comment">// 宽度（字节数）</span><br>    <span class="hljs-type">size_t</span> height;  <span class="hljs-comment">// 高度（元素行数）</span><br>    <span class="hljs-type">size_t</span> depth;   <span class="hljs-comment">// 深度（切片数）</span><br>&#125;;<br><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaPitchedPtr</span> &#123;<br>    <span class="hljs-type">void</span>* ptr;      <span class="hljs-comment">// 内存指针</span><br>    <span class="hljs-type">size_t</span> pitch;   <span class="hljs-comment">// 行间距（字节数）</span><br>    <span class="hljs-type">size_t</span> xsize;   <span class="hljs-comment">// 实际分配的宽度</span><br>    <span class="hljs-type">size_t</span> ysize;   <span class="hljs-comment">// 实际分配的高度</span><br>&#125;;<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span></span><br><br><span class="hljs-comment">//device code</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">Mykernel</span><span class="hljs-params">(<span class="hljs-type">float</span>* devPtr, <span class="hljs-type">size_t</span> pitch, <span class="hljs-type">int</span> width, <span class="hljs-type">int</span> height)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> r = <span class="hljs-number">0</span>; r &lt; height; ++r)<br>    &#123;<br>        <span class="hljs-type">float</span>* row = (<span class="hljs-type">float</span>*)((<span class="hljs-type">char</span>*)devPtr + r * pitch);<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> c = <span class="hljs-number">0</span>; c &lt; width; ++c)<br>            <span class="hljs-type">int</span> element = row[c];<br>    &#125;<br>&#125;<br><br><span class="hljs-comment">//host code</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> width = <span class="hljs-number">64</span>, height = <span class="hljs-number">64</span>;<br>    <span class="hljs-type">float</span>* devPtr;<br>    <span class="hljs-type">size_t</span> pitch;<br><br>    <span class="hljs-built_in">cudaMallocPitch</span>(&amp;devPtr, &amp;pitch, width, height);<br><br>    Mykernel&lt;&lt;&lt;<span class="hljs-number">100</span>, <span class="hljs-number">512</span>&gt;&gt;&gt;(devPtr, pitch, width, height);<br>&#125;<br></code></pre></td></tr></table></figure><p><img src="3D%E7%BB%93%E6%9E%84.jpg" alt="3D结构"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span></span><br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">Mykernel</span><span class="hljs-params">(cudaPitchedPtr devPitchPtr,</span></span><br><span class="hljs-params"><span class="hljs-function">                    <span class="hljs-type">int</span> width, <span class="hljs-type">int</span> height, <span class="hljs-type">int</span> depth)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">char</span>* devPtr = (<span class="hljs-type">char</span>*)devPitchPtr.ptr;<br>    <span class="hljs-type">size_t</span> pitch = devPitchPtr.pitch;<br>    <span class="hljs-type">size_t</span> slicePitch = height * pitch;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> z = <span class="hljs-number">0</span>; z &lt; depth; ++z)<br>    &#123;<br>        <span class="hljs-type">char</span>* slice = devPtr + z*slicePitch;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y = <span class="hljs-number">0</span>; y &lt; height; ++y)<br>        &#123;<br>            <span class="hljs-type">float</span>* row = (<span class="hljs-type">float</span>*)(slice + y* pitch);<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> x = <span class="hljs-number">0</span>; x &lt; width; ++x)<br>                <span class="hljs-type">float</span> element = row[x];<br>        &#125;<br>    &#125;<br><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> width = <span class="hljs-number">64</span>, height = <span class="hljs-number">64</span>, depth = <span class="hljs-number">64</span>;<br>    cudaExtent extent = <span class="hljs-built_in">make_cudaExtent</span>(width * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), height, depth);<br><br>    cudaPitchedPtr devPitchedPtr;<br>    <span class="hljs-built_in">cudaMalloc3D</span>(&amp;devPitchedPtr, extent);<br><br>    Mykernel&lt;&lt;&lt;<span class="hljs-number">100</span>, <span class="hljs-number">512</span>&gt;&gt;&gt;(devPitchedPtr, width, height, depth);<br>&#125;<br></code></pre></td></tr></table></figure><p>有如下的变量声明</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++">__device__ <span class="hljs-type">float</span> devData; <span class="hljs-comment">//设备端(Device)的全局变量</span><br>__constant__ <span class="hljs-type">float</span> constData; <span class="hljs-comment">//只读且缓存</span><br>__shared__ <span class="hljs-type">float</span> sharedData;  <span class="hljs-comment">// 更快但块内可见</span><br></code></pre></td></tr></table></figure><p>使用 <code>cudaMemcpyToSymbol</code>将host内存拷贝到device端</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaMemcpyToSymbol</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">void</span>* symbol, <span class="hljs-comment">// 设备端的符号(通常是全局变量或常量)</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">void</span>* src,    <span class="hljs-comment">// 主机端源数据指针</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> count,       <span class="hljs-comment">// 要拷贝的字节数</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> offset = <span class="hljs-number">0</span>,  <span class="hljs-comment">// 可选参数，符号地址的偏移量(默认为0)</span></span></span><br><span class="hljs-params"><span class="hljs-function">    cudaMemcpyKind kind = cudaMemcpyHostToDevice <span class="hljs-comment">// 拷贝方向</span></span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span>;<br></code></pre></td></tr></table></figure><p>使用 <code>cudaMemcpyFromSymbol</code>将device侧拷贝到host内存</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaMemcpyFromSymbol</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">void</span>* dst,          <span class="hljs-comment">// 主机端目标指针</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">void</span>* symbol, <span class="hljs-comment">// 设备端的符号</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> count,       <span class="hljs-comment">// 要拷贝的字节数</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> offset = <span class="hljs-number">0</span>,  <span class="hljs-comment">// 可选参数，符号地址的偏移量(默认为0)</span></span></span><br><span class="hljs-params"><span class="hljs-function">    cudaMemcpyKind kind = cudaMemcpyDeviceToHost <span class="hljs-comment">// 拷贝方向</span></span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span>;<br></code></pre></td></tr></table></figure><h3 id="Shared-Memory">Shared Memory</h3><p>共享内存使用<code>__shared__</code>说明符实现，其速度比全局内存快得多，下面代码是一个矩阵乘法的实现</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span></span><br><br><span class="hljs-keyword">typedef</span> <span class="hljs-keyword">struct</span><br>&#123;<br>    <span class="hljs-type">int</span> width;<br>    <span class="hljs-type">int</span> height;<br>    <span class="hljs-type">float</span>* elements;<br><br>&#125; Matrix;<br><br><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> BLOCK_SIZE 16</span><br><br><span class="hljs-comment">//Forward declaration</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatMulkernel</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix, <span class="hljs-type">const</span> Matrix, Matrix)</span></span>;<br><br><br><span class="hljs-comment">//Host code</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">MatMul</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix A, <span class="hljs-type">const</span> Matrix B, Matrix C)</span></span><br><span class="hljs-function"></span>&#123;<br>    Matrix d_A;<br>    d_A.width = A.width; d_A.height = A.height;<br>    <span class="hljs-type">size_t</span> size = A.width * A.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_A.elements, size);<br>    <span class="hljs-comment">//host to device</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(d_A.elements, A.elements, size, cudaMemcpyHostToDevice);<br><br>    Matrix d_B;<br>    d_B.width = B.width, d_B.height = B.height;<br>    size = B.width * B.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_B.elements, size);<br>    <span class="hljs-comment">//host to device</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(d_B.elements, B.elements, size, cudaMemcpyHostToDevice);<br><br><br>    Matrix d_C;<br>    d_C.width = C.width, d_C.height = C.height;<br>    size = C.width * C.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-comment">//only need to malloc memory in device </span><br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_C.elements, size);<br><br>    <span class="hljs-function">dim3 <span class="hljs-title">dimBlock</span><span class="hljs-params">(BLOCK_SIZE, BLOCK_SIZE)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">dimGrid</span><span class="hljs-params">(B.width / BLOCK_SIZE, A.height / BLOCK_SIZE)</span></span>;<br><br>    MatMulkernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C);<br><br>    <span class="hljs-comment">//read C from device</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(C.elements, d_C.elements, size, cudaMemcpyDeviceToHost);<br><br><br>    <span class="hljs-built_in">cudaFree</span>(d_A.elements);<br>    <span class="hljs-built_in">cudaFree</span>(d_B.elements);<br>    <span class="hljs-built_in">cudaFree</span>(d_C.elements);<br>&#125;<br><br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatMulkernel</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix A, <span class="hljs-type">const</span> Matrix B, Matrix C)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">float</span> Cvalue = <span class="hljs-number">0</span>;<br>    <span class="hljs-type">int</span> row = blockDim.y * blockIdx.y + threadIdx.y;<br>    <span class="hljs-type">int</span> col = blockDim.x * blockIdx.x + threadIdx.x;<br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; A.width; ++i)<br>    &#123;<br>        Cvalue += A.elements[row * A.width + i] * B.elements[i * B.width + col];<br>    &#125;<br>    C.elements[row * C.width + col] = Cvalue;<br>&#125;<br><br></code></pre></td></tr></table></figure><p><img src="Matrix_Multiplication.png" alt="Matrix_Multiplication"></p><p>进一步采用共享内存优化，将大矩阵分解为 <code>BLOCK_SIZE × BLOCK_SIZE</code> 的子矩阵，每个线程块计算一个子矩阵 <code>Csub</code>，每个线程计算 <code>Csub</code> 的一个元素，<code>As</code> 和 <code>Bs</code> 存储在共享内存中，减少对全局内存的访问，每个线程块只需加载 <code>Asub</code> 和 <code>Bsub</code> 一次，然后所有线程共享这些数据。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span></span><br><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> BLOCK_SIZE 16</span><br><br><span class="hljs-keyword">typedef</span> <span class="hljs-keyword">struct</span><br>&#123;<br>    <span class="hljs-type">int</span> width;<br>    <span class="hljs-type">int</span> height;<br>    <span class="hljs-type">int</span> stride; <span class="hljs-comment">/// 矩阵的内存步长（通常等于width, 表示一行有多少个元素）</span><br>    <span class="hljs-type">float</span> * elements;<br>&#125; Matrix;<br><br><span class="hljs-comment">//Get matrix element in[row, col]</span><br><span class="hljs-function">__device__ <span class="hljs-type">float</span> <span class="hljs-title">GetElement</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix A, <span class="hljs-type">int</span> row, <span class="hljs-type">int</span> col)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">return</span> A.elements[row * A.stride + col];<br>&#125;<br><br><span class="hljs-comment">//Set matrix element in[row,col]</span><br><span class="hljs-function">__device__ <span class="hljs-type">void</span> <span class="hljs-title">SetElement</span><span class="hljs-params">(Matrix A, <span class="hljs-type">int</span> row, <span class="hljs-type">int</span> col, <span class="hljs-type">float</span> value)</span></span><br><span class="hljs-function"></span>&#123;<br>    A.elements[row * A.stride + col] = value;<br>&#125;<br><br><span class="hljs-comment">//Get subMatrix in [row, col] (row col describe block not element)</span><br><span class="hljs-function">__device__ Matrix <span class="hljs-title">GetSubMatrix</span><span class="hljs-params">(Matrix A, <span class="hljs-type">int</span> row, <span class="hljs-type">int</span> col)</span></span><br><span class="hljs-function"></span>&#123;<br>    Matrix Asub;<br>    Asub.width = BLOCK_SIZE;<br>    Asub.height = BLOCK_SIZE;<br>    Asub.stride = A.stride;<br>    Asub.elements = &amp;A.elements[A.stride * row * BLOCK_SIZE + BLOCK_SIZE * col];<br><br>    <span class="hljs-keyword">return</span> Asub;<br>&#125;<br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatMulkernel</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix, <span class="hljs-type">const</span> Matrix, Matrix)</span></span>;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">MatMul</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix A, <span class="hljs-type">const</span> Matrix B, Matrix C)</span></span><br><span class="hljs-function"></span>&#123;<br>    Matrix d_A;<br>    d_A.width = d_A.stride = A.width; d_A.height = A.height;<br>    <span class="hljs-type">size_t</span> size = d_A.width * d_A.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_A.elements, size);<br>    <span class="hljs-built_in">cudaMemcpy</span>(d_A.elements, A.elements, size, cudaMemcpyHostToDevice);<br><br><br>    Matrix d_B;<br>    d_B.width = d_B.stride = B.width; d_B.height = B.height;<br>    <span class="hljs-type">size_t</span> size = d_B.width * d_B.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_B.elements, size);<br>    <span class="hljs-built_in">cudaMemcpy</span>(d_B.elements, B.elements, size, cudaMemcpyHostToDevice);<br><br><br>    Matrix d_C;<br>    d_C.width = d_C.stride = C.width; d_C.height = C.height;<br>    <span class="hljs-type">size_t</span> size = d_C.width * d_C.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_C.elements, size);<br><br>    <span class="hljs-function">dim3 <span class="hljs-title">dimBlock</span><span class="hljs-params">(BLOCK_SIZE, BLOCK_SIZE)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">dimGrid</span><span class="hljs-params">(B.width / BLOCK_SIZE, A.height / BLOCK_SIZE)</span></span>;<br><br>    MatMulkernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C);<br><br>    <span class="hljs-comment">//read C from device</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(C.elements, d_C.elements, size, cudaMemcpyDeviceToHost);<br><br><br>    <span class="hljs-built_in">cudaFree</span>(d_A.elements);<br>    <span class="hljs-built_in">cudaFree</span>(d_B.elements);<br>    <span class="hljs-built_in">cudaFree</span>(d_C.elements);<br>    <br>&#125;<br><br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatMulkernel</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix A, <span class="hljs-type">const</span> Matrix B, Matrix C)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> blockRow = blockDim.y;<br>    <span class="hljs-type">int</span> blockCol = blockDim.x;<br><br>    <span class="hljs-comment">//Each Block computes one sub-matrix of C</span><br>    Matrix Csub = <span class="hljs-built_in">GetSubMatrix</span>(C, blockRow, blockCol);<br><br>    <span class="hljs-comment">//Thread row and col in Csub</span><br>    <span class="hljs-type">int</span> row = threadIdx.y;<br>    <span class="hljs-type">int</span> col = threadIdx.x;<br><br>    <span class="hljs-comment">//Each thread computes one element of Csub</span><br>    <span class="hljs-type">float</span> Cvalue = <span class="hljs-number">0</span>;<br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> m = <span class="hljs-number">0</span>; m &lt; (A.width / BLOCK_SIZE); ++m)<br>    &#123;<br>        Matrix Asub = <span class="hljs-built_in">GetSubMatrix</span>(A, blockRow, m);<br>        Matrix Bsub = <span class="hljs-built_in">GetSubMatrix</span>(B, m, blockCol);<br><br>        <span class="hljs-comment">//shared_memory to store Asub and Bsub</span><br>        __shared__ <span class="hljs-type">float</span> As[BLOCK_SIZE][BLOCK_SIZE];<br>        __shared__ <span class="hljs-type">float</span> Bs[BLOCK_SIZE][BLOCK_SIZE];<br><br>        As[row][col] = <span class="hljs-built_in">GetElement</span>(A, row, col);<br>        Bs[row][col] = <span class="hljs-built_in">GetElement</span>(B, row, col);<br><br>        <span class="hljs-comment">//确保上述所有操作同步</span><br>        __syncthreads();<br><br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> e = <span class="hljs-number">0</span>; e &lt; BLOCK_SIZE; e++)<br>            Cvalue += As[row][e] * Bs[e][col];<br><br>        <br>        __syncthreads();<br>    &#125;<br><br>    <span class="hljs-built_in">SetElement</span>(C, row, col, Cvalue);<br>&#125;<br><br></code></pre></td></tr></table></figure><p>值得注意的是 <code>GetSubMatrix()</code>的逻辑，其中的<code>[row,col]</code>是以整个<strong>Block</strong>为单位，而不再是以单个element为单元, 同时矩阵的宽度和高度必须是 <code>BLOCK_SIZE</code> 的整数倍。</p><p><img src="subMatrix.jpg" alt="subMatrix"></p><p><img src="Shared_memory.png" alt="Shared_memory"></p><h3 id="Page-Locked-Host-Memory（页锁定主机内存）">Page-Locked Host Memory（页锁定主机内存）</h3><p>Page-Locked Host Memory（也称为 <strong>Pinned Memory</strong> 或 <strong>Non-Pageable Memory</strong>）是 CUDA 中一种特殊的主机（CPU）内存分配方式，它<strong>禁止操作系统对这块内存进行分页交换（Page-Out）</strong>，从而保证内存始终驻留在物理 RAM 中，不会因虚拟内存机制被换出到磁盘.</p><p>在默认情况下，主机内存是 <strong>Pageable（可分页的）</strong>，操作系统可以随时将不活跃的内存页换出到磁盘（Swap Space）。但在 GPU 计算中，这会带来两个问题：</p><ol><li><strong>异步内存拷贝的效率问题</strong><ul><li>CUDA 的 <code>cudaMemcpy</code> 默认是同步操作，但如果使用 <code>cudaMemcpyAsync</code>（异步拷贝），要求源或目标内存必须是 <strong>Page-Locked</strong>，否则无法保证 DMA（直接内存访问）的正确性。</li><li>如果内存可分页，GPU 驱动必须先临时锁定内存，再执行拷贝，这会降低性能。</li></ul></li><li><strong>零拷贝内存（Zero-Copy）支持</strong><ul><li>Page-Locked Memory 可以直接映射到 GPU 地址空间，允许 GPU 直接访问主机内存（避免显式拷贝），但要求内存必须是锁定的</li></ul></li></ol><p>所以页锁主机内存适用于<strong>频繁的 CPU-GPU 数据传输</strong>（如深度学习数据加载），<strong>异步内存拷贝（<code>cudaMemcpyAsync</code>）</strong>，以及<strong>Zero-Copy 内存（GPU 直接访问主机内存）</strong></p><p>使用<code>cudaHostAlloc</code> 、<code>cudaFreeHost</code> 进行分配和释放内存</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-type">float</span> *h_data;<br><span class="hljs-built_in">cudaHostAlloc</span>((<span class="hljs-type">void</span>**)&amp;h_data, size_in_bytes, cudaHostAllocDefault);<br><span class="hljs-comment">// ... 使用 h_data ...</span><br><span class="hljs-built_in">cudaFreeHost</span>(h_data);<br></code></pre></td></tr></table></figure><p><code>cudaHostAlloc</code>有如下的几个选项：</p><ol><li><p><strong>可移植内存（Portable Memory）</strong></p><p>Page-Locked内存通常仅对分配时当前的GPU设备有效，通过<code>cudaHostAllocPortable</code>标志分配的内存可被系统中<strong>所有GPU设备</strong>直接使用，适用于多GPU环境。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 分配可移植的Page-Locked内存</span><br><span class="hljs-built_in">cudaHostAlloc</span>(&amp;h_portable, N*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaHostAllocPortable);<br><br><span class="hljs-comment">// 在多个设备上使用</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> dev = <span class="hljs-number">0</span>; dev &lt; num_devices; dev++) &#123;<br>    <span class="hljs-built_in">cudaSetDevice</span>(dev);<br>    <span class="hljs-built_in">cudaMemcpyAsync</span>(d_data[dev], h_portable, N*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice, stream[dev]);<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p><strong>写合并内存（Write-Combining Memory）</strong></p><p>Page-Locked内存通常是<strong>可缓存的</strong>（Cacheable），会占用CPU的L1/L2缓存,通过<code>cudaHostAllocWriteCombined</code>分配的内存为<strong>写合并内存</strong>，释放L1/L2缓存资源供其他应用使用，适用于主机频繁写入、GPU读取的数据（如实时数据采集）。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 分配Write-Combining内存（仅主机写入）</span><br><span class="hljs-built_in">cudaHostAlloc</span>(&amp;h_wc, N*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaHostAllocWriteCombined);<br><br><span class="hljs-comment">// 主机填充数据（快速）</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; N; i++) h_wc[i] = data[i];<br><br><span class="hljs-comment">// 传输到GPU（高效）</span><br><span class="hljs-built_in">cudaMemcpyAsync</span>(d_data, h_wc, N*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice, stream);<br></code></pre></td></tr></table></figure></li><li><p><strong>映射内存（Mapped Memory）</strong></p><p>利用<strong>零拷贝（Zero-Copy）</strong>，实现主机内存直接映射到GPU地址空间，无需显式拷贝。</p><p>主机地址：通过<code>cudaHostAlloc</code>返回。</p><p>设备地址：通过<code>cudaHostGetDevicePointer</code>获取。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-type">float</span> *h_mapped;<br><span class="hljs-built_in">cudaHostAlloc</span>(&amp;h_mapped, size, cudaHostAllocMapped);<br><br><span class="hljs-type">float</span> *d_mapped;<br><span class="hljs-built_in">cudaHostGetDevicePointer</span>(&amp;d_mapped, h_mapped, <span class="hljs-number">0</span>);<br></code></pre></td></tr></table></figure><p>内核访问时自动触发传输，无需<code>cudaMemcpy</code>, 同时确保了数据传输与内核计算自动并行。</p><h3 id="异步并发执行">异步并发执行</h3><p>CUDA将以下操作视为独立任务，可并行执行：</p><ul><li><strong>主机计算</strong>（CPU任务）</li><li><strong>设备计算</strong>（GPU内核）</li><li><strong>主机→设备内存传输</strong>（H2D）</li><li><strong>设备→主机内存传输</strong>（D2H）</li><li><strong>设备内内存传输</strong>（D2D）</li><li><strong>多设备间内存传输</strong></li></ul><ol><li><p><strong>Host与Device并发执行</strong></p><p><strong>异步函数</strong>：通过非阻塞调用（如<code>cudaMemcpyAsync</code>、<code>kernel&lt;&lt;&lt;...&gt;&gt;&gt;</code>）让主机线程立即返回，无需等待设备完成。</p><p>注：<code>cudaMemcpyAsync</code> 是 CUDA 中用于<strong>异步内存拷贝</strong>的核心函数，它的主要作用是在<strong>不阻塞主机（CPU）线程</strong>的情况下，在主机（Host）与设备（Device）之间或设备内部传输数据。与同步版本的 <code>cudaMemcpy</code> 不同，<code>cudaMemcpyAsync</code> 允许 CPU 在数据传输的同时继续执行其他任务，从而实现<strong>计算与传输的重叠</strong>，提升程序的整体效率。</p></li><li><p><strong>并发内核执行</strong></p><p>设备需<strong>Compute Capability ≥ 2.0</strong>，且<code>concurrentKernels</code>属性为1。</p><ul><li><p><strong>上下文隔离</strong>：不同CUDA上下文的内核无法并发。</p></li><li><p><strong>资源竞争</strong>：占用大量纹理内存或本地内存的内核会降低并发性。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> can_concurrent;<br><span class="hljs-built_in">cudaDeviceGetAttribute</span>(&amp;can_concurrent, cudaDevAttrConcurrentKernels, dev);<br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>数据传输与内核执行重叠</strong></p><ul><li><p>设备需支持<code>asyncEngineCount &gt; 0</code>。</p></li><li><p><strong>必须使用Page-Locked主机内存</strong></p><ol><li><p><strong>H2D/D2H传输与内核执行重叠</strong>：</p><ul><li>异步拷贝（<code>cudaMemcpyAsync</code>）与内核并发。</li></ul></li><li><p><strong>设备内拷贝（D2D）与内核执行重叠</strong>：</p><ul><li><p>需<code>concurrentKernels</code>支持</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaStream_t stream1, stream2;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream1);<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream2);<br><br><span class="hljs-comment">// 异步H2D传输（流1）</span><br><span class="hljs-built_in">cudaMemcpyAsync</span>(d_data, h_data, size, cudaMemcpyHostToDevice, stream1);<br><br><span class="hljs-comment">// 内核执行（流2，与传输重叠）</span><br>kernel&lt;&lt;&lt;grid, block, <span class="hljs-number">0</span>, stream2&gt;&gt;&gt;(d_data);<br><br><span class="hljs-comment">// 异步D2H传输（流1）</span><br><span class="hljs-built_in">cudaMemcpyAsync</span>(h_result, d_data, size, cudaMemcpyDeviceToHost, stream1);<br></code></pre></td></tr></table></figure></li></ul></li></ol></li></ul></li><li><p><strong>并发数据传输</strong></p><ul><li>设备需<code>asyncEngineCount = 2</code>（Compute Capability ≥ 2.0）。</li><li><strong>必须使用Page-Locked内存</strong>。</li><li>同时进行H2D和D2H传输（双向重叠）</li></ul></li><li><p><strong>流管理</strong></p><p><strong>流</strong>是命令序列（如内核、内存拷贝），在<strong>同一流内顺序执行</strong>，<strong>不同流间可能并发</strong>，<strong>无显式同步时，流间执行顺序不确定</strong>。</p><p>流的创建与销毁</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaStream_t stream;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream);  <span class="hljs-comment">// 创建流</span><br><br><span class="hljs-comment">// 在流中执行操作</span><br>kernel&lt;&lt;&lt;grid, block, <span class="hljs-number">0</span>, stream&gt;&gt;&gt;(...);<br><span class="hljs-built_in">cudaMemcpyAsync</span>(..., stream);<br><br><span class="hljs-built_in">cudaStreamDestroy</span>(stream);  <span class="hljs-comment">// 销毁流（若流未完成，自动等待后释放资源）</span><br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaStream_t streams[<span class="hljs-number">2</span>];<br><span class="hljs-type">float</span> *h_pinned, *d_data;<br><span class="hljs-built_in">cudaMallocHost</span>(&amp;h_pinned, <span class="hljs-number">2</span> * size);  <span class="hljs-comment">// Page-Locked内存</span><br><span class="hljs-built_in">cudaMalloc</span>(&amp;d_data, <span class="hljs-number">2</span> * size);<br><br><span class="hljs-comment">// 创建流</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">2</span>; i++) <br>    <span class="hljs-built_in">cudaStreamCreate</span>(&amp;streams[i]);<br><br><span class="hljs-comment">// 异步操作（流间可能并发）</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">2</span>; i++) &#123;<br>    <span class="hljs-built_in">cudaMemcpyAsync</span>(d_data + i*size, h_pinned + i*size, <br>                    size, cudaMemcpyHostToDevice, streams[i]);<br>    kernel&lt;&lt;&lt;grid, block, <span class="hljs-number">0</span>, streams[i]&gt;&gt;&gt;(d_data + i*size);<br>    <span class="hljs-built_in">cudaMemcpyAsync</span>(h_pinned + i*size, d_data + i*size,<br>                    size, cudaMemcpyDeviceToHost, streams[i]);<br>&#125;<br><br><span class="hljs-comment">// 同步所有流</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">2</span>; i++) <br>    <span class="hljs-built_in">cudaStreamSynchronize</span>(streams[i]);<br><br><span class="hljs-comment">// 释放资源</span><br><span class="hljs-built_in">cudaFreeHost</span>(h_pinned);<br><span class="hljs-built_in">cudaFree</span>(d_data);<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">2</span>; i++) <br>    <span class="hljs-built_in">cudaStreamDestroy</span>(streams[i]);<br></code></pre></td></tr></table></figure><p>未指定流或流为<code>0</code>时使用<strong>全局NULL流</strong>（隐式同步所有操作），即默认的编译选项<code>--default-stream legacy</code>，也可以指定–<code>default-stream per-thread</code>或定义宏<code>CUDA_API_PER_THREAD_DEFAULT_STREAM</code>来使每个主机线程都有独立的默认流，支持并发。</p><p>流的同步方式有两种：</p><ol><li><p>显式同步</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaStreamSynchronize</span>(stream);  <span class="hljs-comment">// 等待流完成</span><br><span class="hljs-built_in">cudaDeviceSynchronize</span>();        <span class="hljs-comment">// 等待所有流完成</span><br><br><span class="hljs-built_in">cudaStreamWaitEvent</span>();<br><span class="hljs-built_in">cudaStreamQuery</span>(<br></code></pre></td></tr></table></figure></li><li><p>隐式同步</p><p>NULL流中的操作会同步所有其他流。</p></li></ol></li></ol><p>当主机线程在两个流的命令之间插入以下操作时，<strong>不同流的命令将失去并发性</strong>，导致串行执行:</p><ol><li><p><strong>页锁定主机内存分配</strong></p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss"><span class="hljs-built_in">cudaMallocHost</span>(&amp;ptr, size);  <span class="hljs-comment">// 阻塞所有流的并发</span><br></code></pre></td></tr></table></figure></li><li><p><strong>设备内存分配</strong></p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss"><span class="hljs-built_in">cudaMalloc</span>(&amp;d_ptr, size);    <span class="hljs-comment">// 强制同步</span><br></code></pre></td></tr></table></figure></li><li><p><strong>设备内存初始化（如cudaMemset）</strong></p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss"><span class="hljs-built_in">cudaMemset</span>(d_ptr, <span class="hljs-number">0</span>, size);  <span class="hljs-comment">// 隐式同步点</span><br></code></pre></td></tr></table></figure></li><li><p><strong>同一设备内存的拷贝（D2D）</strong></p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss"><span class="hljs-built_in">cudaMemcpy</span>(dst, src, size, cudaMemcpyDeviceToDevice); <span class="hljs-comment">// 内部同步</span><br></code></pre></td></tr></table></figure></li><li><p><strong>NULL流（默认流）中的任何操作</strong></p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss">kernel&lt;&lt;&lt;<span class="hljs-attribute">grid</span>, block&gt;&gt;&gt;();  <span class="hljs-comment">// 默认流操作会同步所有流</span><br></code></pre></td></tr></table></figure></li><li><p><strong>L1/Shared内存配置切换</strong><br>（针对Compute Capability 3.x/7.x设备）</p></li></ol><p>两个流之间的执行重叠量取决于向每个流发出命令的顺序，以及设备是否支持数据传输和内核执行的重叠，并发内核执行，或并发数据传输。</p><table><thead><tr><th style="text-align:left"><strong>场景</strong></th><th style="text-align:left"><strong>问题根源</strong></th><th style="text-align:left"><strong>优化方法</strong></th></tr></thead><tbody><tr><td style="text-align:left">不支持并发数据传输的设备</td><td style="text-align:left">H2D/D2H共用引擎</td><td style="text-align:left">无解，需升级硬件</td></tr><tr><td style="text-align:left">支持并发数据传输的设备</td><td style="text-align:left">任务提交顺序限制重叠</td><td style="text-align:left">分组提交（H2D→Kernel→D2H）</td></tr><tr><td style="text-align:left">计算能力≤3.0的设备</td><td style="text-align:left">内核启动依赖全局线程块进度</td><td style="text-align:left">提前提交所有内核，延迟D2H</td></tr><tr><td style="text-align:left">任何设备</td><td style="text-align:left">NULL流或隐式同步操作</td><td style="text-align:left">使用显式流，避免全局操作</td></tr></tbody></table><p>例如，我们在不支持并发数据传输的设备上，执行下面的逻辑：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 流0：H2D -&gt; Kernel -&gt; D2H</span><br><span class="hljs-comment">// 流1：H2D -&gt; Kernel -&gt; D2H （提交顺序导致串行化）</span><br></code></pre></td></tr></table></figure><p>流1的H2D必须等待流0的D2H完成（因为D2H和H2D共用传输引擎）,由于<strong>零重叠</strong>，所以流0和流1是完全串行执行。</p><p><strong><code>cudaLaunchHostFunc</code></strong> 函数在CUDA流的特定位置插入一个<strong>主机端函数</strong>，该函数会在流中<strong>此前所有命令完成</strong>后自动触发，由CUDA runtime调度，不会阻塞</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaLaunchHostFunc</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    cudaStream_t stream,          <span class="hljs-comment">// 目标流</span></span></span><br><span class="hljs-params"><span class="hljs-function">    cudaHostFn_t fn,              <span class="hljs-comment">// 回调函数指针</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">void</span>* userData                <span class="hljs-comment">// 传递给回调的用户数据</span></span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span>;<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">void</span> CUDART_CB <span class="hljs-title">MyCallback</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    cudaStream_t stream,  <span class="hljs-comment">// 关联的流</span></span></span><br><span class="hljs-params"><span class="hljs-function">    cudaError_t status,   <span class="hljs-comment">// 流中前置操作的状态（成功/错误）</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">void</span>* data            <span class="hljs-comment">// 用户数据</span></span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span>;<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-built_in">cudaMemcpyAsync</span>(..., stream);  <span class="hljs-comment">// 操作1</span><br>kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;();     <span class="hljs-comment">// 操作2</span><br><span class="hljs-built_in">cudaLaunchHostFunc</span>(stream, MyCallback, data); <span class="hljs-comment">// 回调</span><br><span class="hljs-comment">// MyCallback 仅在操作1和2完成后执行</span><br></code></pre></td></tr></table></figure><p>需要注意的是回调函数调用CUDA API（如<code>cudaMemcpy</code>），可能等待自身完成，导致死锁。</p><p>在运行时，高优先级流中的待处理工作优先于低优先级流中的待处理工作</p><p>可以使用<code>cudaDeviceGetStreamPriorityRange()</code>获取当前的优先级范围，并且利用<code>cudaStreamCreateWithPriority()</code>设定流的优先级。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// get the range of stream priorities for this device</span><br><span class="hljs-type">int</span> priority_high, priority_low;<br><span class="hljs-built_in">cudaDeviceGetStreamPriorityRange</span>(&amp;priority_low, &amp;priority_high);<br><span class="hljs-comment">// create streams with highest and lowest available priorities</span><br>cudaStream_t st_high, st_low;<br><span class="hljs-built_in">cudaStreamCreateWithPriority</span>(&amp;st_high, cudaStreamNonBlocking, priority_high);<br><span class="hljs-built_in">cudaStreamCreateWithPriority</span>(&amp;st_low, cudaStreamNonBlocking, priority_low);<br></code></pre></td></tr></table></figure><h3 id="Graphs">Graphs</h3><p>传统流模型每次内核启动或内存拷贝都需要CPU驱动执行设置工作（如参数验证、GPU命令生成），对于短时内核，开销占比显著，而且无法让CUDA看到完整任务流，难以全局优化。</p><p>对此Graphs将整个工作流（包括操作和依赖）预先定义为<strong>图结构</strong>，后续可重复执行，也方便了实例化阶段完成大部分初始化，减少运行时开销。</p><p>以下是创建一个图的简单流程：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// Create the graph - it starts out empty</span><br><span class="hljs-built_in">cudaGraphCreate</span>(&amp;graph, <span class="hljs-number">0</span>);<br><span class="hljs-comment">// For the purpose of this example, we&#x27;ll create</span><br><span class="hljs-comment">// the nodes separately from the dependencies to</span><br><span class="hljs-comment">// demonstrate that it can be done in two stages.</span><br><span class="hljs-comment">// Note that dependencies can also be specified</span><br><span class="hljs-comment">// at node creation.</span><br><span class="hljs-built_in">cudaGraphAddKernelNode</span>(&amp;a, graph, <span class="hljs-literal">NULL</span>, <span class="hljs-number">0</span>, &amp;nodeParams);<br><span class="hljs-built_in">cudaGraphAddKernelNode</span>(&amp;b, graph, <span class="hljs-literal">NULL</span>, <span class="hljs-number">0</span>, &amp;nodeParams);<br><span class="hljs-built_in">cudaGraphAddKernelNode</span>(&amp;c, graph, <span class="hljs-literal">NULL</span>, <span class="hljs-number">0</span>, &amp;nodeParams);<br><span class="hljs-built_in">cudaGraphAddKernelNode</span>(&amp;d, graph, <span class="hljs-literal">NULL</span>, <span class="hljs-number">0</span>, &amp;nodeParams);<br><span class="hljs-comment">// Now set up dependencies on each node</span><br><span class="hljs-built_in">cudaGraphAddDependencies</span>(graph, &amp;a, &amp;b, <span class="hljs-number">1</span>); <span class="hljs-comment">// A-&gt;B</span><br><span class="hljs-built_in">cudaGraphAddDependencies</span>(graph, &amp;a, &amp;c, <span class="hljs-number">1</span>); <span class="hljs-comment">// A-&gt;C</span><br><span class="hljs-built_in">cudaGraphAddDependencies</span>(graph, &amp;b, &amp;d, <span class="hljs-number">1</span>); <span class="hljs-comment">// B-&gt;D</span><br><span class="hljs-built_in">cudaGraphAddDependencies</span>(graph, &amp;c, &amp;d, <span class="hljs-number">1</span>); <span class="hljs-comment">// C-&gt;D</span><br></code></pre></td></tr></table></figure><p><img src="Graph.png" alt="Graph"></p><p>也可以同步捕获流来创建一个图：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaGraph_t graph;<br><br><span class="hljs-built_in">cudaStreamBeginCapture</span>(stream); <span class="hljs-comment">// 进入捕获模式,后续放入流的操作不会立即执行，而是被记录到内部图中</span><br>kernel_A&lt;&lt;&lt; ..., stream &gt;&gt;&gt;(...); <span class="hljs-comment">//被捕获为图节点</span><br>kernel_B&lt;&lt;&lt; ..., stream &gt;&gt;&gt;(...);<br><span class="hljs-built_in">libraryCall</span>(stream); <span class="hljs-comment">//// 库函数调用（需支持捕获）</span><br>kernel_C&lt;&lt;&lt; ..., stream &gt;&gt;&gt;(...);<br><span class="hljs-built_in">cudaStreamEndCapture</span>(stream, &amp;graph); <span class="hljs-comment">//// 返回构建的图graph</span><br></code></pre></td></tr></table></figure><p>支持普通流和每线程流（<code>cudaStreamPerThread</code>），<strong>不支持</strong>传统NULL流（<code>cudaStreamLegacy</code>），可以通过下述代码查询：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-built_in">cudaStreamIsCapturing</span>(stream, &amp;isCapturing);  <span class="hljs-comment">// 检查流是否处于捕获模式</span><br></code></pre></td></tr></table></figure><p>上图的依赖关系，我们可以通过引入事件来实现跨流的依赖：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// stream1 is the origin stream</span><br><span class="hljs-built_in">cudaStreamBeginCapture</span>(stream1);<br>kernel_A&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);<br><span class="hljs-comment">// Fork into stream2</span><br><span class="hljs-built_in">cudaEventRecord</span>(event1, stream1);<br><span class="hljs-built_in">cudaStreamWaitEvent</span>(stream2, event1);<br>kernel_B&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);<br>kernel_C&lt;&lt;&lt; ..., stream2 &gt;&gt;&gt;(...);<br><span class="hljs-comment">// Join stream2 back to origin stream (stream1)</span><br><span class="hljs-built_in">cudaEventRecord</span>(event2, stream2);<br><span class="hljs-built_in">cudaStreamWaitEvent</span>(stream1, event2);<br>kernel_D&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);<br><span class="hljs-comment">// End capture in the origin stream</span><br><span class="hljs-built_in">cudaStreamEndCapture</span>(stream1, &amp;graph);<br><span class="hljs-comment">// stream1 and stream2 no longer in capture mode</span><br></code></pre></td></tr></table></figure><p>可以这样理解上述的代码：首先创建了stream1这条流，从kernel_A 触发，生成对的event1事件，创建第二条流等待event1事件，也就是kernel_A的创建完成，那么stream2也自然建立了和kernel_A的关系，kernel_B延续stream1，kernel_C延续stream2, 之后建立第二个事件 也就是stream2（kernel_C的是否完成），同时stream1流等待event2，当stream1流和stream2流都通过时，也就是kernel_B和kernel_C都完成时，kernel_C延续stream1，从而实现了图示的效果。</p><p>注意下面的代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// stream1 is the origin stream</span><br><span class="hljs-built_in">cudaStreamBeginCapture</span>(stream1);<br>kernel_A&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);<br><br><span class="hljs-comment">// Fork into stream2</span><br><span class="hljs-built_in">cudaEventRecord</span>(event1, stream1);<br><span class="hljs-built_in">cudaStreamWaitEvent</span>(stream2, event1);<br>kernel_B&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);<br>kernel_C&lt;&lt;&lt; ..., stream2 &gt;&gt;&gt;(...);<br><br><span class="hljs-comment">// 错误依赖关系</span><br><span class="hljs-built_in">cudaEventRecord</span>(event2, stream1);<br><span class="hljs-built_in">cudaStreamWaitEvent</span>(stream2, event2);<br><br><span class="hljs-comment">//这里编程stream2 无影响</span><br>kernel_D&lt;&lt;&lt; ..., stream2 &gt;&gt;&gt;(...);<br><br><span class="hljs-comment">// End capture in the origin stream</span><br><span class="hljs-built_in">cudaStreamEndCapture</span>(stream1, &amp;graph);<br><span class="hljs-comment">// stream1 and stream2 no longer in capture mode</span><br></code></pre></td></tr></table></figure><p>由于先创建了stream1，所以stream1最大可能最先执行完，所以stream1需要等待stream2的完成，而不是stream2等待stream1，而最后的kernel_D无论基于哪个流，二者都已经同步完成，所以不影响。</p><p>需要注意的是：<code>cudaStreamBeginCapture</code>哪个流开始，<code>cudaStreamEndCapture</code>必须结束对应的流。</p><p>捕获模式下的流和事件仅是<strong>图构建的临时抽象</strong>，不代表实际GPU任务队列。尝试同步/查询会导致：</p><ul><li>逻辑矛盾（无法查询未提交执行的任务状态）</li><li>潜在死锁（捕获未完成却要求同步）</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaStreamBeginCapture</span>(stream);<br>kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;();<br><span class="hljs-built_in">cudaStreamSynchronize</span>(stream); <span class="hljs-comment">// 错误！流在捕获模式</span><br></code></pre></td></tr></table></figure><p>跨图合并也是禁止的：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 图1</span><br><span class="hljs-built_in">cudaStreamBeginCapture</span>(stream1);<br><span class="hljs-built_in">cudaEventRecord</span>(event1, stream1);<br><span class="hljs-built_in">cudaStreamEndCapture</span>(stream1, &amp;graph1);<br><br><span class="hljs-comment">// 图2（错误尝试合并）</span><br><span class="hljs-built_in">cudaStreamBeginCapture</span>(stream2);<br><span class="hljs-built_in">cudaStreamWaitEvent</span>(stream2, event1); <span class="hljs-comment">// 错误！event1属于不同图</span><br></code></pre></td></tr></table></figure><p>可以按照如下流程处理失效的情况：</p><p><img src="%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png" alt="处理流程"></p><p>最后需要注意的是，图定义对象（<code>cudaGraph_t</code>）不能被多线程同时访问（包括创建/修改/销毁）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaGraph_t graph;<br><br><span class="hljs-comment">// 线程1</span><br><span class="hljs-built_in">cudaGraphAddKernelNode</span>(&amp;node1, graph, ...);<br><br><span class="hljs-comment">// 线程2（同时操作同一graph）</span><br><span class="hljs-built_in">cudaGraphAddMemcpyNode</span>(&amp;node2, graph, ...); <span class="hljs-comment">// 危险！</span><br></code></pre></td></tr></table></figure><p>这时候需要使用线程互斥锁保护图对象操作：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++">std::mutex graph_mutex;<br>&#123;<br>    <span class="hljs-function">std::lock_guard&lt;std::mutex&gt; <span class="hljs-title">lock</span><span class="hljs-params">(graph_mutex)</span></span>;<br>    <span class="hljs-built_in">cudaGraphAddKernelNode</span>(&amp;node, graph, ...);<br>&#125;<br></code></pre></td></tr></table></figure><p>可执行图实例（<code>cudaGraphExec_t</code>）不能并发执行同一实例,连续启动同一可执行图时，CUDA保证</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++">Launch <span class="hljs-number">1</span> → Launch <span class="hljs-number">2</span> → Launch <span class="hljs-number">3</span> （严格顺序）<br></code></pre></td></tr></table></figure><p>即使使用不同流，也无法实现同一图实例的并行执行：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaGraphLaunch</span>(execGraph, stream1);  <span class="hljs-comment">// 第一次启动</span><br><span class="hljs-built_in">cudaGraphLaunch</span>(execGraph, stream2);  <span class="hljs-comment">// 阻塞直到第一次完成</span><br></code></pre></td></tr></table></figure><p>可以采用如下的方式实现多线程共享图：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 主线程构建图</span><br>cudaGraph_t graph;<br><span class="hljs-built_in">buildGraph</span>(&amp;graph);<br><br><span class="hljs-comment">// 子线程使用（只读）</span><br>cudaGraphExec_t localExec;<br><span class="hljs-built_in">cudaGraphInstantiate</span>(&amp;localExec, graph, ...); <span class="hljs-comment">// 每个线程独立实例化</span><br><span class="hljs-built_in">cudaGraphLaunch</span>(localExec, localStream);<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaGraphExec_t execGraph1, execGraph2;<br><span class="hljs-built_in">cudaGraphInstantiate</span>(&amp;execGraph1, graph, ...);<br><span class="hljs-built_in">cudaGraphInstantiate</span>(&amp;execGraph2, graph, ...); <span class="hljs-comment">// 创建副本</span><br><br><span class="hljs-comment">// 真正并行执行</span><br><span class="hljs-built_in">cudaGraphLaunch</span>(execGraph1, stream1);<br><span class="hljs-built_in">cudaGraphLaunch</span>(execGraph2, stream2);<br></code></pre></td></tr></table></figure><p>CUDA 事件(Event)是 CUDA 编程中用于监控设备执行进度和精确计时的机制。它们允许应用程序在程序中的任意点异步记录事件，并查询这些事件的完成状态。</p><ul><li>事件完成意味着所有在该事件之前提交的任务（或指定流中的所有命令）已完成</li><li>流0(默认流)中的事件在所有流中所有前置任务和命令完成后才标记为完成</li><li>主要用于：计时操作、同步点、性能分析</li></ul><p><code>cudaEventCreate</code> 函数创建一个新的事件对象，可以通过 <code>cudaEvent_t</code> 类型的变量引用:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaEvent_t start, stop;<br><span class="hljs-built_in">cudaEventCreate</span>(&amp;start);<br><span class="hljs-built_in">cudaEventCreate</span>(&amp;stop);<br></code></pre></td></tr></table></figure><p>当不再需要事件时，应该销毁它以释放相关资源:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaEventDestroy</span>(start);<br><span class="hljs-built_in">cudaEventDestroy</span>(stop);<br></code></pre></td></tr></table></figure><p>事件最常用的场景是测量代码段的执行时间，特别是异步操作的执行时间:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 记录开始事件</span><br><span class="hljs-built_in">cudaEventRecord</span>(start, <span class="hljs-number">0</span>);  <span class="hljs-comment">// 0表示默认流</span><br><br><span class="hljs-comment">// 执行要计时的代码</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">2</span>; ++i) &#123;<br>    <span class="hljs-built_in">cudaMemcpyAsync</span>(inputDev + i * size, inputHost + i * size,<br>                   size, cudaMemcpyHostToDevice, stream[i]);<br>    MyKernel&lt;&lt;&lt;<span class="hljs-number">100</span>, <span class="hljs-number">512</span>, <span class="hljs-number">0</span>, stream[i]&gt;&gt;&gt;<br>              (outputDev + i * size, inputDev + i * size, size);<br>    <span class="hljs-built_in">cudaMemcpyAsync</span>(outputHost + i * size, outputDev + i * size,<br>                   size, cudaMemcpyDeviceToHost, stream[i]);<br>&#125;<br><br><span class="hljs-comment">// 记录结束事件</span><br><span class="hljs-built_in">cudaEventRecord</span>(stop, <span class="hljs-number">0</span>);<br><br><span class="hljs-comment">// 等待事件完成</span><br><span class="hljs-built_in">cudaEventSynchronize</span>(stop);<br><br><span class="hljs-comment">// 计算时间差</span><br><span class="hljs-type">float</span> elapsedTime;<br><span class="hljs-built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);<br></code></pre></td></tr></table></figure><h3 id="Multi-Device-System（多设备系统）">Multi-Device System（多设备系统）</h3><p>在具有多个GPU的系统中，CUDA提供了设备枚举功能来识别和查询可用设备：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-type">int</span> deviceCount;<br><span class="hljs-built_in">cudaGetDeviceCount</span>(&amp;deviceCount);  <span class="hljs-comment">// 获取系统中CUDA设备数量</span><br><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> device = <span class="hljs-number">0</span>; device &lt; deviceCount; ++device) &#123;<br>    cudaDeviceProp deviceProp;<br>    <span class="hljs-built_in">cudaGetDeviceProperties</span>(&amp;deviceProp, device);  <span class="hljs-comment">// 获取设备属性</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Device %d: %s\n&quot;</span>, device, deviceProp.name);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;  Compute Capability: %d.%d\n&quot;</span>, <br>           deviceProp.major, deviceProp.minor);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;  Total Global Memory: %.2f GB\n&quot;</span>, <br>           deviceProp.totalGlobalMem/<span class="hljs-number">1024.0</span>/<span class="hljs-number">1024.0</span>/<span class="hljs-number">1024.0</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>每个主机线程有&quot;当前设备&quot;的概念，内存分配和内核启动都在当前设备上执行，默认当前设备是设备0：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">//使用cudaSetDevice切换设备</span><br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">0</span>);  <span class="hljs-comment">// 切换到设备0</span><br><span class="hljs-type">float</span>* d_data0;<br><span class="hljs-built_in">cudaMalloc</span>(&amp;d_data0, size);  <span class="hljs-comment">// 在设备0上分配内存</span><br><br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">1</span>);  <span class="hljs-comment">// 切换到设备1</span><br><span class="hljs-type">float</span>* d_data1;<br><span class="hljs-built_in">cudaMalloc</span>(&amp;d_data1, size);  <span class="hljs-comment">// 在设备1上分配内存</span><br></code></pre></td></tr></table></figure><p>涉及到对于流的处理时，如果内核启动是向未与当前设备关联的流发出的，则内核启动将失败：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">0</span>); <span class="hljs-comment">// Set device 0 as current</span><br>cudaStream_t s0;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;s0); <span class="hljs-comment">// Create stream s0 on device 0</span><br>MyKernel&lt;&lt;&lt;<span class="hljs-number">100</span>, <span class="hljs-number">64</span>, <span class="hljs-number">0</span>, s0&gt;&gt;&gt;(); <span class="hljs-comment">// Launch kernel on device 0 in s0</span><br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">1</span>); <span class="hljs-comment">// Set device 1 as current</span><br>cudaStream_t s1;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;s1); <span class="hljs-comment">// Create stream s1 on device 1</span><br>MyKernel&lt;&lt;&lt;<span class="hljs-number">100</span>, <span class="hljs-number">64</span>, <span class="hljs-number">0</span>, s1&gt;&gt;&gt;(); <span class="hljs-comment">// Launch kernel on device 1 in s1</span><br><span class="hljs-comment">// This kernel launch will fail:</span><br>MyKernel&lt;&lt;&lt;<span class="hljs-number">100</span>, <span class="hljs-number">64</span>, <span class="hljs-number">0</span>, s0&gt;&gt;&gt;(); <span class="hljs-comment">// Launch kernel on device 1 in s0</span><br></code></pre></td></tr></table></figure><p>对于跨设备操作，遵循以下的规则：</p><table><thead><tr><th style="text-align:left">操作</th><th style="text-align:left">跨设备行为</th></tr></thead><tbody><tr><td style="text-align:left">cudaEventRecord</td><td style="text-align:left">事件和流必须在同一设备</td></tr><tr><td style="text-align:left">cudaEventElapsedTime</td><td style="text-align:left">两个事件必须在同一设备</td></tr><tr><td style="text-align:left">cudaEventSynchronize/Query</td><td style="text-align:left">支持跨设备操作</td></tr><tr><td style="text-align:left">cudaStreamWaitEvent</td><td style="text-align:left">支持跨设备同步</td></tr></tbody></table><p>对于支持PCIe拓扑或NVLink连接的设备之间可以进行点对点访问：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-type">int</span> canAccessPeer;<br><span class="hljs-built_in">cudaDeviceCanAccessPeer</span>(&amp;canAccessPeer, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>);  <span class="hljs-comment">// 检查设备0能否访问设备1</span><br><br><span class="hljs-keyword">if</span> (canAccessPeer) &#123;<br>    <span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">0</span>);<br>    <span class="hljs-built_in">cudaDeviceEnablePeerAccess</span>(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>);  <span class="hljs-comment">// 设备0启用对设备1的访问</span><br>    <br>    <span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">1</span>);<br>    <span class="hljs-built_in">cudaDeviceEnablePeerAccess</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);  <span class="hljs-comment">// 设备1启用对设备0的访问</span><br>&#125;<br></code></pre></td></tr></table></figure><p>P2P内存访问可直接访问对等设备内存，避免通过主机内存中转，使得指针在不同设备间保持有效性。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 设备0上分配内存</span><br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">0</span>);<br><span class="hljs-type">float</span>* p0;<br><span class="hljs-built_in">cudaMalloc</span>(&amp;p0, size);<br><br><span class="hljs-comment">// 设备1上直接使用设备0的内存</span><br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">1</span>);<br>MyKernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(p0);  <span class="hljs-comment">// 直接访问设备0的内存</span><br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 传统方法(通过主机中转)</span><br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">0</span>);<br><span class="hljs-built_in">cudaMemcpy</span>(hostPtr, dev0Ptr, size, cudaMemcpyDeviceToHost);<br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">1</span>);<br><span class="hljs-built_in">cudaMemcpy</span>(dev1Ptr, hostPtr, size, cudaMemcpyHostToDevice);<br><br><span class="hljs-comment">// P2P直接拷贝</span><br><span class="hljs-built_in">cudaMemcpyPeer</span>(dev1Ptr, <span class="hljs-number">1</span>, dev0Ptr, <span class="hljs-number">0</span>, size);  <span class="hljs-comment">// 设备0到设备1的直接拷贝</span><br></code></pre></td></tr></table></figure><p>异步拷贝方式如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaMemcpyPeerAsync</span>(dest, destDevice, src, srcDevice, size, stream);<br></code></pre></td></tr></table></figure><p>在Linux系统上应禁用IOMMU以获得最佳P2P性能，IOMMU（Input-Output Memory Management Unit）是一种硬件功能，它类似于CPU中的MMU（内存管理单元），但专门为I/O设备设计，用于管理设备对系统内存的访问。</p><table><thead><tr><th style="text-align:left">特性</th><th style="text-align:left">MMU</th><th style="text-align:left">IOMMU</th></tr></thead><tbody><tr><td style="text-align:left">服务对象</td><td style="text-align:left">CPU</td><td style="text-align:left">I/O设备</td></tr><tr><td style="text-align:left">主要功能</td><td style="text-align:left">虚拟地址→物理地址转换</td><td style="text-align:left">设备地址→物理地址转换</td></tr><tr><td style="text-align:left">保护目标</td><td style="text-align:left">进程间内存隔离</td><td style="text-align:left">设备间内存隔离</td></tr></tbody></table></li></ol><p>统一地址空间是CUDA架构中一项重要特性，它从根本上简化了多设备系统中的内存管理，在64位进程中，CUDA创建一个跨越<strong>主机内存</strong>和<strong>所有计算能力2.0+设备内存</strong>的单一虚拟地址空间：</p><ul><li>每个内存地址在系统中具有唯一性</li><li>指针值本身包含位置信息（主机/设备）</li><li>地址范围：通常为40位或48位虚拟地址空间（取决于GPU架构）</li></ul><p>利用GPU同一地址空间可以实现：</p><p>**指针属性的查询：**通过<code>cudaPointerGetAttributes()</code>可确定指针的实际位置</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaPointerAttributes attributes;<br><span class="hljs-built_in">cudaPointerGetAttributes</span>(&amp;attributes, ptr);<br><br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Memory type: %s\n&quot;</span>, <br>       attributes.type == cudaMemoryTypeHost ? <span class="hljs-string">&quot;Host&quot;</span> :<br>       attributes.type == cudaMemoryTypeDevice ? <span class="hljs-string">&quot;Device&quot;</span> : <span class="hljs-string">&quot;Managed&quot;</span>);<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Device ID: %d\n&quot;</span>, attributes.device);<br></code></pre></td></tr></table></figure><p>**智能内存的拷贝：**使用<code>cudaMemcpyDefault</code>自动判断拷贝方</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 自动识别src和dst位置（主机↔设备/设备↔设备）</span><br><span class="hljs-built_in">cudaMemcpy</span>(dst, src, size, cudaMemcpyDefault);<br></code></pre></td></tr></table></figure><p>便携式内存分配<code>cudaHostAlloc</code>分配的内存自动支持</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">float</span> *h_data;<br><span class="hljs-built_in">cudaHostAlloc</span>(&amp;h_data, size, cudaHostAllocDefault);<br><br><span class="hljs-comment">// 所有支持统一地址空间的设备均可直接访问</span><br>MyKernel&lt;&lt;&lt;...&gt;&gt;&gt;(h_data);  <span class="hljs-comment">// 无需获取设备指针</span><br></code></pre></td></tr></table></figure><p>和Linux的进程间通信一样，CUDA IPC（Inter-Process Communication）是一组允许不同进程间共享GPU内存和事件的高级功能，对于同一进程的所有线程天然共享设备指针和事件句柄，不同进程的地址空间相互独立，需显式IPC机制。</p><p>需要注意的是，<code>cudaMallocManaged</code>分配的内存不支持IPC，通信进程必须使用相同版本的CUDA驱动和运行时。</p><p>可以通过<strong>内存共享</strong>和事件共享来实现IPC通信：</p><p>对于<strong>内存共享</strong>，在GPU页表中建立跨进程映射，使用唯一的IPC句柄替代裸指针，同时采用引用计数，防止过早释放共享内存。</p><p>发送方进程执行：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">float</span>* d_data;<br><span class="hljs-built_in">cudaMalloc</span>(&amp;d_data, size);  <span class="hljs-comment">// 分配设备内存</span><br><br>cudaIpcMemHandle_t handle;<br><span class="hljs-built_in">cudaIpcGetMemHandle</span>(&amp;handle, d_data);  <span class="hljs-comment">// 获取IPC句柄</span><br><br><span class="hljs-comment">// 通过OS机制传递handle（如共享内存、管道、文件等）</span><br><span class="hljs-built_in">write_to_ipc_channel</span>(&amp;handle, <span class="hljs-built_in">sizeof</span>(handle));<br></code></pre></td></tr></table></figure><p>接收方进程执行：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaIpcMemHandle_t handle;<br><span class="hljs-built_in">read_from_ipc_channel</span>(&amp;handle, <span class="hljs-built_in">sizeof</span>(handle));  <span class="hljs-comment">// 获取句柄</span><br><br><span class="hljs-type">float</span>* foreign_d_data;<br><span class="hljs-built_in">cudaIpcOpenMemHandle</span>(&amp;foreign_d_data, handle, <br>                    cudaIpcMemLazyEnablePeerAccess);  <span class="hljs-comment">// 打开句柄</span><br><br><span class="hljs-comment">// 现在foreign_d_data可当作本地设备指针使用</span><br></code></pre></td></tr></table></figure><p>可以指定Handle的打开模式：</p><table><thead><tr><th style="text-align:left">模式标志</th><th style="text-align:left">说明</th></tr></thead><tbody><tr><td style="text-align:left"><code>cudaIpcMemLazyEnablePeerAccess</code></td><td style="text-align:left">按需自动建立P2P访问</td></tr><tr><td style="text-align:left"><code>cudaIpcMemLazyDisablePeerAccess</code></td><td style="text-align:left">禁用P2P访问</td></tr></tbody></table><p>对于<strong>事件共享</strong>，在设备层面创建跨进程可见对象，通过GPU硬件信号实现跨进程同步。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 进程A创建事件并共享</span><br>cudaEvent_t event;<br><span class="hljs-built_in">cudaEventCreate</span>(&amp;event, cudaEventInterprocess);<br>cudaIpcEventHandle_t event_handle;<br><span class="hljs-built_in">cudaIpcGetEventHandle</span>(&amp;event_handle, event);<br><br><span class="hljs-comment">// 进程B打开事件</span><br>cudaEvent_t foreign_event;<br><span class="hljs-built_in">cudaIpcOpenEventHandle</span>(&amp;foreign_event, event_handle);<br></code></pre></td></tr></table></figure><p>必须使用<code>cudaEventInterprocess</code>标志创建事件，此时的共享事件只能用于同步，而不能用于计时。</p><h3 id="CUDA-Error-Checking">CUDA Error Checking</h3><p>所有CUDA Runtime函数运行失败都会返回错误码，对于异步函数，因为该函数在设备完成任务之前返回，错误代码仅仅报告在运行任务之前host上发生的错误，不可能报告device上任何异步错误，需要后续进行同步操作检测。</p><p>对于同步函数：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaError_t err = <span class="hljs-built_in">cudaMemcpy</span>(dst, src, size, cudaMemcpyHostToDevice);<br><span class="hljs-comment">// 错误立即可知，包含设备执行错误</span><br></code></pre></td></tr></table></figure><p>对于异步函数：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++">MyKernel&lt;&lt;&lt;...&gt;&gt;&gt;(); <span class="hljs-comment">// 启动不返回错误</span><br><span class="hljs-comment">// 仅后续同步才能发现内核执行错误</span><br></code></pre></td></tr></table></figure><p>CUDA Errorshiyong如下API进行检测：</p><table><thead><tr><th style="text-align:left">函数</th><th style="text-align:left">返回值</th><th style="text-align:left">副作用</th><th style="text-align:left">适用场景</th></tr></thead><tbody><tr><td style="text-align:left"><code>cudaPeekAtLastError()</code></td><td style="text-align:left">当前错误码</td><td style="text-align:left">无</td><td style="text-align:left">检查错误但不破坏错误上下文</td></tr><tr><td style="text-align:left"><code>cudaGetLastError()</code></td><td style="text-align:left">当前错误码</td><td style="text-align:left">重置为cudaSuccess</td><td style="text-align:left">标准错误处理流程</td></tr></tbody></table><p>对于标准的内核错误，参照如下方式进行：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 清除之前可能存在的错误</span><br><span class="hljs-built_in">cudaGetLastError</span>();<br><br><span class="hljs-comment">// 启动内核</span><br>MyKernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(...);<br><br><span class="hljs-comment">// 检查启动参数错误</span><br>cudaError_t launchErr = <span class="hljs-built_in">cudaPeekAtLastError</span>();<br><span class="hljs-keyword">if</span> (launchErr != cudaSuccess) &#123;<br>    <span class="hljs-built_in">handleLaunchError</span>(launchErr);<br>&#125;<br><br><span class="hljs-comment">// 同步并检查执行错误</span><br>cudaError_t syncErr = <span class="hljs-built_in">cudaDeviceSynchronize</span>();<br><span class="hljs-keyword">if</span> (syncErr != cudaSuccess) &#123;<br>    <span class="hljs-built_in">handleRuntimeError</span>(syncErr);<br>&#125;<br></code></pre></td></tr></table></figure><p>对于流式错误检测：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs C++">cudaStream_t stream;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream);<br><br><span class="hljs-comment">// 启动多个异步操作</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; N; ++i) &#123;<br>    <span class="hljs-built_in">cudaGetLastError</span>(); <span class="hljs-comment">// 清除错误状态</span><br>    Kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(...);<br>    cudaError_t err = <span class="hljs-built_in">cudaStreamQuery</span>(stream);<br>    <span class="hljs-keyword">if</span> (err == cudaErrorNotReady) &#123;<br>        <span class="hljs-comment">// 正常情况，操作未完成</span><br>    &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (err != cudaSuccess) &#123;<br>        <span class="hljs-comment">// 真实错误处理</span><br>    &#125;<br>&#125;<br><br><span class="hljs-comment">// 最终错误检查</span><br>cudaError_t finalErr = <span class="hljs-built_in">cudaStreamSynchronize</span>(stream);<br></code></pre></td></tr></table></figure><p>如何处理错误，在CUDA 8+中定义了错误回调的机制：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> __host__ <span class="hljs-title">errorCallback</span><span class="hljs-params">(cudaStream_t stream, cudaError_t status, <span class="hljs-type">void</span>* userData)</span> </span>&#123;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Async error detected: %s\n&quot;</span>, <span class="hljs-built_in">cudaGetErrorString</span>(status));<br>&#125;<br><br><span class="hljs-comment">// 注册回调</span><br><span class="hljs-built_in">cudaStreamAddCallback</span>(stream, errorCallback, <span class="hljs-literal">nullptr</span>, <span class="hljs-number">0</span>);<br></code></pre></td></tr></table></figure><h3 id="Texture-and-Surface-Memory">Texture and Surface Memory</h3><p>纹理内存（texture memory）和表面内存（surface memory）类似于常量内存，是有缓存的全局变量，可见范围和生命周期一样，一般仅可读（表面内存可写），但内存容量更大。</p><h4 id="Texture-Memory">Texture Memory</h4><p>在旧式纹理API中使用如下方式静态引用纹理：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 声明纹理引用（必须在文件作用域）</span><br>texture&lt;<span class="hljs-type">float</span>, <span class="hljs-number">2</span>, cudaReadModeElementType&gt; texRef;<br><br><span class="hljs-comment">// 运行时绑定</span><br>cudaChannelFormatDesc desc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;<span class="hljs-type">float</span>&gt;();<br><span class="hljs-built_in">cudaBindTexture2D</span>(<span class="hljs-number">0</span>, texRef, devPtr, desc, width, height, pitch);<br><br><span class="hljs-comment">// 内核中使用</span><br><span class="hljs-type">float</span> val = <span class="hljs-built_in">tex2D</span>(texRef, x, y);<br></code></pre></td></tr></table></figure><p>CUDA3.0+可以在运行时动态创建纹理：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaTextureDesc</span><br>&#123;<br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureAddressMode</span> addressMode[<span class="hljs-number">3</span>];<br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureFilterMode</span> filterMode;<br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureReadMode</span> readMode;<br>    <span class="hljs-type">int</span> sRGB;<br>    <span class="hljs-type">int</span> normalizedCoords;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> maxAnisotropy;<br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureFilterMode</span> mipmapFilterMode;<br>    <span class="hljs-type">float</span> mipmapLevelBias;<br>    <span class="hljs-type">float</span> minMipmapLevelClamp;<br>    <span class="hljs-type">float</span> maxMipmapLevelClamp;<br>&#125;;<br><br><br><br><span class="hljs-comment">// 创建纹理对象</span><br>cudaResourceDesc resDesc;<br><span class="hljs-built_in">memset</span>(&amp;resDesc, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(resDesc));<br>resDesc.resType = cudaResourceTypeLinear;<br>resDesc.res.linear.devPtr = devPtr;<br>resDesc.res.linear.desc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;<span class="hljs-type">float</span>&gt;();<br>resDesc.res.linear.sizeInBytes = width * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br><br>cudaTextureDesc texDesc;<br><span class="hljs-built_in">memset</span>(&amp;texDesc, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(texDesc));<br>texDesc.addressMode[<span class="hljs-number">0</span>] = cudaAddressModeClamp;<br>texDesc.filterMode = cudaFilterModeLinear;<br>texDesc.readMode = cudaReadModeElementType;<br><br>cudaTextureObject_t texObj;<br><span class="hljs-built_in">cudaCreateTextureObject</span>(&amp;texObj, &amp;resDesc, &amp;texDesc, <span class="hljs-literal">NULL</span>);<br><br><span class="hljs-comment">// 内核中使用</span><br><span class="hljs-type">float</span> val = <span class="hljs-built_in">tex1Dfetch</span>&lt;<span class="hljs-type">float</span>&gt;(texObj, x);<br></code></pre></td></tr></table></figure><p>下面一段代码展示了利用纹理对象实现图像旋转的操作：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span></span><br><br><span class="hljs-comment">// rotate kernel</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">transformKernel</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, cudaTextureObject_t textObj, <span class="hljs-type">int</span> width, <span class="hljs-type">int</span> height, <span class="hljs-type">float</span> theta)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-comment">//归一化 纹理采样前的标准预处理步骤</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;<br><br>    <span class="hljs-type">float</span> u = x / (<span class="hljs-type">float</span>)width;<br>    <span class="hljs-type">float</span> v = y / (<span class="hljs-type">float</span>)height;<br><br>    <span class="hljs-comment">//旋转中心</span><br>    u -= <span class="hljs-number">0.5f</span>;<br>    v -= <span class="hljs-number">0.5f</span>;<br>    <span class="hljs-type">float</span> tu = u * <span class="hljs-built_in">cosf</span>(theta) - v * <span class="hljs-built_in">sinf</span>(theta) + <span class="hljs-number">0.5f</span>;<br>    <span class="hljs-type">float</span> tv = v * <span class="hljs-built_in">cosf</span>(theta) - u * <span class="hljs-built_in">sinf</span>(theta) + <span class="hljs-number">0.5f</span>;<br><br>    <span class="hljs-comment">//read from texture to global memory</span><br>    <span class="hljs-comment">//tex2D是纹理采样函数</span><br>    output[y * width + x] = <span class="hljs-built_in">tex2D</span>&lt;<span class="hljs-type">float</span>&gt;(textObj, tu, tv);<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">float</span>* h_data;<br>    <span class="hljs-type">int</span> width, height;<br>    <span class="hljs-type">float</span> angle;<br>    <span class="hljs-type">size_t</span> size = width * height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br><br>    <span class="hljs-comment">//创建32位单通道浮点格式的CUDA数组</span><br>    <span class="hljs-comment">//CUDA数组是纹理内存的优化存储格式</span><br>    cudaChannelFormatDesc channelDesc = <br>                <span class="hljs-built_in">cudaCreateChannelDesc</span>(<span class="hljs-number">32</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, cudaChannelFormatKindFloat);<br>    <br>    cudaArray* cuArray;<br>    <span class="hljs-built_in">cudaMallocArray</span>(&amp;cuArray, &amp;channelDesc, width, height);<br><br>    <span class="hljs-comment">//copy host memory to device memory located at address h_data</span><br>    <span class="hljs-built_in">cudaMemcpyToArray</span>(cuArray, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, h_data, size, cudaMemcpyHostToDevice);<br><br>    <span class="hljs-comment">//texture</span><br>    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaResourceDesc</span> resDesc;<br>    <span class="hljs-built_in">memset</span>(&amp;resDesc, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(resDesc));<br>    resDesc.resType = cudaResourceTypeArray;<br>    resDesc.res.array.array = cuArray;<br><br>    <span class="hljs-comment">//texture object parameters</span><br>    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaTextureDesc</span> texDesc;<br>    <span class="hljs-built_in">memset</span>(&amp;texDesc, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(texDesc));<br>    texDesc.addressMode[<span class="hljs-number">0</span>] = cudaAddressModeWrap;<br>    texDesc.addressMode[<span class="hljs-number">1</span>] = cudaAddressModeWrap;<br>    texDesc.filterMode = cudaFilterModeLinear;<br>    texDesc.readMode = cudaReadModeElementType;<br>    texDesc.normalizedCoords = <span class="hljs-number">1</span>;<br><br>    <span class="hljs-comment">//creater texture object</span><br>    cudaTextureObject_t texobj = <span class="hljs-number">0</span>;<br>    <span class="hljs-built_in">cudaCreateTextureObject</span>(&amp;texobj, &amp;resDesc, &amp;texDesc, <span class="hljs-literal">NULL</span>);<br><br>    <span class="hljs-type">float</span>* output;<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;output, width * height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>));<br><br>    <span class="hljs-comment">//Invoke kernel</span><br>    <span class="hljs-function">dim3 <span class="hljs-title">dimBlock</span><span class="hljs-params">(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">dimGrid</span><span class="hljs-params">((width + dimBlock.x - <span class="hljs-number">1</span>) / dimBlock.x,</span></span><br><span class="hljs-params"><span class="hljs-function">                (height + dimBlock.y - <span class="hljs-number">1</span>) / dimBlock.y)</span></span>;<br><br>    transformKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt; (output, texobj, width, height, angle);<br><br>    <span class="hljs-comment">//destory texture obj</span><br>    <span class="hljs-built_in">cudaDestroyTextureObject</span>(texobj);<br><br>    <span class="hljs-comment">//free device memory</span><br>    <span class="hljs-built_in">cudaFreeArray</span>(cuArray);<br>    <span class="hljs-built_in">cudaFree</span>(output);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>纹理引用必须在文件作用域声明为静态全局变量，其基本语法为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++">texture&lt;DataType, Type, ReadMode&gt; texRef;<br></code></pre></td></tr></table></figure><ol><li><strong>DataType</strong>：指定纹素(texel)的数据类型</li><li><strong>Type</strong>（可选，默认为<code>cudaTextureType1D</code>）：指定纹理类型：<ul><li><code>cudaTextureType1D</code>：一维纹理</li><li><code>cudaTextureType2D</code>：二维纹理</li><li><code>cudaTextureType3D</code>：三维纹理</li><li><code>cudaTextureType1DLayered</code>：一维分层纹理</li><li><code>cudaTextureType2DLayered</code>：二维分层纹理</li></ul></li><li><strong>ReadMode</strong>（可选，默认为<code>cudaReadModeElementType</code>）：读取模式</li></ol><p>上述属性在编译时确定，不可更改，运行时可以更改的属性通过<code>textureReference</code>结构体定义：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">textureReference</span> &#123;<br>    <span class="hljs-type">int</span> normalized;  <span class="hljs-comment">// 是否使用归一化坐标</span><br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureFilterMode</span> filterMode;  <span class="hljs-comment">// 滤波模式</span><br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureAddressMode</span> addressMode[<span class="hljs-number">3</span>];  <span class="hljs-comment">// 寻址模式</span><br>    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaChannelFormatDesc</span> channelDesc;  <span class="hljs-comment">// 通道格式描述</span><br>    <span class="hljs-type">int</span> sRGB;  <span class="hljs-comment">// 是否使用sRGB色彩空间</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> maxAnisotropy;  <span class="hljs-comment">// 各向异性过滤的最大值</span><br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureFilterMode</span> mipmapFilterMode;  <span class="hljs-comment">// Mipmap滤波模式</span><br>    <span class="hljs-type">float</span> mipmapLevelBias;  <span class="hljs-comment">// Mipmap级别偏置</span><br>    <span class="hljs-type">float</span> minMipmapLevelClamp;  <span class="hljs-comment">// 最小Mipmap级别钳制</span><br>    <span class="hljs-type">float</span> maxMipmapLevelClamp;  <span class="hljs-comment">// 最大Mipmap级别钳制</span><br>&#125;;<br></code></pre></td></tr></table></figure><p>在使用纹理引用前，必须将其绑定到内存或CUDA数组。</p><p>绑定到线性内存，使用低级API：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++">texture&lt;<span class="hljs-type">float</span>, cudaTextureType2D, cudaReadModeElementType&gt; texRef;<br>textureReference* texRefPtr;<br><span class="hljs-built_in">cudaGetTextureReference</span>(&amp;texRefPtr, &amp;texRef);<br>cudaChannelFormatDesc channelDesc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;<span class="hljs-type">float</span>&gt;();<br><span class="hljs-type">size_t</span> offset;<br><span class="hljs-built_in">cudaBindTexture2D</span>(&amp;offset, texRefPtr, devPtr, &amp;channelDesc, width, height, pitch);<br></code></pre></td></tr></table></figure><p>使用高级API：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++">texture&lt;<span class="hljs-type">float</span>, cudaTextureType2D, cudaReadModeElementType&gt; texRef;<br>cudaChannelFormatDesc channelDesc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;<span class="hljs-type">float</span>&gt;();<br><span class="hljs-type">size_t</span> offset;<br><span class="hljs-built_in">cudaBindTexture2D</span>(&amp;offset, texRef, devPtr, channelDesc, width, height, pitch);<br></code></pre></td></tr></table></figure><p>绑定到CUDA数组，使用低级API：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++">texture&lt;<span class="hljs-type">float</span>, cudaTextureType2D, cudaReadModeElementType&gt; texRef;<br>textureReference* texRefPtr;<br><span class="hljs-built_in">cudaGetTextureReference</span>(&amp;texRefPtr, &amp;texRef);<br>cudaChannelFormatDesc channelDesc;<br><span class="hljs-built_in">cudaGetChannelDesc</span>(&amp;channelDesc, cuArray);<br><span class="hljs-built_in">cudaBindTextureToArray</span>(texRef, cuArray, &amp;channelDesc);<br></code></pre></td></tr></table></figure><p>使用高级API：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++">texture&lt;<span class="hljs-type">float</span>, cudaTextureType2D, cudaReadModeElementType&gt; texRef;<br><span class="hljs-built_in">cudaBindTextureToArray</span>(texRef, cuArray);<br></code></pre></td></tr></table></figure><p>使用如下命令解绑纹理：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaUnbindTexture</span>(texRef);<br></code></pre></td></tr></table></figure><p>对于上述旋转变换逻辑，使用纹理引用的代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 2D float texture</span><br>texture&lt;<span class="hljs-type">float</span>, cudaTextureType2D, cudaReadModeElementType&gt; texRef;<br><br><span class="hljs-comment">// Simple transformation kernel</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">transformKernel</span><span class="hljs-params">(<span class="hljs-type">float</span>* output,</span></span><br><span class="hljs-params"><span class="hljs-function">                               <span class="hljs-type">int</span> width, </span></span><br><span class="hljs-params"><span class="hljs-function">                               <span class="hljs-type">int</span> height,</span></span><br><span class="hljs-params"><span class="hljs-function">                               <span class="hljs-type">float</span> theta)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-comment">// Calculate normalized texture coordinates</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;<br>    <span class="hljs-type">float</span> u = x / (<span class="hljs-type">float</span>)width;<br>    <span class="hljs-type">float</span> v = y / (<span class="hljs-type">float</span>)height;<br><br>    <span class="hljs-comment">// Transform coordinates</span><br>    u -= <span class="hljs-number">0.5f</span>;<br>    v -= <span class="hljs-number">0.5f</span>;<br>    <span class="hljs-type">float</span> tu = u * <span class="hljs-built_in">cosf</span>(theta) - v * <span class="hljs-built_in">sinf</span>(theta) + <span class="hljs-number">0.5f</span>;<br>    <span class="hljs-type">float</span> tv = v * <span class="hljs-built_in">cosf</span>(theta) + u * <span class="hljs-built_in">sinf</span>(theta) + <span class="hljs-number">0.5f</span>;<br><br>    <span class="hljs-comment">// Read from texture and write to global memory</span><br>    output[y * width + x] = <span class="hljs-built_in">tex2D</span>(texRef, tu, tv);<br>&#125;<br><br><span class="hljs-comment">// Host code</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-comment">// Allocate CUDA array in device memory</span><br>    cudaChannelFormatDesc channelDesc =<br>        <span class="hljs-built_in">cudaCreateChannelDesc</span>(<span class="hljs-number">32</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,<br>                             cudaChannelFormatKindFloat);<br>    cudaArray* cuArray;<br>    <span class="hljs-built_in">cudaMallocArray</span>(&amp;cuArray, &amp;channelDesc, width, height);<br><br>    <span class="hljs-comment">// Copy to device memory some data located at address h_data</span><br>    <span class="hljs-comment">// in host memory</span><br>    <span class="hljs-built_in">cudaMemcpyToArray</span>(cuArray, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, h_data, size,<br>                     cudaMemcpyHostToDevice);<br><br>    <span class="hljs-comment">// Set texture reference parameters</span><br>    texRef.addressMode[<span class="hljs-number">0</span>] = cudaAddressModeWrap;<br>    texRef.addressMode[<span class="hljs-number">1</span>] = cudaAddressModeWrap;<br>    texRef.filterMode = cudaFilterModeLinear;<br>    texRef.normalized = <span class="hljs-literal">true</span>;<br><br>    <span class="hljs-comment">// Bind the array to the texture reference</span><br>    <span class="hljs-built_in">cudaBindTextureToArray</span>(texRef, cuArray, channelDesc);<br><br>    <span class="hljs-comment">// Allocate result of transformation in device memory</span><br>    <span class="hljs-type">float</span>* output;<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;output, width * height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>));<br><br>    <span class="hljs-comment">// Invoke kernel</span><br>    <span class="hljs-function">dim3 <span class="hljs-title">dimBlock</span><span class="hljs-params">(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">dimGrid</span><span class="hljs-params">((width + dimBlock.x - <span class="hljs-number">1</span>) / dimBlock.x,</span></span><br><span class="hljs-params"><span class="hljs-function">                (height + dimBlock.y - <span class="hljs-number">1</span>) / dimBlock.y)</span></span>;<br>    transformKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(output, width, height,<br>                                         angle);<br><br>    <span class="hljs-comment">// Free device memory</span><br>    <span class="hljs-built_in">cudaFreeArray</span>(cuArray);<br>    <span class="hljs-built_in">cudaFree</span>(output);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>CUDA C++没有直接的16位浮点数据类型，通过<code>unsigned short</code>类型进行转换，对于设备端的转换函数可以使用如下接口：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">unsigned</span> <span class="hljs-type">short</span> __float2half_rn(<span class="hljs-type">float</span> f);  <span class="hljs-comment">// 32位浮点转16位</span><br><span class="hljs-type">float</span> __half2float(<span class="hljs-type">unsigned</span> <span class="hljs-type">short</span> h);    <span class="hljs-comment">// 16位转32位浮点</span><br></code></pre></td></tr></table></figure><p>对于主机端转换使用OpenEXR库中的等效函数。</p><p>以下是一些高级纹理特性的介绍：</p><p>**分层纹理（Layered Textures）**也被称为纹理数组（Texture Array）由多个相同维度、大小和数据类型的常规纹理层组成。</p><p>一维分层纹理使用一个整数索引(层)和一个浮点坐标(层内位置)寻址；二维分层纹理使用一个整数索引(层)和两个浮点坐标(层内位置)寻址。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaMalloc3DArray</span>(&amp;cuArray, &amp;desc, <span class="hljs-built_in">make_cudaExtent</span>(width, height, <span class="hljs-number">0</span>), cudaArrayLayered);<br><br><span class="hljs-built_in">tex1DLayered</span>(texRef, x, layer);        <span class="hljs-comment">// 一维分层</span><br><span class="hljs-built_in">tex2DLayered</span>(texRef, x, y, layer);     <span class="hljs-comment">// 二维分层</span><br></code></pre></td></tr></table></figure><p>**立方体贴图（Cubemap Textures）**是特殊类型的二维分层纹理，有6层代表立方体的面，每层的宽度等于高度，使用三个坐标(x,y,z)作为方向向量进行寻址。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaMalloc3DArray</span>(&amp;cuArray, &amp;desc, <span class="hljs-built_in">make_cudaExtent</span>(size, size, <span class="hljs-number">0</span>), cudaArrayCubemap);<br><br><span class="hljs-built_in">texCubemap</span>(texRef, x, y, z);<br></code></pre></td></tr></table></figure><p>立方体分层纹理（Cubemap layered Textures）由多个立方体贴图组成的序列,使用整数索引(选择立方体)和三个浮点坐标(立方体内位置)寻址.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaMalloc3DArray</span>(&amp;cuArray, &amp;desc, <span class="hljs-built_in">make_cudaExtent</span>(size, size, layers), <br>                 cudaArrayLayered | cudaArrayCubemap);<br>                 <br><span class="hljs-built_in">texCubemapLayered</span>(texRef, x, y, z, layer);<br></code></pre></td></tr></table></figure><h4 id="Surface-Memory">Surface Memory</h4><p>表面内存(Surface Memory)是CUDA中一种特殊的内存访问机制，主要用于计算能力2.0及以上的设备。它提供了对CUDA数组的直接读写能力，与纹理内存相比，表面内存更注重于随机访问和写入操作。</p><p>表面内存通过表面对象(Surface Object)或表面引用(Surface Reference)访问，支持读写操作（纹理内存通常只读），CUDA数组必须使用<code>cudaArraySurfaceLoadStore</code>标志创建。</p><p>表面对象是使用 cudaCreateSurfaceObject（） 从结构 cudaResourceDesc 类型的资源描述中创建，以下是一个表面对象的创建方式：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function">cudaSurfaceObject_t <span class="hljs-title">createSurfaceObject</span><span class="hljs-params">(cudaArray* array)</span> </span>&#123;<br>    cudaResourceDesc resDesc;<br>    <span class="hljs-built_in">memset</span>(&amp;resDesc, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(resDesc));<br>    resDesc.resType = cudaResourceTypeArray;<br>    resDesc.res.array.array = array;<br>    <br>    cudaSurfaceObject_t surfObj;<br>    <span class="hljs-built_in">cudaCreateSurfaceObject</span>(&amp;surfObj, &amp;resDesc);<br>    <span class="hljs-keyword">return</span> surfObj;<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaResourceDesc</span> &#123;<br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaResourceType</span> resType;  <span class="hljs-comment">// 资源类型</span><br>    <br>    <span class="hljs-keyword">union</span> &#123;<br>        <span class="hljs-keyword">struct</span> &#123;<br>            cudaArray_t array;      <span class="hljs-comment">// CUDA数组</span><br>        &#125; array;<br>        <br>        <span class="hljs-keyword">struct</span> &#123;<br>            cudaMipmappedArray_t mipmap;  <span class="hljs-comment">// Mipmap数组</span><br>        &#125; mipmap;<br>        <br>        <span class="hljs-keyword">struct</span> &#123;<br>            <span class="hljs-type">void</span>* devPtr;          <span class="hljs-comment">// 设备指针</span><br>            <span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaChannelFormatDesc</span> desc;  <span class="hljs-comment">// 通道格式</span><br>            <span class="hljs-type">size_t</span> sizeInBytes;     <span class="hljs-comment">// 大小（字节）</span><br>        &#125; linear;<br>        <br>        <span class="hljs-keyword">struct</span> &#123;<br>            <span class="hljs-type">void</span>* devPtr;          <span class="hljs-comment">// 设备指针</span><br>            <span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaChannelFormatDesc</span> desc;  <span class="hljs-comment">// 通道格式</span><br>            <span class="hljs-type">size_t</span> width;         <span class="hljs-comment">// 宽度</span><br>            <span class="hljs-type">size_t</span> height;        <span class="hljs-comment">// 高度</span><br>            <span class="hljs-type">size_t</span> pitchInBytes;  <span class="hljs-comment">// 间距（字节）</span><br>        &#125; pitch2D;<br>    &#125; res;<br>&#125;;<br></code></pre></td></tr></table></figure><p>表面内存通过一组内置函数进行访问:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">//一维表面</span><br><span class="hljs-built_in">surf1Dread</span>(T* data, cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> x);<br><span class="hljs-built_in">surf1Dwrite</span>(T data, cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> x);<br><br><span class="hljs-comment">//二维表面</span><br><span class="hljs-built_in">surf2Dread</span>(T* data, cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y);<br><span class="hljs-built_in">surf2Dwrite</span>(T data, cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y);<br><br><span class="hljs-comment">//三维表面</span><br><span class="hljs-built_in">surf3Dread</span>(T* data, cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y, <span class="hljs-type">int</span> z);<br><span class="hljs-built_in">surf3Dwrite</span>(T data, cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y, <span class="hljs-type">int</span> z);<br><br><span class="hljs-comment">//表面对象销毁</span><br><span class="hljs-built_in">cudaDestroySurfaceObject</span>(cudaSurfaceObject_t surfObj);<br></code></pre></td></tr></table></figure><p>以下是一个使用示例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 创建CUDA数组</span><br>cudaArray* cuArray;<br>cudaChannelFormatDesc channelDesc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;<span class="hljs-type">float</span>&gt;();<br><span class="hljs-built_in">cudaMallocArray</span>(&amp;cuArray, &amp;channelDesc, width, height, cudaArraySurfaceLoadStore);<br><br><span class="hljs-comment">// 创建表面对象</span><br>cudaSurfaceObject_t surfObj = <span class="hljs-built_in">createSurfaceObject</span>(cuArray);<br><br><span class="hljs-comment">// 内核函数：写入表面</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">writeSurface</span><span class="hljs-params">(cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> width, <span class="hljs-type">int</span> height)</span> </span>&#123;<br>    <span class="hljs-type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;<br>    <br>    <span class="hljs-keyword">if</span> (x &lt; width &amp;&amp; y &lt; height) &#123;<br>        <span class="hljs-type">float</span> value = x + y * <span class="hljs-number">0.1f</span>;<br>        <span class="hljs-built_in">surf2Dwrite</span>(value, surfObj, x, y);<br>    &#125;<br>&#125;<br><br><span class="hljs-comment">// 调用内核</span><br><span class="hljs-function">dim3 <span class="hljs-title">blocks</span><span class="hljs-params">(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>)</span></span>;<br><span class="hljs-function">dim3 <span class="hljs-title">grids</span><span class="hljs-params">((width + blocks.x - <span class="hljs-number">1</span>) / blocks.x, (height + blocks.y - <span class="hljs-number">1</span>) / blocks.y)</span></span>;<br>writeSurface&lt;&lt;&lt;grids, blocks&gt;&gt;&gt;(surfObj, width, height);<br><br><span class="hljs-comment">// 清理</span><br><span class="hljs-built_in">cudaDestroySurfaceObject</span>(surfObj);<br><span class="hljs-built_in">cudaFreeArray</span>(cuArray);<br></code></pre></td></tr></table></figure><h3 id="图像互作性">图像互作性</h3><p>CUDA与OpenGL以及Direct3D等的互操作性允许GPU资源在图形渲染和通用计算之间共享，避免了数据在CPU内存中的来回拷贝，显著提高了异构计算的效率</p><p>操作流程如下：</p><ol><li><strong>资源注册</strong>：将OpenGL资源注册为CUDA可访问资源</li><li><strong>资源映射</strong>：将注册的资源映射到CUDA地址空间</li><li><strong>CUDA访问</strong>：通过CUDA内核读写资源</li><li><strong>资源解映射</strong>：解除CUDA对资源的访问</li><li><strong>OpenGL使用</strong>：OpenGL使用修改后的资源进行渲染</li><li><strong>资源注销</strong>：程序结束时注销资源</li></ol><p>与OpenGL的互作，关键API函数如下：</p><p>1.资源的注册</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaGraphicsGLRegisterBuffer</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    cudaGraphicsResource** resource,</span></span><br><span class="hljs-params"><span class="hljs-function">    GLuint buffer,</span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> flags)</span></span>;<br></code></pre></td></tr></table></figure><ul><li><code>resource</code>：返回的CUDA图形资源指针</li><li><code>buffer</code>：OpenGL缓冲区对象名称</li><li><code>flags</code>：注册标志，常用值：<ul><li><code>cudaGraphicsRegisterFlagsNone</code>：默认</li><li><code>cudaGraphicsRegisterFlagsReadOnly</code>：只读</li><li><code>cudaGraphicsRegisterFlagsWriteDiscard</code>：只写（丢弃原有内容）</li><li><code>cudaGraphicsRegisterFlagsSurfaceLoadStore</code>：允许表面读写</li></ul></li></ul><p>2.资源的映射与解映射</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-built_in">cudaGraphicsMapResources</span>(<span class="hljs-type">int</span> count, cudaGraphicsResource_t* resources, cudaStream_t stream);<br><span class="hljs-built_in">cudaGraphicsUnmapResources</span>(<span class="hljs-type">int</span> count, cudaGraphicsResource_t* resources, cudaStream_t stream);<br></code></pre></td></tr></table></figure><p>3.获取映射指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaGraphicsResourceGetMappedPointer</span>(<span class="hljs-type">void</span>** devPtr, <span class="hljs-type">size_t</span>* size, cudaGraphicsResource_t resource);<br></code></pre></td></tr></table></figure><p>以下是一个完整的示例，显示一个动态正弦波图案，由CUDA计算顶点位置并且通过OpenGL渲染：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;GL/glew.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;GL/freeglut.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cuda_runtime.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cuda_gl_interop.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-type">const</span> <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> WIDTH = <span class="hljs-number">800</span>;<br><span class="hljs-type">const</span> <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> HEIGHT = <span class="hljs-number">600</span>;<br><br>GLuint positionsVBO;<br>cudaGraphicsResource* positionsVBO_CUDA = <span class="hljs-literal">nullptr</span>;<br><span class="hljs-type">float</span> animTime = <span class="hljs-number">0.0f</span>;<br><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> CHECK_CUDA_ERROR(val) check((val), #val, __FILE__, __LINE__)</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">check</span><span class="hljs-params">(cudaError_t result, <span class="hljs-type">const</span> <span class="hljs-type">char</span>* <span class="hljs-type">const</span> func, <span class="hljs-type">const</span> <span class="hljs-type">char</span>* <span class="hljs-type">const</span> file, <span class="hljs-type">int</span> line)</span> </span>&#123;<br>    <span class="hljs-keyword">if</span> (result != cudaSuccess) &#123;<br>        std::cerr &lt;&lt; <span class="hljs-string">&quot;CUDA error at &quot;</span> &lt;&lt; file &lt;&lt; <span class="hljs-string">&quot;:&quot;</span> &lt;&lt; line &lt;&lt; <span class="hljs-string">&quot; code=&quot;</span> &lt;&lt; result <br>                  &lt;&lt; <span class="hljs-string">&quot; \&quot;&quot;</span> &lt;&lt; <span class="hljs-built_in">cudaGetErrorString</span>(result) &lt;&lt; <span class="hljs-string">&quot;\&quot; &quot;</span><br>                  &lt;&lt; func &lt;&lt; std::endl;<br>        <span class="hljs-built_in">exit</span>(EXIT_FAILURE);<br>    &#125;<br>&#125;<br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">createVertices</span><span class="hljs-params">(float4* positions, <span class="hljs-type">float</span> time, </span></span><br><span class="hljs-params"><span class="hljs-function">                             <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> width, <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> height)</span> </span>&#123;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;<br>    <br>    <span class="hljs-keyword">if</span> (x &lt; width &amp;&amp; y &lt; height) &#123;  <span class="hljs-comment">// 添加边界检查</span><br>        <span class="hljs-type">float</span> u = x / (<span class="hljs-type">float</span>)width;<br>        <span class="hljs-type">float</span> v = y / (<span class="hljs-type">float</span>)height;<br>        u = u * <span class="hljs-number">2.0f</span> - <span class="hljs-number">1.0f</span>;<br>        v = v * <span class="hljs-number">2.0f</span> - <span class="hljs-number">1.0f</span>;<br>        <br>        <span class="hljs-type">float</span> freq = <span class="hljs-number">4.0f</span>;<br>        <span class="hljs-type">float</span> w = <span class="hljs-built_in">sinf</span>(u * freq + time) * <span class="hljs-built_in">cosf</span>(v * freq + time) * <span class="hljs-number">0.5f</span>;<br>        <br>        positions[y * width + x] = <span class="hljs-built_in">make_float4</span>(u, w, v, <span class="hljs-number">1.0f</span>);<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">init</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-comment">// 初始化OpenGL缓冲区和CUDA互操作</span><br>    <span class="hljs-built_in">glClearColor</span>(<span class="hljs-number">0.0f</span>, <span class="hljs-number">0.0f</span>, <span class="hljs-number">0.0f</span>, <span class="hljs-number">1.0f</span>);<br>    <span class="hljs-built_in">glPointSize</span>(<span class="hljs-number">1.0f</span>);<br>    <br>    <span class="hljs-built_in">glGenBuffers</span>(<span class="hljs-number">1</span>, &amp;positionsVBO);<br>    <span class="hljs-built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, positionsVBO);<br>    <span class="hljs-built_in">glBufferData</span>(GL_ARRAY_BUFFER, WIDTH * HEIGHT * <span class="hljs-built_in">sizeof</span>(float4), <span class="hljs-literal">nullptr</span>, GL_DYNAMIC_DRAW);<br>    <span class="hljs-built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, <span class="hljs-number">0</span>);<br><br>    <span class="hljs-comment">// 检查设备是否支持CUDA-OpenGL互操作</span><br>    cudaDeviceProp prop;<br>    <span class="hljs-built_in">CHECK_CUDA_ERROR</span>(<span class="hljs-built_in">cudaGetDeviceProperties</span>(&amp;prop, <span class="hljs-number">0</span>));<br>    <span class="hljs-keyword">if</span> (!prop.canMapHostMemory || !prop.unifiedAddressing) &#123;<br>        std::cerr &lt;&lt; <span class="hljs-string">&quot;Device does not support required features for interop&quot;</span> &lt;&lt; std::endl;<br>        <span class="hljs-built_in">exit</span>(EXIT_FAILURE);<br>    &#125;<br><br>    <span class="hljs-built_in">CHECK_CUDA_ERROR</span>(<span class="hljs-built_in">cudaGraphicsGLRegisterBuffer</span>(&amp;positionsVBO_CUDA,<br>                                                positionsVBO,<br>                                                cudaGraphicsMapFlagsWriteDiscard));<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">display</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-comment">// 确保OpenGL上下文正确</span><br>    <span class="hljs-built_in">glutSetWindow</span>(<span class="hljs-built_in">glutGetWindow</span>());<br>    <br>    <span class="hljs-comment">// 解除所有可能影响互操作的OpenGL绑定</span><br>    <span class="hljs-built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, <span class="hljs-number">0</span>);<br>    <br>    <span class="hljs-comment">// 映射资源</span><br>    <span class="hljs-built_in">CHECK_CUDA_ERROR</span>(<span class="hljs-built_in">cudaGraphicsMapResources</span>(<span class="hljs-number">1</span>, &amp;positionsVBO_CUDA, <span class="hljs-number">0</span>));<br>    <br>    float4* d_positions = <span class="hljs-literal">nullptr</span>;<br>    <span class="hljs-type">size_t</span> num_bytes;<br>    <span class="hljs-built_in">CHECK_CUDA_ERROR</span>(<span class="hljs-built_in">cudaGraphicsResourceGetMappedPointer</span>(<br>        (<span class="hljs-type">void</span>**)&amp;d_positions, &amp;num_bytes, positionsVBO_CUDA));<br><br>    <span class="hljs-comment">// 执行内核</span><br>    <span class="hljs-function">dim3 <span class="hljs-title">block</span><span class="hljs-params">(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">grid</span><span class="hljs-params">((WIDTH + block.x - <span class="hljs-number">1</span>) / block.x, </span></span><br><span class="hljs-params"><span class="hljs-function">              (HEIGHT + block.y - <span class="hljs-number">1</span>) / block.y)</span></span>;<br>    createVertices&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_positions, animTime, WIDTH, HEIGHT);<br>    <br>    <span class="hljs-comment">// 确保内核执行完成</span><br>    <span class="hljs-built_in">CHECK_CUDA_ERROR</span>(<span class="hljs-built_in">cudaDeviceSynchronize</span>());<br>    <br>    <span class="hljs-comment">// 解映射资源</span><br>    <span class="hljs-built_in">CHECK_CUDA_ERROR</span>(<span class="hljs-built_in">cudaGraphicsUnmapResources</span>(<span class="hljs-number">1</span>, &amp;positionsVBO_CUDA, <span class="hljs-number">0</span>));<br>    <br>    <span class="hljs-comment">// 渲染</span><br>    <span class="hljs-built_in">glClear</span>(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);<br>    <span class="hljs-built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, positionsVBO);<br>    <span class="hljs-built_in">glVertexPointer</span>(<span class="hljs-number">4</span>, GL_FLOAT, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>);<br>    <span class="hljs-built_in">glEnableClientState</span>(GL_VERTEX_ARRAY);<br>    <span class="hljs-built_in">glDrawArrays</span>(GL_POINTS, <span class="hljs-number">0</span>, WIDTH * HEIGHT);<br>    <span class="hljs-built_in">glDisableClientState</span>(GL_VERTEX_ARRAY);<br>    <span class="hljs-built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, <span class="hljs-number">0</span>);  <span class="hljs-comment">// 解除绑定</span><br>    <br>    <span class="hljs-built_in">glutSwapBuffers</span>();<br>    <span class="hljs-built_in">glutPostRedisplay</span>();<br>    <br>    animTime += <span class="hljs-number">0.01f</span>;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">cleanup</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-keyword">if</span> (positionsVBO_CUDA) &#123;<br>        <span class="hljs-built_in">cudaGraphicsUnregisterResource</span>(positionsVBO_CUDA);<br>        positionsVBO_CUDA = <span class="hljs-literal">nullptr</span>;<br>    &#125;<br>    <br>    <span class="hljs-keyword">if</span> (positionsVBO) &#123;<br>        <span class="hljs-built_in">glDeleteBuffers</span>(<span class="hljs-number">1</span>, &amp;positionsVBO);<br>        positionsVBO = <span class="hljs-number">0</span>;<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">keyboard</span><span class="hljs-params">(<span class="hljs-type">unsigned</span> <span class="hljs-type">char</span> key, <span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y)</span> </span>&#123;<br>    <span class="hljs-keyword">if</span> (key == <span class="hljs-number">27</span>) &#123;  <span class="hljs-comment">// ESC键</span><br>        <span class="hljs-built_in">cleanup</span>();<br>        <span class="hljs-built_in">exit</span>(EXIT_SUCCESS);<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-type">int</span> argc, <span class="hljs-type">char</span>** argv)</span> </span>&#123;<br>    <span class="hljs-built_in">glutInit</span>(&amp;argc, argv);<br>    <span class="hljs-built_in">glutInitDisplayMode</span>(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH);<br>    <span class="hljs-built_in">glutInitWindowSize</span>(WIDTH, HEIGHT);<br>    <span class="hljs-type">int</span> window = <span class="hljs-built_in">glutCreateWindow</span>(<span class="hljs-string">&quot;CUDA-OpenGL Interop Demo&quot;</span>);<br>    <br>    <span class="hljs-comment">// 必须在创建窗口后初始化GLEW</span><br>    glewExperimental = GL_TRUE;<br>    GLenum err = <span class="hljs-built_in">glewInit</span>();<br>    <span class="hljs-keyword">if</span> (err != GLEW_OK) &#123;<br>        std::cerr &lt;&lt; <span class="hljs-string">&quot;GLEW init failed: &quot;</span> &lt;&lt; <span class="hljs-built_in">glewGetErrorString</span>(err) &lt;&lt; std::endl;<br>        <span class="hljs-keyword">return</span> EXIT_FAILURE;<br>    &#125;<br>    <br>    <span class="hljs-comment">// 检查必要扩展</span><br>    <span class="hljs-keyword">if</span> (!<span class="hljs-built_in">glewIsSupported</span>(<span class="hljs-string">&quot;GL_VERSION_2_0&quot;</span>)) &#123;<br>        std::cerr &lt;&lt; <span class="hljs-string">&quot;OpenGL 2.0 not supported&quot;</span> &lt;&lt; std::endl;<br>        <span class="hljs-keyword">return</span> EXIT_FAILURE;<br>    &#125;<br><br>    <span class="hljs-built_in">init</span>();<br>    <br>    <span class="hljs-built_in">glutDisplayFunc</span>(display);<br>    <span class="hljs-built_in">glutKeyboardFunc</span>(keyboard);<br>    <span class="hljs-built_in">glutCloseFunc</span>(cleanup);  <span class="hljs-comment">// 确保退出时清理</span><br>    <br>    <span class="hljs-built_in">glutMainLoop</span>();<br>    <span class="hljs-keyword">return</span> EXIT_SUCCESS;<br>&#125;<br></code></pre></td></tr></table></figure><p>确保安装OpenGL开发库(GLEW和FreeGLUT)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt-get update<br><span class="hljs-built_in">sudo</span> apt-get install -y libglew-dev freeglut3-dev<br></code></pre></td></tr></table></figure><p>使用如下编译命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvcc -o cuda_gl_interop cuda_gl_interop.cu -lGL -lGLEW -lglut<br></code></pre></td></tr></table></figure><p>CUDA支持的Direct3D版本：</p><table><thead><tr><th style="text-align:left">Direct3D版本</th><th style="text-align:left">关键创建参数</th></tr></thead><tbody><tr><td style="text-align:left">D3D9Ex</td><td style="text-align:left"><code>DeviceType = D3DDEVTYPE_HAL</code> <code>BehaviorFlags = D3DCREATE_HARDWARE_VERTEXPROCESSING</code></td></tr><tr><td style="text-align:left">D3D10/D3D11</td><td style="text-align:left"><code>DriverType = D3D_DRIVER_TYPE_HARDWARE</code></td></tr></tbody></table><p>使用如下注册函数：</p><table><thead><tr><th style="text-align:left">Direct3D版本</th><th style="text-align:left">注册函数</th></tr></thead><tbody><tr><td style="text-align:left">D3D9Ex</td><td style="text-align:left"><code>cudaGraphicsD3D9RegisterResource()</code></td></tr><tr><td style="text-align:left">D3D10</td><td style="text-align:left"><code>cudaGraphicsD3D10RegisterResource()</code></td></tr><tr><td style="text-align:left">D3D11</td><td style="text-align:left"><code>cudaGraphicsD3D11RegisterResource()</code></td></tr></tbody></table><p>在配备多个NVIDIA GPU的系统中，CUDA将每个物理GPU视为独立的计算设备（device），每个设备有独立的设备ID。但在SLI（Scalable Link Interface）配置下，GPU间的协同工作会带来特殊考量：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> deviceCount;<br><span class="hljs-built_in">cudaGetDeviceCount</span>(&amp;deviceCount); <span class="hljs-comment">// 获取系统中CUDA设备总数</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; deviceCount; i++) &#123;<br>    cudaDeviceProp prop;<br>    <span class="hljs-built_in">cudaGetDeviceProperties</span>(&amp;prop, i); <span class="hljs-comment">// 获取每个设备的详细属性</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;GPU %d: %s\n&quot;</span>, i, prop.name);<br>&#125;<br></code></pre></td></tr></table></figure><p>在SLI配置中，当通过Direct3D或OpenGL分配资源时，内存会在所有SLI组内的GPU上同步分配：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 典型的内存分配失败场景示例</span><br>cudaError_t err = <span class="hljs-built_in">cudaMalloc</span>(&amp;devPtr, size);<br><span class="hljs-keyword">if</span> (err == cudaErrorMemoryAllocation) &#123;<br>    <span class="hljs-comment">// 在SLI配置下可能比预期更早触发内存不足错误</span><br>&#125;<br></code></pre></td></tr></table></figure><table><thead><tr><th style="text-align:left">场景</th><th style="text-align:left">推荐设备选择方法</th><th style="text-align:left">优点</th><th style="text-align:left">缺点</th></tr></thead><tbody><tr><td style="text-align:left">单帧渲染</td><td style="text-align:left">cudaD3D11DeviceListCurrentFrame</td><td style="text-align:left">最小化数据传输</td><td style="text-align:left">需要频繁切换</td></tr><tr><td style="text-align:left">多帧并行</td><td style="text-align:left">轮询分配</td><td style="text-align:left">负载均衡</td><td style="text-align:left">增加同步复杂度</td></tr><tr><td style="text-align:left">数据并行</td><td style="text-align:left">固定设备分配</td><td style="text-align:left">实现简单</td><td style="text-align:left">可能造成瓶颈</td></tr></tbody></table><p>CUDA外部资源互操作性允许CUDA导入由其他API显式导出的资源，实现跨API的高效资源共享。这种机制避免了数据拷贝，特别适用于以下场景：</p><ul><li>与图形API（如OpenGL/Vulkan/Direct3D）共享纹理和缓冲区</li><li>与视频处理API（如NVDEC/NVENC）共享视频帧</li><li>实现多进程GPU资源共享</li><li>与计算框架（如OpenCL）进行互操作</li></ul><p><img src="%E5%86%85%E5%AD%98%E5%AF%BC%E5%85%A5%E6%B5%81%E7%A8%8B.png" alt="内存导入流程"></p><p>有如下关键的API：</p><p>1.内存导入：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaExternalMemoryHandleDesc desc = &#123;&#125;;<br>desc.type = cudaExternalMemoryHandleTypeOpaqueWin32;  <span class="hljs-comment">// Windows句柄类型</span><br>desc.handle.win<span class="hljs-number">32.</span>handle = externalHandle;            <span class="hljs-comment">// 外部句柄</span><br>desc.size = allocationSize;                           <span class="hljs-comment">// 内存大小</span><br><br>cudaExternalMemory_t extMem;<br><span class="hljs-built_in">cudaImportExternalMemory</span>(&amp;extMem, &amp;desc);<br></code></pre></td></tr></table></figure><p>2.获取映射缓冲区</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C++">cudaExternalMemoryBufferDesc bufDesc = &#123;&#125;;<br>bufDesc.offset = <span class="hljs-number">0</span>;      <span class="hljs-comment">// 内存偏移</span><br>bufDesc.size = dataSize;  <span class="hljs-comment">// 映射大小</span><br>bufDesc.flags = <span class="hljs-number">0</span>;        <span class="hljs-comment">// 标志位</span><br><br><span class="hljs-type">void</span>* devPtr;<br><span class="hljs-built_in">cudaExternalMemoryGetMappedBuffer</span>(&amp;devPtr, extMem, &amp;bufDesc);<br></code></pre></td></tr></table></figure><p>3.资源清理</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaFree</span>(devPtr);                   <span class="hljs-comment">// 先释放映射</span><br><span class="hljs-built_in">cudaDestroyExternalMemory</span>(extMem);  <span class="hljs-comment">// 再销毁外部内存</span><br></code></pre></td></tr></table></figure><p>进行同步对象互操作时，支持的类型如下：</p><table><thead><tr><th style="text-align:left">同步类型</th><th style="text-align:left">支持平台</th><th style="text-align:left">特性</th></tr></thead><tbody><tr><td style="text-align:left">Opaque FD</td><td style="text-align:left">Linux</td><td style="text-align:left">文件描述符形式</td></tr><tr><td style="text-align:left">Opaque Win32</td><td style="text-align:left">Windows</td><td style="text-align:left">NT句柄形式</td></tr><tr><td style="text-align:left">NVIDIA SCI</td><td style="text-align:left">跨平台</td><td style="text-align:left">高性能NVIDIA专用接口</td></tr></tbody></table><p>整体的信号量工作流程如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 导入信号量</span><br>cudaExternalSemaphoreHandleDesc semDesc = &#123;&#125;;<br>semDesc.type = cudaExternalSemaphoreHandleTypeOpaqueWin32;<br>semDesc.handle.win<span class="hljs-number">32.</span>handle = semaphoreHandle;<br><br>cudaExternalSemaphore_t extSem;<br><span class="hljs-built_in">cudaImportExternalSemaphore</span>(&amp;extSem, &amp;semDesc);<br><br><span class="hljs-comment">// 信号操作序列</span><br>cudaStream_t stream;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream);<br><br><span class="hljs-comment">// Vulkan完成写入后，CUDA等待信号</span><br>cudaExternalSemaphoreWaitParams waitParams = &#123;&#125;;<br>waitParams.params.fence.value = <span class="hljs-number">0</span>;  <span class="hljs-comment">// 同步值</span><br><span class="hljs-built_in">cudaWaitExternalSemaphoresAsync</span>(&amp;extSem, &amp;waitParams, <span class="hljs-number">1</span>, stream);<br><br><span class="hljs-comment">// CUDA计算任务</span><br>myKernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(...);<br><br><span class="hljs-comment">// CUDA完成后触发信号</span><br>cudaExternalSemaphoreSignalParams signalParams = &#123;&#125;;<br>signalParams.params.fence.value = <span class="hljs-number">1</span>;<br><span class="hljs-built_in">cudaSignalExternalSemaphoresAsync</span>(&amp;extSem, &amp;signalParams, <span class="hljs-number">1</span>, stream);<br></code></pre></td></tr></table></figure><p><img src="%E8%B7%A8API%E5%90%8C%E6%AD%A5%E6%A8%A1%E5%BC%8F.png" alt="跨API同步模式"></p><p><strong>Windows平台(NT句柄)</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 安全属性设置</span><br>SECURITY_ATTRIBUTES sa = &#123;&#125;;<br>sa.nLength = <span class="hljs-built_in">sizeof</span>(sa);<br>sa.bInheritHandle = TRUE;  <span class="hljs-comment">// 允许继承</span><br><br><span class="hljs-comment">// 创建可导出资源</span><br>HANDLE handle;<br>cudaExternalMemoryHandleDesc desc = &#123;&#125;;<br>desc.type = cudaExternalMemoryHandleTypeOpaqueWin32;<br>desc.handle.win<span class="hljs-number">32.</span>handle = handle;<br>desc.flags = cudaExternalMemoryDedicated;  <span class="hljs-comment">// 专用内存标志</span><br></code></pre></td></tr></table></figure><p><strong>Linux平台(文件描述符)</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 导出FD</span><br><span class="hljs-type">int</span> fd = <span class="hljs-built_in">exportVulkanMemoryToFD</span>();<br><br>cudaExternalMemoryHandleDesc desc = &#123;&#125;;<br>desc.type = cudaExternalMemoryHandleTypeOpaqueFd;<br>desc.handle.fd = fd;<br>desc.size = memSize;<br></code></pre></td></tr></table></figure><p><strong>NVIDIA SCI(高性能互连)</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 使用NVIDIA专用接口</span><br>cudaExternalMemoryHandleDesc desc = &#123;&#125;;<br>desc.type = cudaExternalMemoryHandleTypeNvSciBuf;<br>desc.handle.nvSciBufObject = nvSciBufObj;<br></code></pre></td></tr></table></figure><h2 id="CUDA硬件实现">CUDA硬件实现</h2><p>NVIDIA显卡的核心是由许多&quot;流式多处理器&quot;(SM)组成的阵列。当CPU启动一个CUDA计算任务时：</p><ul><li>计算任务被分成多个&quot;线程块&quot;</li><li>这些块会被分配到空闲的SM上执行</li><li>每个SM可以同时运行多个线程块</li><li>当一个块完成时，新的块会立即补上、</li></ul><p>每个SM需要同时处理数百个线程，为此NVIDIA发明了独特的&quot;SIMT&quot;(单指令多线程)技术，SIMT相较于CPU有如下区别：</p><table><thead><tr><th style="text-align:left">特性</th><th style="text-align:left">CPU</th><th style="text-align:left">GPU(SIMT)</th></tr></thead><tbody><tr><td style="text-align:left">执行方式</td><td style="text-align:left">一次处理1个线程</td><td style="text-align:left">一次处理32个线程(1个warp)</td></tr><tr><td style="text-align:left">分支预测</td><td style="text-align:left">有</td><td style="text-align:left">无</td></tr><tr><td style="text-align:left">线程切换</td><td style="text-align:left">开销大</td><td style="text-align:left">零开销</td></tr><tr><td style="text-align:left">适合场景</td><td style="text-align:left">复杂逻辑</td><td style="text-align:left">大批量简单计算</td></tr></tbody></table><p>Warp，称之为线程束，就像纺织中的&quot;经线束&quot;一样，32个线程捆绑成一组执行，所有线程同时开始执行相同指令，但每个线程有自己的数据和执行路径。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 示例：查看warp大小</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">showWarpSize</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-keyword">if</span>(threadIdx.x == <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;1个warp包含%d个线程\n&quot;</span>, warpSize); <span class="hljs-comment">// 总是32</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>尽量确保一个warp中的32个线程走相同的路径：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 高效访问：32个线程连续读取内存</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">goodMemoryAccess</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, <span class="hljs-type">float</span>* input)</span> </span>&#123;<br>    <span class="hljs-type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;<br>    output[i] = input[i]; <span class="hljs-comment">// 像军训排队一样整齐</span><br>&#125;<br><br><span class="hljs-comment">// 低效访问：线程跳跃式访问</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">badMemoryAccess</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, <span class="hljs-type">float</span>* input)</span> </span>&#123;<br>    <span class="hljs-type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;<br>    output[i*<span class="hljs-number">10</span>] = input[i*<span class="hljs-number">10</span>]; <span class="hljs-comment">// 像散兵游勇</span><br>&#125;<br></code></pre></td></tr></table></figure><p>为了避免降低warp效率，我们可以采用一些策略：</p><p>1.减少分叉</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 优化前：可能造成warp分叉</span><br><span class="hljs-keyword">if</span>(index % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) &#123;<br>    <span class="hljs-comment">// A路径</span><br>&#125; <span class="hljs-keyword">else</span> &#123;<br>    <span class="hljs-comment">// B路径</span><br>&#125;<br><br><span class="hljs-comment">// 优化后：减少分叉</span><br>result = (index % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) ? <span class="hljs-built_in">calcA</span>() : <span class="hljs-built_in">calcB</span>();<br></code></pre></td></tr></table></figure><ol start="2"><li>自动配置资源</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 自动计算最佳配置</span><br><span class="hljs-type">int</span> blockSize;   <span class="hljs-comment">// 每个块的线程数</span><br><span class="hljs-type">int</span> minGridSize; <span class="hljs-comment">// 最少需要多少个块</span><br><span class="hljs-built_in">cudaOccupancyCalculateBestBlockSize</span>(&amp;blockSize, &amp;minGridSize, myKernel);<br></code></pre></td></tr></table></figure><p>可以执行调试命令查看相关性能：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看warp执行效率</span><br>nvprof --metrics achieved_occupancy ./myApp<br><br><span class="hljs-comment"># 检查内存访问模式</span><br>nsight-compute --<span class="hljs-built_in">set</span> default --section MemoryWorkloadAnalysis ./myApp<br></code></pre></td></tr></table></figure><p>GPU的流式多处理器(SM)采用独特的<strong>硬件级上下文管理</strong>：</p><ul><li>每个warp的完整执行状态（程序计数器、寄存器等）始终保存在芯片上</li><li>切换不同warp的执行<strong>不需要保存/恢复上下文</strong></li><li>硬件调度器每个时钟周期选择就绪warp发射指令</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 示例：查看当前设备的warp调度器数量</span><br>cudaDeviceProp prop;<br><span class="hljs-built_in">cudaGetDeviceProperties</span>(&amp;prop, <span class="hljs-number">0</span>);<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;每个SM的warp调度器数量: %d\n&quot;</span>, prop.maxThreadsPerMultiProcessor / <span class="hljs-number">32</span>);<br></code></pre></td></tr></table></figure><p>SM资源分区模型由以下几种类型：</p><table><thead><tr><th style="text-align:left">资源类型</th><th style="text-align:left">分配单位</th><th style="text-align:left">特性</th></tr></thead><tbody><tr><td style="text-align:left">寄存器文件</td><td style="text-align:left">每个线程</td><td style="text-align:left">最快的存储，生命周期随线程</td></tr><tr><td style="text-align:left">共享内存</td><td style="text-align:left">每个线程块</td><td style="text-align:left">块内线程共享，手动管理</td></tr><tr><td style="text-align:left">L1缓存/纹理缓存</td><td style="text-align:left">整个SM</td><td style="text-align:left">自动缓存</td></tr></tbody></table><p>对于特定的资源，有其合理的分配方式：</p><p>每个块的warp数量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">warps_per_block = ceil(threads_per_block, 32) / 32<br></code></pre></td></tr></table></figure><p>寄存器分配：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">总寄存器 = 每个线程寄存器数 × 线程数 × warp数量<br></code></pre></td></tr></table></figure><p>共享内存分配：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">__shared__ <span class="hljs-built_in">float</span> tile[32][32]; // 静态分配<br>extern __shared__ int dynamic[]; // 动态分配<br></code></pre></td></tr></table></figure><h2 id="CUDA性能指南">CUDA性能指南</h2><p>性能优化围绕三个基本策略展开：</p><ul><li>最大化并行执行以实现最大利用率</li><li>优化内存使用以实现最大内存吞吐量</li><li>优化指令使用以实现最大的指令吞吐量</li></ul><h3 id="最大化利用率">最大化利用率</h3><p>为了最大限度地提高利用率，应用程序的结构应尽可能多地公开并行性，并有效地将这种并行性映射到系统的各个组件，以使它们在大部分时间都保持忙碌。</p><p>在<strong>应用层面</strong>的高层设计上，应用程序应通过异步函数调用和流(stream)技术，最大化主机(CPU)、设备(GPU)以及连接总线之间的并行执行。基本原则是：</p><ul><li>CPU处理串行任务</li><li>GPU处理并行任务</li></ul><p>对于需要线程间数据同步的并行任务，存在两种情况：</p><ol><li><p><strong>同一线程块内同步</strong>：使用<code>__syncthreads()</code>和共享内存</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp">__shared__ <span class="hljs-type">float</span> temp[<span class="hljs-number">32</span>];  <span class="hljs-comment">// 共享内存</span><br>temp[threadIdx.x] = data;<br>__syncthreads();           <span class="hljs-comment">// 块内同步</span><br></code></pre></td></tr></table></figure></li><li><p><strong>跨线程块同步</strong>：必须通过全局内存和多次内核调用实现（效率较低）</p></li></ol><p>在<strong>设备内部</strong>，应最大化多处理器(SM)间的并行执行：</p><ul><li>通过流(stream)实现多内核并发执行：</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp">cudaStream_t stream1, stream2;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream1);<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream2);<br>kernel1&lt;&lt;&lt;..., stream1&gt;&gt;&gt;();<br>kernel2&lt;&lt;&lt;..., stream2&gt;&gt;&gt;();<br></code></pre></td></tr></table></figure><p>在<strong>多处理器层面</strong>，每个SM内部，应最大化各功能单元的并行利用率：</p><ul><li><strong>Warp调度</strong>：每个时钟周期选择就绪的warp执行指令</li><li><strong>延迟隐藏</strong>：通过足够多的活跃warp掩盖指令延迟</li><li><strong>计算能力差异</strong>：<ul><li>5.x/6.1/7.x设备：需要4×延迟周期数的warp</li><li>6.0设备：需要2×延迟周期数的warp</li><li>3.x设备：需要8×延迟周期数的warp</li></ul></li></ul><p>CUDA提供API帮助优化线程块配置：</p><p>占用率计算示例：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 设备代码</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MyKernel</span><span class="hljs-params">(<span class="hljs-type">int</span> *d, <span class="hljs-type">int</span> *a, <span class="hljs-type">int</span> *b)</span> </span>&#123;<br>    <span class="hljs-type">int</span> idx = threadIdx.x + blockIdx.x * blockDim.x;<br>    d[idx] = a[idx] * b[idx];<br>&#125;<br><br><span class="hljs-comment">// 主机代码</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-type">int</span> numBlocks, blockSize = <span class="hljs-number">32</span>;<br>    cudaDeviceProp prop;<br>    <span class="hljs-built_in">cudaGetDeviceProperties</span>(&amp;prop, <span class="hljs-number">0</span>);<br>    <br>    <span class="hljs-comment">// 计算占用率</span><br>    <span class="hljs-built_in">cudaOccupancyMaxActiveBlocksPerMultiprocessor</span>(<br>        &amp;numBlocks, MyKernel, blockSize, <span class="hljs-number">0</span>);<br>    <br>    <span class="hljs-type">float</span> occupancy = (numBlocks * blockSize / prop.warpSize) / <br>                    (<span class="hljs-type">float</span>)(prop.maxThreadsPerMultiProcessor / prop.warpSize);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;占用率: %.1f%%\n&quot;</span>, occupancy * <span class="hljs-number">100</span>);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>自动配置启动参数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">launchKernel</span><span class="hljs-params">(<span class="hljs-type">int</span> *array, <span class="hljs-type">int</span> count)</span> </span>&#123;<br>    <span class="hljs-type">int</span> blockSize, minGridSize, gridSize;<br>    <br>    <span class="hljs-comment">// 获取最优配置</span><br>    <span class="hljs-built_in">cudaOccupancyMaxPotentialBlockSize</span>(<br>        &amp;minGridSize, &amp;blockSize, MyKernel, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>);<br>    <br>    <span class="hljs-comment">// 根据数据量调整网格大小</span><br>    gridSize = (count + blockSize - <span class="hljs-number">1</span>) / blockSize;<br>    MyKernel&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(array, count);<br>    <br>    <span class="hljs-built_in">cudaDeviceSynchronize</span>();<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="最大化内存吞吐量">最大化内存吞吐量</h3><p>一、内存传输优化原则</p><ol><li>最小化主机-设备数据传输</li></ol><ul><li><p><strong>数据传输层级对比</strong>：</p><table><thead><tr><th style="text-align:left">传输类型</th><th style="text-align:left">带宽</th><th style="text-align:left">延迟</th><th style="text-align:left">优化建议</th></tr></thead><tbody><tr><td style="text-align:left">主机↔设备</td><td style="text-align:left">低</td><td style="text-align:left">高</td><td style="text-align:left">尽量减少</td></tr><tr><td style="text-align:left">全局内存访问</td><td style="text-align:left">中</td><td style="text-align:left">中</td><td style="text-align:left">优化访问模式</td></tr><tr><td style="text-align:left">片上内存(共享/L1)</td><td style="text-align:left">高</td><td style="text-align:left">低</td><td style="text-align:left">最大化使用</td></tr></tbody></table></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 坏实践：频繁小数据传输</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;<span class="hljs-number">1000</span>; i++) &#123;<br>    <span class="hljs-built_in">cudaMemcpy</span>(devPtr+i, hostPtr+i, <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice);<br>&#125;<br><br><span class="hljs-comment">// 好实践：批量传输</span><br><span class="hljs-built_in">cudaMemcpy</span>(devPtr, hostPtr, <span class="hljs-number">1000</span>*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice);<br></code></pre></td></tr></table></figure><ol start="2"><li>使用固定内存(pinned memory)</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">float</span>* hostPtr;<br><span class="hljs-built_in">cudaHostAlloc</span>(&amp;hostPtr, size, cudaHostAllocDefault);  <span class="hljs-comment">// 分配固定内存</span><br>kernel&lt;&lt;&lt;...&gt;&gt;&gt;(devPtr, ...);<br><span class="hljs-built_in">cudaMemcpyAsync</span>(hostPtr, devPtr, size, cudaMemcpyDeviceToHost, stream);<br></code></pre></td></tr></table></figure><p>二、全局内存访问优化</p><ol><li>合并访问模式</li></ol><ul><li><strong>理想情况</strong>：一个warp的32个线程访问连续的128字节内存块</li><li><strong>访问模式对比</strong>：</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 非合并访问(低效)</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">badAccess</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, <span class="hljs-type">float</span>* input, <span class="hljs-type">int</span> stride)</span> </span>&#123;<br>    <span class="hljs-type">int</span> tid = threadIdx.x + blockIdx.x * blockDim.x;<br>    output[tid] = input[tid * stride];  <span class="hljs-comment">// 跨步访问</span><br>&#125;<br><br><span class="hljs-comment">// 合并访问(高效)</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">goodAccess</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, <span class="hljs-type">float</span>* input)</span> </span>&#123;<br>    <span class="hljs-type">int</span> tid = threadIdx.x + blockIdx.x * blockDim.x;<br>    output[tid] = input[tid];  <span class="hljs-comment">// 连续访问</span><br>&#125;<br></code></pre></td></tr></table></figure><ol start="2"><li>二维数组的特殊处理</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 使用cudaMallocPitch处理非对齐宽度</span><br><span class="hljs-type">size_t</span> pitch;<br><span class="hljs-type">float</span>* devPtr;<br><span class="hljs-built_in">cudaMallocPitch</span>(&amp;devPtr, &amp;pitch, width*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), height);<br><br><span class="hljs-comment">// 拷贝时指定pitch</span><br><span class="hljs-built_in">cudaMemcpy2D</span>(devPtr, pitch, hostPtr, width*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), <br>             width*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), height, cudaMemcpyHostToDevice);<br></code></pre></td></tr></table></figure><p>三、各类内存特性与优化</p><ol><li><p>共享内存使用技巧</p><p><strong>bank冲突避免</strong>：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp">__shared__ <span class="hljs-type">float</span> tile[<span class="hljs-number">32</span>][<span class="hljs-number">32</span><span class="hljs-number">+1</span>]; <span class="hljs-comment">// 添加padding消除bank冲突</span><br><br><span class="hljs-comment">// 无冲突访问模式</span><br><span class="hljs-type">float</span> val = tile[threadIdx.y][threadIdx.x]; <span class="hljs-comment">// 线程ID与bank分布匹配</span><br></code></pre></td></tr></table></figure><p><strong>典型使用模式</strong>：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">sharedMemExample</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, <span class="hljs-type">float</span>* input)</span> </span>&#123;<br>    __shared__ <span class="hljs-type">float</span> temp[BLOCK_SIZE];<br>    <span class="hljs-type">int</span> tid = threadIdx.x;<br>    <br>    <span class="hljs-comment">// 1. 从全局内存加载到共享内存</span><br>    temp[tid] = input[blockIdx.x * blockDim.x + tid];<br>    <br>    <span class="hljs-comment">// 2. 同步等待所有线程完成加载</span><br>    __syncthreads();<br>    <br>    <span class="hljs-comment">// 3. 处理共享内存数据</span><br>    temp[tid] *= <span class="hljs-number">2.0f</span>;<br>    <br>    <span class="hljs-comment">// 4. 同步确保处理完成</span><br>    __syncthreads();<br>    <br>    <span class="hljs-comment">// 5. 写回全局内存</span><br>    output[blockIdx.x * blockDim.x + tid] = temp[tid];<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>常量内存与纹理内存</p><p><strong>常量内存适用场景</strong>：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp">__constant__ <span class="hljs-type">float</span> constData[<span class="hljs-number">256</span>];<br><br><span class="hljs-comment">// 初始化常量内存</span><br><span class="hljs-built_in">cudaMemcpyToSymbol</span>(constData, hostData, <span class="hljs-built_in">sizeof</span>(hostData));<br></code></pre></td></tr></table></figure><p><strong>纹理内存优势</strong>：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp">texture&lt;<span class="hljs-type">float</span>, <span class="hljs-number">2</span>, cudaReadModeElementType&gt; texRef;<br><br><span class="hljs-comment">// 绑定纹理</span><br><span class="hljs-built_in">cudaBindTexture2D</span>(<span class="hljs-number">0</span>, texRef, devPtr, channelDesc, width, height, pitch);<br><br><span class="hljs-comment">// 内核中访问</span><br><span class="hljs-type">float</span> val = <span class="hljs-built_in">tex2D</span>(texRef, x, y);<br></code></pre></td></tr></table></figure></li></ol><h3 id="最大化指令吞吐量">最大化指令吞吐量</h3><p>一、算术运算优化策略</p><ol><li><p>精度与速度的权衡</p><table><thead><tr><th style="text-align:left">运算类型</th><th style="text-align:left">推荐操作</th><th style="text-align:left">加速比</th><th style="text-align:left">适用场景</th></tr></thead><tbody><tr><td style="text-align:left">单精度除法</td><td style="text-align:left">使用<code>__fdividef(x,y)</code></td><td style="text-align:left">2-5倍</td><td style="text-align:left">精度要求不高时</td></tr><tr><td style="text-align:left">平方根倒数</td><td style="text-align:left">直接调用<code>rsqrtf()</code></td><td style="text-align:left">3倍</td><td style="text-align:left">图形渲染、物理模拟</td></tr><tr><td style="text-align:left">三角函数</td><td style="text-align:left">控制参数范围&lt;105615.0f</td><td style="text-align:left">10倍</td><td style="text-align:left">避免慢速路径</td></tr></tbody></table></li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 快速除法示例</span><br><span class="hljs-type">float</span> result = __fdividef(a, b);  <span class="hljs-comment">// 比常规除法快</span><br><br><span class="hljs-comment">// 优化三角函数调用</span><br><span class="hljs-keyword">if</span>(<span class="hljs-built_in">fabs</span>(x) &lt; <span class="hljs-number">105615.0f</span>) &#123;        <span class="hljs-comment">// 保持使用快速路径</span><br>    <span class="hljs-type">float</span> s = <span class="hljs-built_in">sinf</span>(x); <br>&#125;<br></code></pre></td></tr></table></figure><ol start="2"><li>半精度浮点运算技巧</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 使用half2类型实现双倍吞吐量</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">halfPrecisionKernel</span><span class="hljs-params">(half2* data)</span> </span>&#123;<br>    half2 a = data[threadIdx.x];<br>    half2 b = __hadd2(a, <span class="hljs-built_in">make_half2</span>(<span class="hljs-number">1.0f</span>, <span class="hljs-number">1.0f</span>)); <span class="hljs-comment">// 同时加两个数</span><br>    data[threadIdx.x] = __hmul2(a, b);            <span class="hljs-comment">// 同时乘两个数</span><br>&#125;<br></code></pre></td></tr></table></figure><p>二、控制流优化方法</p><ol><li>避免分支分歧（<strong>Warp对齐条件</strong>：确保同一warp内线程执行相同路径）</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 差实践：基于threadIdx的条件分支</span><br><span class="hljs-keyword">if</span>(threadIdx.x % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) &#123; <span class="hljs-comment">/* 导致warp分歧 */</span> &#125;<br><br><span class="hljs-comment">// 好实践：Warp对齐条件</span><br><span class="hljs-keyword">if</span>((threadIdx.x / warpSize) % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) &#123; <span class="hljs-comment">/* 保持warp统一 */</span> &#125;、<br></code></pre></td></tr></table></figure><ol start="2"><li>循环展开优化</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">pragma</span> unroll 4  <span class="hljs-comment">// 手动指定展开因子</span></span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;N; i++) &#123;<br>    <span class="hljs-comment">// 循环体</span><br>&#125;<br></code></pre></td></tr></table></figure><p>三. 同步指令性能</p><h3 id="各架构同步吞吐量对比">各架构同步吞吐量对比</h3><table><thead><tr><th style="text-align:left">计算能力</th><th style="text-align:left"><code>__syncthreads()</code>吞吐量</th><th style="text-align:left">相当于每周期操作数</th></tr></thead><tbody><tr><td style="text-align:left">3.x</td><td style="text-align:left">高</td><td style="text-align:left">128 ops/cycle</td></tr><tr><td style="text-align:left">5.x/6.1/6.2</td><td style="text-align:left">中</td><td style="text-align:left">64 ops/cycle</td></tr><tr><td style="text-align:left">6.0</td><td style="text-align:left">低</td><td style="text-align:left">32 ops/cycle</td></tr><tr><td style="text-align:left">7.x</td><td style="text-align:left">最低</td><td style="text-align:left">16 ops/cycle</td></tr></tbody></table><p>四. 类型转换优化</p><ol><li>避免隐式转换开销</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 差实践：引发int转换</span><br><span class="hljs-type">short</span> a = b + c;  <span class="hljs-comment">// 转换为int运算</span><br><br><span class="hljs-comment">// 好实践：使用合适类型</span><br><span class="hljs-type">int</span> a = b + c;    <span class="hljs-comment">// 无转换开销</span><br></code></pre></td></tr></table></figure><ol start="2"><li>浮点常量优化</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">float</span> a = <span class="hljs-number">3.141592653589793</span>;   <span class="hljs-comment">// 引发双精度到单精度转换</span><br><span class="hljs-type">float</span> b = <span class="hljs-number">3.141592653589793f</span>;  <span class="hljs-comment">// 直接使用单精度常量</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>高性能计算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Perf Linux使用教程</title>
    <link href="/Diffcc/Diffcc.github.io/2025/06/10/20250610/"/>
    <url>/Diffcc/Diffcc.github.io/2025/06/10/20250610/</url>
    
    <content type="html"><![CDATA[<p><code>perf</code> 是 Linux 内核提供的一个强大的性能分析工具，基于 <strong>硬件性能计数器（Performance Monitoring Counters, PMC）</strong> 和 <strong>内核事件采样</strong> 来监控系统性能。</p><span id="more"></span><h2 id="基本原理">基本原理</h2><p>Linux性能计数器是一个基于内核的子系统，它提供一个性能分析框架，比如硬件**（CPU、PMU（Performance Monitoring Unit））<strong>功能和软件</strong>（软件计数器、tracepoint）**功能。通过perf，应用程序可以利用PMU、tracepoint和内核中的计数器来进行性能统计。</p><ul><li>现代 CPU（如 Intel、AMD、ARM）都内置了 <strong>性能监控单元（PMU, Performance Monitoring Unit）</strong>，可以统计各种硬件事件：<strong>CPU 周期数（cycles）</strong>，<strong>指令数（instructions）</strong>，<strong>缓存命中/失效（cache-misses, cache-references）</strong>，<strong>分支预测失败（branch-misses）</strong>，<strong>分支预测失败（branch-misses）</strong>，<code>perf</code> 通过 <strong>PMU</strong> 读取这些计数器，计算 <strong>CPI（Cycles Per Instruction）</strong>、<strong>缓存命中率</strong> 等指标，帮助分析程序瓶颈。</li><li>除了硬件事件，<code>perf</code> 还可以监控 <strong>内核/用户态软件事件</strong>：<strong>CPU 调度事件（context-switches, migrations）</strong>，<strong>缺页异常（page-faults）</strong>，<strong>系统调用（syscalls）</strong>，<strong>块设备 I/O（block:block_rq_issue</strong>，这些数据可以帮忙分析系统调用开销、调度延迟、I/O瓶颈等问题。</li><li><code>perf</code>支持<strong>动态探针（Dynamic Probes）</strong>，类似于<code>ftrace</code>和<code>eBPF</code>：<strong>kprobes</strong>：动态追踪内核函数，<strong>uprobes</strong>：动态追踪用户态函数，<strong>tracepoints</strong>：静态内核跟踪点（如 <code>sched:sched_switch</code>）。</li></ul><p>每隔一个固定时间，CPU上产生一个中断，Perf监控当前是哪个进程、哪个函数，然后给对应的进程和函数加一个统计值，这样就知道CPU有多少时间在某个进程或某个函数上。</p><p><img src="Perf%E5%8E%9F%E7%90%86%E5%9B%BE.jpg" alt="Perf原理图"></p><h2 id="安装">安装</h2><h3 id="1-云端">1.云端</h3><figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs http">https://ui.perfetto.dev/<br></code></pre></td></tr></table></figure><p>Google浏览器访问上述链接，通过导入<code>.json</code>记录文件，生成性能火焰图</p><p><img src="Perf%E7%BD%91%E9%A1%B5%E7%AB%AF%E7%95%8C%E9%9D%A2.png" alt="Perf网页端界面"></p><h3 id="2-本地">2. 本地</h3><ol><li>运行 <code>perf –version</code> 查看当前需要安装哪些库文件</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">(base) dongnan@server:~/GccTrace_ws/GccTrace/build$ perf --version<br>WARNING: perf not found <span class="hljs-keyword">for</span> kernel 5.4.0-150<br><br>  You may need to install the following packages <span class="hljs-keyword">for</span> this specific kernel:<br>    linux-tools-5.4.0-150-generic<br>    linux-cloud-tools-5.4.0-150-generic<br><br>  You may also want to install one of the following packages to keep up to <span class="hljs-built_in">date</span>:<br>    linux-tools-generic<br>    linux-cloud-tools-generic<br>    <br>    <br>    <br><span class="hljs-built_in">sudo</span> apt-get install linux-tools-5.4.0-150-generic linux-cloud-tools-5.4.0-150-generic linux-tools-generic linux-cloud-tools-generic<br></code></pre></td></tr></table></figure><ol start="2"><li>安装相应库文件 重新运行 perf –version 能够输出对应的版本信息</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">(base) dongnan@server:~/GccTrace_ws/GccTrace/build$ perf --version<br><br>perf version 5.4.233<br></code></pre></td></tr></table></figure><h2 id="使用">使用</h2><h3 id="1-实时分析">1.实时分析</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf top<br></code></pre></td></tr></table></figure><p>只能实时查看目前为止占用cpu较高的进程，不便于事后分析</p><p>-e：指定性能事件</p><p>-a：显示在所有CPU上的性能统计信息</p><p>-C：显示在指定CPU上的性能统计信息</p><p>-p：指定进程PID</p><p>-t：指定线程TID</p><p>-s：指定待解析的符号信息</p><p>-g: 记录函数的调用关系</p><p>执行下面命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> perf top -e task-clock -g -p 21437 -s <span class="hljs-built_in">comm</span>,dso,symbol<br><br><span class="hljs-comment">#comm：触发事件的进程名</span><br><span class="hljs-comment">#dso：Dynamic Shared Object，可以是应用程序、内核、动态链接库、模块</span><br><span class="hljs-comment">#symbol：函数名</span><br></code></pre></td></tr></table></figure><table><thead><tr><th><code>-e task-clock</code></th><th>监控 <strong>任务时钟（Task Clock）</strong> 事件，表示进程占用 CPU 的时间（单位：毫秒）。</th></tr></thead><tbody><tr><td><code>-g</code></td><td>记录 <strong>调用栈（Call Graph）</strong>，便于分析函数调用关系。</td></tr><tr><td><code>-p 21437</code></td><td>仅监控 <strong>进程 ID 为 21437</strong> 的进程。</td></tr><tr><td><code>-s comm,dso,symbol</code></td><td>显示字段： - <code>comm</code>：进程名。 - <code>dso</code>：动态共享对象（如 <code>libc.so</code>、可执行文件）。 - <code>symbol</code>：函数符号名（如 <code>malloc</code>、<code>main</code>）</td></tr></tbody></table><p><img src="Perf_top.png" alt="Perf_top"></p><p>执行下面命令监控所有进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf top -e task-clock -g<br></code></pre></td></tr></table></figure><p><img src="%E7%9B%91%E6%8E%A7%E6%89%80%E6%9C%89%E8%BF%9B%E7%A8%8B.png" alt="监控所有进程"></p><h3 id="2-生成数据文件后离线分析"><strong>2.生成数据文件后离线分析</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record<br></code></pre></td></tr></table></figure><p><code>perf record</code>用于 <strong>采集性能数据并保存到文件</strong>（默认生成 <code>perf.data</code>），便于后续离线分析（如 <code>perf report</code>、生成火焰图等</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record [选项] &lt;<span class="hljs-built_in">command</span>&gt;  <span class="hljs-comment"># 监控指定命令的执行</span><br></code></pre></td></tr></table></figure><table><thead><tr><th style="text-align:left">选项</th><th style="text-align:left">作用</th></tr></thead><tbody><tr><td style="text-align:left"><code>-e &lt;事件&gt;</code></td><td style="text-align:left">指定监控的事件（如 <code>cycles</code>, <code>cache-misses</code>, <code>task-clock</code>）。</td></tr><tr><td style="text-align:left"><code>-p &lt;PID&gt;</code></td><td style="text-align:left">监控指定进程（通过进程 ID）。</td></tr><tr><td style="text-align:left"><code>-F &lt;频率&gt;</code></td><td style="text-align:left">采样频率（Hz，如 <code>-F 99</code> 表示每秒采样 99 次）。</td></tr><tr><td style="text-align:left"><code>-g</code></td><td style="text-align:left">记录调用栈（用于生成火焰图）。</td></tr><tr><td style="text-align:left"><code>-a</code></td><td style="text-align:left">监控所有 CPU（系统级分析）。</td></tr><tr><td style="text-align:left"><code>-o &lt;文件&gt;</code></td><td style="text-align:left">指定输出文件（默认 <code>perf.data</code>）。</td></tr><tr><td style="text-align:left"><code>--call-graph &lt;方法&gt;</code></td><td style="text-align:left">调用栈记录方式（<code>fp</code>=帧指针，<code>dwarf</code>=调试信息）。</td></tr><tr><td style="text-align:left"><code>-s</code></td><td style="text-align:left">按线程分离统计。</td></tr><tr><td style="text-align:left"><code>-C &lt;CPU列表&gt;</code></td><td style="text-align:left">仅监控指定 CPU（如 <code>-C 0,1</code>）。</td></tr></tbody></table><h4 id="使用perf启动服务（不需要root权限）">使用perf启动服务（不需要root权限）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -g ./my_program  <span class="hljs-comment"># 监控程序运行，记录调用栈</span><br></code></pre></td></tr></table></figure><p>程序结束后生成 <code>perf.data</code>，用 <code>perf report</code> 分析</p><h4 id="挂接到已启动的进程（需要root权限）">挂接到已启动的进程（需要root权限）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -g -p &lt;PID&gt;      <span class="hljs-comment"># 监控指定进程</span><br></code></pre></td></tr></table></figure><p>按 <code>Ctrl+C</code> 停止采样</p><h4 id="指定采样事件">指定采样事件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -e cycles -g ./my_program   <span class="hljs-comment"># 监控 CPU 周期</span><br>perf record -e cache-misses -g ./my_program  <span class="hljs-comment"># 监控缓存失效</span><br></code></pre></td></tr></table></figure><p>可用事件列表：<code>perf list</code></p><h4 id="设置采样频率">设置采样频率</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -F 99 -g ./my_program  <span class="hljs-comment"># 每秒采样 99 次</span><br></code></pre></td></tr></table></figure><p>频率越高，数据越精确，但开销越大。</p><h4 id="系统级监控（所有-CPU）"><strong>系统级监控（所有 CPU）</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -ag                  <span class="hljs-comment"># 监控所有进程和 CPU</span><br>perf record -ag -e context-switches  <span class="hljs-comment"># 监控上下文切换</span><br></code></pre></td></tr></table></figure><h3 id="3-分析数据">3.分析数据</h3><ol><li>对于生成的.data目录可以直接运行 <code>perf report</code>查看相关报告</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf report                     <span class="hljs-comment"># 交互式查看热点</span><br>perf report --stdio             <span class="hljs-comment"># 命令行模式输出</span><br><br><span class="hljs-comment">#按函数/符号排序，显示 CPU 时间占比。</span><br></code></pre></td></tr></table></figure><ol start="2"><li>通过火焰图分析（需要用到<em><strong>FlameGraph</strong></em>工具，也可以直接使用网页端<a href="https://ui.perfetto.dev/%EF%BC%89">https://ui.perfetto.dev/）</a></li></ol><p>CPU火焰图中的每一个方框是一个函数，方框的长度，代表了它的执行时间，所以越宽的函数，执行越久。火焰图的楼层每高一层，就是更深一级的函数被调用，最顶层的函数，是叶子函数。</p><p><img src="%E7%81%AB%E7%84%B0%E5%9B%BE.png" alt="火焰图"></p><ul><li>FlameGraph工具使用</li></ul><p>（1）安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> --depth 1 https://github.com/brendangregg/FlameGraph<br></code></pre></td></tr></table></figure><p>（2）使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl &gt; perf.svg<br><br>firefox perf.svg <span class="hljs-comment"># .svg文件可以通过浏览器打开</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Linux系统编程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch简明教程</title>
    <link href="/Diffcc/Diffcc.github.io/2025/06/07/20250607-2/"/>
    <url>/Diffcc/Diffcc.github.io/2025/06/07/20250607-2/</url>
    
    <content type="html"><![CDATA[<p>本文可以作为Pytorch初学者的快速上手文档，参考B站小土堆</p><span id="more"></span><h1>Pytorch</h1><h2 id="1-安装环境">1.安装环境</h2><p>==pyhton 3.6  Anaconda 3.5.2==<br>==CUDA+显卡驱动==</p><p><code>create -n pytorch python=3.6</code><br><code>conda activate pytorch</code><br><code>pip list</code></p><h2 id="2-Pytorch安装">2. Pytorch安装</h2><p><img src="pytorch%E4%B8%8B%E8%BD%BD.png" alt="pytorch下载"></p><p>查看nvidia对应的Version</p><p><img src="CUDA%E7%89%88%E6%9C%AC.png" alt="CUDA版本"></p><p>验证是否安装成功</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">python<br><span class="hljs-keyword">import</span> torch<br>torch.cuda.is_available()  <span class="hljs-comment">##显示是否可用GPU  True</span><br></code></pre></td></tr></table></figure><h2 id="3-Pycharm-Jupyter">3.Pycharm+Jupyter</h2><p><strong>Pycharm 配置pytorch</strong><br>python console 中运行(便于直接查看变量属性)</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clean"><span class="hljs-keyword">import</span> torch<br>torch.cuda.is_available()  ##显示是否可用GPU  <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><p><strong>Jupyter在pytorch环境中安装</strong></p><p><code>conda install -c conda-forge nb_conda</code><br><code>jupyter notebook</code><br><img src="Jupyter.png" alt="Jupyter"></p><p>Jupyter中  shift+enter  跳转下一行代码</p><h2 id="4-Pytorch中两个重要函数">4.Pytorch中两个重要函数</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stylus">Pytorch就是一个package<br><br><span class="hljs-function"><span class="hljs-title">dir</span><span class="hljs-params">()</span></span>:打开，看见<br><span class="hljs-function"><span class="hljs-title">dir</span><span class="hljs-params">(pytorch)</span></span><br>输出：<span class="hljs-number">1</span>、<span class="hljs-number">2</span>、<span class="hljs-number">3</span>、<span class="hljs-number">4</span><br><span class="hljs-function"><span class="hljs-title">dir</span><span class="hljs-params">(pytorch.<span class="hljs-number">3</span>)</span></span><br>输出：<span class="hljs-selector-tag">a</span>,<span class="hljs-selector-tag">b</span>,c<br><br><br><span class="hljs-function"><span class="hljs-title">help</span><span class="hljs-params">()</span></span>:说明书<br><span class="hljs-function"><span class="hljs-title">help</span><span class="hljs-params">(pytorch.<span class="hljs-number">3</span>.a)</span></span><br>输出：a的具体功能<br></code></pre></td></tr></table></figure><p><img src="%E8%BE%93%E5%87%BA1.png" alt="输出1"></p><h2 id="5-Pycharm对比Jupyter">5.Pycharm对比Jupyter</h2><p>同时在==python==文件、==Python控制台==、==Jupyter==运行一下代码：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Start！&quot;</span>)<br><span class="hljs-attribute">a</span>=<span class="hljs-string">&quot;hello world&quot;</span><br><span class="hljs-attribute">b</span>=2019<br><span class="hljs-attribute">c</span>=a+b<br><span class="hljs-built_in">print</span>(c)<br></code></pre></td></tr></table></figure><p>代码是以块为一个整体运行</p><p>Python文件：文件的块是所有行的代码，从头开始运行  适用于大型项目<br>Python控制台：以任意行为块运行  但是阅读性不好  可以显示每个变量属性<br>Jupyter：可以以任意代码块运行 利于代码的阅读与修改</p><p><img src="%E5%AF%B9%E6%AF%94.png" alt="对比"></p><h2 id="6-Pytorch数据加载">6.Pytorch数据加载</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clean">Dataset  ##提供一种方式去获取数据及其label值<br>Dataloader   ##为网络提供不同的数据形式  Batchsize<br></code></pre></td></tr></table></figure><p><strong>Dataset</strong></p><ul><li>如何获取每一个数据及其label</li><li>告诉我们总共有多少数据</li></ul><p><strong>Jupyter可以方便查看Dataset功能</strong></p><p><img src="%E6%9F%A5%E7%9C%8BDataset.png" alt="查看Dataset"></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ruby">from torch.utils.data import <span class="hljs-title class_">Dataset</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_">Dataset</span>):<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>.idx</span>)<br></code></pre></td></tr></table></figure><p><strong>read_data程序</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> dataset<br>form PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_ inherited__">dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,root_dir,label_dir</span>):<br>        <span class="hljs-variable language_">self</span>.root_dir=root_dir    <span class="hljs-comment">##初始中引入全局变量</span><br>        <span class="hljs-variable language_">self</span>.label_dir=label_dir   <span class="hljs-comment">##全局变量</span><br>        <span class="hljs-variable language_">self</span>.path=os.path.join(<span class="hljs-variable language_">self</span>.root_dir,<span class="hljs-variable language_">self</span>.label_dir)   <span class="hljs-comment">##变量地址相加获取Image地址</span><br>        <span class="hljs-variable language_">self</span>.image_path=os.listdir(<span class="hljs-variable language_">self</span>.path)  <span class="hljs-comment">##获取地址中的列表内容</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self,idx</span>):<br>        image_name=<span class="hljs-variable language_">self</span>.image_path[idx]   <span class="hljs-comment">##获取列表中名字</span><br>        image_item_path=os.path.join(<span class="hljs-variable language_">self</span>.path,image_name)  <span class="hljs-comment">##获取每张图片的地址（包括自身名字）</span><br>        img=Image.<span class="hljs-built_in">open</span>(image_item_path)  <span class="hljs-comment">##打开图片</span><br>        label=<span class="hljs-variable language_">self</span>.label_dir  <span class="hljs-comment">##获取标签</span><br>        <span class="hljs-keyword">return</span> img,label  <span class="hljs-comment">##返回图像与标签</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.image_path)  <span class="hljs-comment">##获取数据长度</span><br>    <br>root_dir=<span class="hljs-string">&quot;&quot;</span><br>ants_label_dir=<span class="hljs-string">&quot;&quot;</span><br>ants_dataset=MyData(root_dir,ants_label_dir)<br>        <br></code></pre></td></tr></table></figure><h2 id="7-TensorBoard-使用">7. TensorBoard 使用</h2><p>==探究模型不同阶段的输出图像效果==</p><style>.ilpwzjdxhzzr{zoom:80%;}</style><img src="Diffcc/2025/06/07/20250607-2/Tensorboard.png" class="ilpwzjdxhzzr" alt="Tensorboard"><p><code>writer.add_scalar()</code></p><p><img src="Tensorboard%E8%AE%BE%E7%BD%AE.png" alt="Tensorboard设置"></p><p><strong>安装tensorboard</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs txt">conda activate pytorch<br>pip install tensorboard<br></code></pre></td></tr></table></figure><p><strong>运行tensorboard</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">tensorboard --logdir=logs --host=<span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span> <span class="hljs-comment">##logdir=logs文件也就是 Summarywriter（“logs”）中的文件地址</span><br>tensorboard --logdir=logs --host=<span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span> --port=<span class="hljs-number">6007</span>  <span class="hljs-comment">##定义端口名字 防止冲突</span><br></code></pre></td></tr></table></figure><p><img src="%E6%9C%AC%E5%9C%B0%E5%9C%B0%E5%9D%80.png" alt="本地地址"></p><p><img src="%E8%BE%93%E5%87%BA2.png" alt="输出2"></p><p>==注意：程序运行多次，logs文件中会生成不一样的图像  每向wirter中写入上一个程序 数据会保留==<br>==删除 logs==</p><p><strong>读取图像</strong></p><p><code>wirter.add-image()</code></p><p><img src="%E8%AF%BB%E5%8F%96%E5%9B%BE%E5%83%8F.png" alt="读取图像"></p><p>==其中image读取类型为==</p><p><img src="%E5%9B%BE%E7%89%87%E7%B1%BB%E5%9E%8B.png" alt="图片类型"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">PIL中 Image.<span class="hljs-built_in">open</span>(path)读取的类型为<br>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;PIL.JpegImagePlugin.JpegImageFile&#x27;</span>&gt;<br><br>利用Opencv读取numpy类型<br><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image_path=<span class="hljs-string">&quot;相对路径&quot;</span><br>img=Image.<span class="hljs-built_in">open</span>(image_path)<br>img_narry=np.array(img)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(img_narry))<br>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;numpy.ndarray&#x27;</span>&gt;<br><span class="hljs-built_in">print</span>(img_array.shape)<br>(<span class="hljs-number">512</span>,<span class="hljs-number">768</span>,<span class="hljs-number">3</span>)   (HWC)   <span class="hljs-comment">## add_image默认dataformats为tensorboard的CHW</span><br><span class="hljs-comment">##使用numpy型需指定</span><br><br>writer=SUmmaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>writer.add_image(<span class="hljs-string">&quot;test&quot;</span>,img_array,<span class="hljs-number">1</span>(步骤此处为第一步)，dataformats=<span class="hljs-string">&#x27;HWC&#x27;</span>)<br>writer.close()<br></code></pre></td></tr></table></figure><p><img src="test.png" alt="test"></p><h2 id="8-Transforms">8.Transforms</h2><h3 id="8-1-ToTensor">8.1 ToTensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchversion <span class="hljs-keyword">import</span> transforms  <span class="hljs-comment">#在transforms中alt+7  查看结构体</span><br></code></pre></td></tr></table></figure><p><strong>transforms结构及用法</strong></p><p>==利用其中的不同功能实现具体效果==</p><p><img src="transforms%E7%94%A8%E6%B3%95.png" alt="transforms用法"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><span class="hljs-keyword">from</span> torchversion <span class="hljs-keyword">import</span> transforms<br><br><span class="hljs-comment"># python的用法 -》tensor数据类型</span><br><span class="hljs-comment"># transforms.toTensor</span><br><span class="hljs-comment">#1、transforms该如何使用(python）</span><br><span class="hljs-comment">#2、为什么需要tensor数据类型</span><br><br>img_path=<span class="hljs-string">&quot;data/train/ants_image/5650366_e22b7e1065.jpg&quot;</span><br>img=Image.<span class="hljs-built_in">open</span>(img_path)<br><br><span class="hljs-comment">#1、transforms该如何使用(python）</span><br>tensor_trans=transforms.ToTensor()  <span class="hljs-comment">#利用classToTensor    创建自己的工具tensor_trans()</span><br>tensor_img=tensor_trans(img(Ctrl+P可以查看具体的所需数据类型))  <span class="hljs-comment">#使用自己的工具tensor_trans()转换成tensor类型图片</span><br><br><span class="hljs-comment">#2、为什么需要tensor数据类型</span><br>tensor 包括了神经网络中的参数类型 例如 梯度 反向传播 等<br><br>ToTensor包含将PIL和OPENCV读取的类型输入<br><br>writer=SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>writer.add_image(<span class="hljs-string">&quot;Tensor_image&quot;</span>,tensor_img)<br>writer.close()<br></code></pre></td></tr></table></figure><p><img src="Tensor_img.png" alt="Tensor_img"></p><h3 id="8-2-常见的Transforms">8.2 常见的Transforms</h3><ul><li>输入： <em>PIL</em>   <em>Image.open()</em></li><li>输出：<em>tensor</em>      <em>ToTensor()</em></li><li>作用：<em>narrays</em>    <em>cv.imread()</em></li></ul><p><strong>Compose</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">Call的用法， 不需要再用.调用 直接括号调用即可<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Person</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self,name</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;__call__&quot;</span>+<span class="hljs-string">&quot; Hello &quot;</span>+name)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">hello</span>(<span class="hljs-params">self,name</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;hello&quot;</span>+ name)<br><br>person=Person()<br>person(<span class="hljs-string">&quot;zhangsan&quot;</span>)<br>person.hello(<span class="hljs-string">&quot;zhangsan&quot;</span>)<br><br>输出：<br>__call__ Hello zhangsan<br>hellozhangsan<br></code></pre></td></tr></table></figure><p><strong>ToTensor</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">将PIL读取文件 OPENCV读取文件转换成tensor<br><br>trans_totensor=transforms.ToTensor()<br>img_tensor=trans_totensor(img)<br></code></pre></td></tr></table></figure><p><strong>ToPILImage</strong></p><p><code>*&quot;&quot;&quot;Convert a tensor or an ndarray to PIL Image - this does not scale values.*</code></p><p><strong>Normalize</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;Normalize a tensor image with mean and standard deviation.</span><br><span class="hljs-string">This transform does not support PIL Image.</span><br><span class="hljs-string">Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n``</span><br><span class="hljs-string">channels, this transform will normalize each channel of the input</span><br><span class="hljs-string">``torch.*Tensor`` i.e.,</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">``output[channel] = (input[channel] - mean[channel]) / std[channel]``</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">归一化用来消除量纲影响</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">trans_norm=transforms.Normalize([<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>],[<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>])  <br><span class="hljs-comment">#三通道 mean[channel]=[0.5,0.5,0.5]  平均值</span><br><span class="hljs-comment">#std[channel]=[0.5,0.5,0.5]   标准差</span><br><span class="hljs-comment">#output[channel] = (input[channel] - mean[channel]) / std[channel]</span><br><span class="hljs-comment">#此处相当于  2*Input-1 = Output</span><br>img_norm=trans_norm(tensor_img)<br></code></pre></td></tr></table></figure><p><strong>Resize</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;Resize the input image to the given size.</span><br><span class="hljs-string">If the image is torch Tensor, it is expected</span><br><span class="hljs-string">to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions</span><br><span class="hljs-string"></span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">trans_resize=transforms.Resize((<span class="hljs-number">512</span>,<span class="hljs-number">512</span>))<br><span class="hljs-comment"># Resize 输入图片需要为PIL类型 否则会报错</span><br><br>img_resize=trans_resize(img)<br><br><span class="hljs-comment">#将PIL 再次转换成tensor</span><br>img_size=trans_totensor(img_resize)<br><br></code></pre></td></tr></table></figure><p><strong>compose_resize</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">compose利用call()循环其内的命令<br><br>trans_resize_2=transforms.Resize(<span class="hljs-number">512</span>)  <span class="hljs-comment">#单参数将最小的那个边变成此时的值</span><br><span class="hljs-comment">##Compose([a,b,c])  将参数传递给a，将a生成的输出传递给b，将b的输出传递给c，输出此时c的输出</span><br>trans_compose=transforms.Compose([trans_resize_2,trans_tensor])<span class="hljs-comment"># 同时实现了改变大小和将PIL文件转换成tensor文件</span><br>img_resize_2=trans_compose(img)<br></code></pre></td></tr></table></figure><p><strong>RandomCrop</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#随机裁剪</span><br><br>trans_Crop=transforms.RandomCrop(<span class="hljs-number">30</span>)  <span class="hljs-comment">#裁剪30*30的方形</span><br>trans_compose_2=transforms.Compose([trans_Crop,trans_tensor])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    img_randomcrop=trans_compose_2(img)<br>    writer.add_image(<span class="hljs-string">&quot;RandomCrop&quot;</span>,img_randomcrop,i)  <span class="hljs-comment">#随机从原图中裁剪30*30的十张图片</span><br></code></pre></td></tr></table></figure><ul><li><strong>关注输入与输出</strong></li><li><strong>学会查看官方文档</strong></li><li><strong>关注方法需要什么参数</strong></li><li><strong>不知道返回类型  <em>print</em>   <em>print(type)</em>  <em>debug</em></strong></li></ul><p>==在Tensorboard使用一定要转换成tensor数据类型==</p><h2 id="9-torchvision中的数据集的使用">9.torchvision中的数据集的使用</h2><p>torchvision 0.9.0版本数据集地址：<a href="https://pytorch.org/vision/0.9/">https://pytorch.org/vision/0.9/</a><br>.io  输入输出模块<br>.models  常见的神经网络模块<br>.ops  特殊操作<br>.utils 常见工具(tensorboard)<br>.transforms</p><p>例如CIFAR数据集</p><p><img src="CIFAR10%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="CIFAR10数据集"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><br>train_set=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">True</span>,download=<span class="hljs-literal">True</span>)<br>test_set=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,download=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment">##可将下载链接拷贝到迅雷进行下载  下载完的数据集直接拷贝到自己新建的一个dataset文件夹，download自动设置为true，会自动校验下载和解压</span><br><br><span class="hljs-built_in">print</span>(test_set[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(test_set.classes)<br><br>img,target=test_set[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(img)<br><span class="hljs-built_in">print</span>(target)<br><span class="hljs-built_in">print</span>(test_set.classes[target])<br>img.show()<br><br>Files already downloaded <span class="hljs-keyword">and</span> verified<br>Files already downloaded <span class="hljs-keyword">and</span> verified<br>(&lt;PIL.Image.Image image mode=RGB size=32x32 at <span class="hljs-number">0x23C7C9B97E0</span>&gt;, <span class="hljs-number">3</span>)<br>[<span class="hljs-string">&#x27;airplane&#x27;</span>, <span class="hljs-string">&#x27;automobile&#x27;</span>, <span class="hljs-string">&#x27;bird&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;deer&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;frog&#x27;</span>, <span class="hljs-string">&#x27;horse&#x27;</span>, <span class="hljs-string">&#x27;ship&#x27;</span>, <span class="hljs-string">&#x27;truck&#x27;</span>]<br>&lt;PIL.Image.Image image mode=RGB size=32x32 at <span class="hljs-number">0x23C7C9B97E0</span>&gt;<br><span class="hljs-number">3</span><br>cat<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#转换成tensor类型</span><br><br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>dataset_transforms=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])<br><br>train_set=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">True</span>,transform=dataset_transforms,download=<span class="hljs-literal">True</span>)<br>test_set=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=dataset_transforms,download=<span class="hljs-literal">True</span>)<br><br>Writer=SummaryWriter(<span class="hljs-string">&quot;p10&quot;</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    img,target=test_set[i]<br>    Writer.add_image(<span class="hljs-string">&quot;test_set&quot;</span>,img,i)<br></code></pre></td></tr></table></figure><h2 id="10-Dataload的使用">10.Dataload的使用</h2><p>将dataset获取数据加载到神经网络之中</p><p>pytorch 1.8.1  ：<a href="https://pytorch.org/docs/1.8.1/data.html?highlight=dataloader#torch.utils.data.DataLoader">https://pytorch.org/docs/1.8.1/data.html?highlight=dataloader#torch.utils.data.DataLoader</a></p><p><img src="DataLoad.png" alt="DataLoad"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-comment">#数据集</span><br>test_dataset=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><span class="hljs-comment">#载入数据集</span><br>test_loader=DataLoader(dataset=test_dataset,batch_size=<span class="hljs-number">4</span>,shuffle=<span class="hljs-literal">True</span>,num_workers=<span class="hljs-number">0</span>,drop_last=<span class="hljs-literal">False</span>)<br><br>输出：<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>]) <span class="hljs-comment">#4张图片3通道32×32图片</span><br>tensor([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>])  <span class="hljs-comment">#target随机  默认随机策略</span><br><br><br><span class="hljs-comment">#batch_size=4 每次取出4张图片及其target 并打包成新的img，target</span><br><span class="hljs-comment">#shuffle=True  抓取完完整的一轮之后   之后一轮是否和前面一轮一样</span><br><span class="hljs-comment">#num_workers=0  仅仅主线程</span><br><span class="hljs-comment">#drop_last=False 不丢弃最后剩余的图片</span><br><br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">test_loader=DataLoader(dataset=test_dataset,batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-literal">True</span>,num_workers=<span class="hljs-number">0</span>,drop_last=<span class="hljs-literal">False</span>)<br><br>Writer=SummaryWriter(<span class="hljs-string">&quot;dataloader&quot;</span>)<br><br>step=<span class="hljs-number">0</span><br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_loader:<br>    imgs,targets=data<br>    <span class="hljs-comment"># print(imgs.shape)</span><br>    <span class="hljs-comment"># print(targets)</span><br>    Writer.add_images(<span class="hljs-string">&quot;test_data&quot;</span>,imgs,step)  <span class="hljs-comment">#此处为add_images  而非单个图片add_image</span><br>    step+=<span class="hljs-number">1</span><br><br>Writer.close()<br></code></pre></td></tr></table></figure><p><img src="Batch_size_64.png" alt="Batch_size_64"></p><p><img src="batch_size_64%E4%BD%9916.png" alt="batch_size_64余16"></p><p>==batch_size=64 每次抓取64张，最后一次剩余16张，由于drop_last设置为flase，即不丢弃最后的16张==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">test_loader=DataLoader(dataset=test_dataset,batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-literal">True</span>,num_workers=<span class="hljs-number">0</span>,drop_last=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):   <span class="hljs-comment">#epoch表示训练两轮  shuffle设置为True  即两轮不一样</span><br>    step = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_loader:<br>        imgs,targets=data<br>    <span class="hljs-comment"># print(imgs.shape)</span><br>    <span class="hljs-comment"># print(targets)</span><br>        Writer.add_images(<span class="hljs-string">&quot;Epoch:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch),imgs,step)  <span class="hljs-comment">#&quot;Epoch:&#123;&#125;&quot;.format(epoch) 表示出输出Epoch：0和Epoch：1</span><br>        <br>        step+=<span class="hljs-number">1</span><br>        <br>Writer.close()<br><br></code></pre></td></tr></table></figure><p><img src="epoch.png" alt="epoch"></p><p><img src="epoch1.png" alt="epoch1"></p><h2 id="11-神经网络的基本骨架—nn-module">11.神经网络的基本骨架—nn.module</h2><p><strong>torch.nn</strong>:<a href="https://pytorch.org/docs/1.8.1/nn.html">https://pytorch.org/docs/1.8.1/nn.html</a></p><p><strong>.Containers</strong>   神经网络骨架</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):  //Model为自定义的名字  括号内为调用的父类<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Model, <span class="hljs-variable language_">self</span>).__init__()  //调用nn.Mdoule 的初始化 重写nn.module的方法 可以用Ctrl+O 选择第一个<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)  //卷积<span class="hljs-number">1</span><br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)  //卷积<span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):   //前向传播  pytorch 根据前向传播能自动计算反向传播<br>        x = F.relu(<span class="hljs-variable language_">self</span>.conv1(x))  //conv1卷积<span class="hljs-number">1</span>  relu为非线性处理<br>        <span class="hljs-keyword">return</span> F.relu(<span class="hljs-variable language_">self</span>.conv2(x))//在进行一次卷积<span class="hljs-number">2</span> 非线性处理<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span>  nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NNtest</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,<span class="hljs-built_in">input</span></span>):<br>        output=<span class="hljs-built_in">input</span>+<span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> output<br><br>nntest=NNtest()  <span class="hljs-comment">##调用NNtest中的nn.module的 _init_方法</span><br>x=torch.tensor(<span class="hljs-number">1.0</span>)  <span class="hljs-comment">##x=tensor(1,)</span><br>output=nntest(x)  <span class="hljs-comment">##调用forward方法   nn.module中的__call__会自动把参数传递给forward进行处理</span><br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure><h2 id="12-卷积操作">12.卷积操作</h2><p><img src="conv.png" alt="conv"></p><p><img src="%E5%8F%82%E6%95%B0.png" alt="参数"></p><p><strong>stride</strong>：步长<br><strong>padding</strong>：填充操作（1，1）上下左右依次插入空行空列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span>  torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-built_in">input</span>=torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>],<br>                   [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>],<br>                   [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],<br>                   [<span class="hljs-number">5</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],<br>                   [<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]])<br><br>kernel=torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>],  //对应与weight<br>                    [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],<br>                    [<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]])<br><br><span class="hljs-comment">##conv2d的输入方式（minibatch，in_channel,iH,iW)  使用reshape进行尺寸改变  H代表行  W代表列</span><br><br><span class="hljs-built_in">input</span>=torch.reshape(<span class="hljs-built_in">input</span>,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">5</span>))<br>kernel=torch.reshape(kernel,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br><br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">input</span>.shape)<br><span class="hljs-built_in">print</span>(kernel.shape)<br><br>output=F.conv2d(<span class="hljs-built_in">input</span>,kernel,stride=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(output)<br><br><span class="hljs-comment">##padding操作</span><br>output2=F.conv2d(<span class="hljs-built_in">input</span>,kernel,stride=<span class="hljs-number">1</span>,padding=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br><span class="hljs-built_in">print</span>(output2)<br><br>结果：<br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br>tensor([[[[<span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>],<br>          [<span class="hljs-number">18</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>],<br>          [<span class="hljs-number">13</span>,  <span class="hljs-number">9</span>,  <span class="hljs-number">3</span>]]]])<br><br>tensor([[[[ <span class="hljs-number">1</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>, <span class="hljs-number">10</span>,  <span class="hljs-number">8</span>],<br>          [ <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>,  <span class="hljs-number">6</span>],<br>          [ <span class="hljs-number">7</span>, <span class="hljs-number">18</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>,  <span class="hljs-number">8</span>],<br>          [<span class="hljs-number">11</span>, <span class="hljs-number">13</span>,  <span class="hljs-number">9</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>],<br>          [<span class="hljs-number">14</span>, <span class="hljs-number">13</span>,  <span class="hljs-number">9</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">4</span>]]]])<br></code></pre></td></tr></table></figure><p><img src="%E8%BE%93%E5%87%BA3.png" alt="输出3"></p><p><img src="%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png" alt="计算公式"></p><h2 id="13-神经网络—卷积层">13.神经网络—卷积层</h2><p><a href="https://pytorch.org/docs/1.8.1/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">https://pytorch.org/docs/1.8.1/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</a></p><p><img src="conv2d.png" alt="conv2d"></p><p><img src="conv2d%E5%8F%82%E6%95%B0.png" alt="conv2d参数"></p><h4 id="参数解释">参数解释</h4><ul><li><strong>in_channels</strong>:(N,C~in~,H~in~,W~in~)  N=batch_size  C~in~为通道数  H~in~为行（高）  W~in~为宽（列）</li><li><strong>out_channels</strong>:(N,C~out~,H~out~,W~out~)  batch_size 始终不变</li><li><strong>kernel_size</strong>:(N,C,H,W)</li><li><strong>stride</strong>: 步长</li><li><strong>padding</strong>:填充（1，1）对输入图像周围填充一行一列</li><li><strong>padding_mode</strong>:’zeros’  填充0</li><li><strong>dilation</strong>：空洞卷积</li></ul><p><img src="%E5%8D%B7%E7%A7%AF%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="卷积示意图"></p><p><img src="%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF.png" alt="空洞卷积"></p><p><img src="%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF%E4%BD%9C%E7%94%A8.png" alt="空洞卷积作用"></p><ul><li><p><strong>groups</strong>: 分组卷积：<a href="https://zhuanlan.zhihu.com/p/490685194">https://zhuanlan.zhihu.com/p/490685194</a></p></li><li><p><strong>bias</strong>：偏置  偏置等于卷积核的个数即kernel_size中（N,C,H,W)中的C</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>data_set=torchvison.dataset.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transforme=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br>dataloader=DataLoader(data_set,batch_size=<span class="hljs-number">64</span>,shuttle=<span class="hljs-literal">True</span>,Drop_last=<span class="hljs-literal">False</span>)<br><br>Class NNconv2d(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NNconv2d,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1=Conv2d(in_channels=<span class="hljs-number">3</span>,out_channels=<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">3</span>,stride=<span class="hljs-number">1</span>,padding=<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x=<span class="hljs-variable language_">self</span>.conv1(x)<br>        <span class="hljs-keyword">return</span> x<br><br>Myconv2d=NNconv2d()<br>step=<span class="hljs-number">0</span><br>writer=SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    outputs=Myconv2d(imgs)<br>    <span class="hljs-comment">##inputs.size(64,3,32,32)</span><br>    <span class="hljs-comment">##outputs.size(64,6,30,30)  由于tensorboard只能输出RGB三通道的图像  因此需要把6通道转换为3通道</span><br>    <span class="hljs-comment">##batch_size 设置为-1  自动根据后面设置的参数设置batch_size</span><br>    writer.add_images(<span class="hljs-string">&quot;Input&quot;</span>,imgs,step)<br>    outputs=torch.reshape(outputs,(-<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">30</span>,<span class="hljs-number">30</span>))<br>    writer.add_images(<span class="hljs-string">&quot;Output&quot;</span>,outputs,step)<br>    step+=<span class="hljs-number">1</span><br>    <br>writer.close()<br><br></code></pre></td></tr></table></figure><h2 id="14-神经网络—最大池化的使用">14.神经网络—最大池化的使用</h2><p>==最大池化取卷积核框内的最大值==</p><p><img src="MaxPool2d.png" alt="MaxPool2d"></p><p><img src="MaxPool2d%E5%8F%82%E6%95%B0.png" alt="MaxPool2d参数"></p><p><strong>ceil_mode</strong>:True为cell  即对于卷积核当中有空元素（0）的时候 选择保留其中剩下的最大值     默认为False</p><p>一般只需要设置<strong>kernel_size</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> MaxPool2d<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><span class="hljs-comment"># input=torch.tensor([[1,2,0,3,1],</span><br><span class="hljs-comment">#                     [0,1,2,3,1],</span><br><span class="hljs-comment">#                     [1,2,1,0,0],</span><br><span class="hljs-comment">#                     [5,2,3,1,1],</span><br><span class="hljs-comment">#                     [2,1,0,1,1]],dtype=torch.float32)  #将矩阵中的整形LONG转换成浮点型</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># input=torch.reshape(input,(-1,1,5,5))</span><br><span class="hljs-comment"># print(input.shape)</span><br>data_set=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br>dataloader=DataLoader(data_set,batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">nnMaxpool</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(nnMaxpool,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.maxpool=MaxPool2d(kernel_size=<span class="hljs-number">3</span>,ceil_mode=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x=<span class="hljs-variable language_">self</span>.maxpool(x)<br>        <span class="hljs-keyword">return</span> x<br><br>Max_pool=nnMaxpool()<br><span class="hljs-comment"># output=Max_pool(input)</span><br><span class="hljs-comment"># print(output)</span><br>writer=SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>step=<span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    outputs=Max_pool(imgs)<br>    writer.add_images(<span class="hljs-string">&quot;out_put_maxpoll&quot;</span>,outputs,step)<br>    step+=<span class="hljs-number">1</span><br><br>writer.close()<br><br><br><br>输出：<br>tensor([[[[<span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],<br>          [<span class="hljs-number">5.</span>, <span class="hljs-number">1.</span>]]]])<br><br><span class="hljs-comment">#池化的作用：大大减小训练量，加快训练类似于电影1080P转换成720P  但仍然能满足需求</span><br></code></pre></td></tr></table></figure><p><img src="%E8%BE%93%E5%87%BA4.png" alt="输出4"></p><h2 id="15-神经网络—非线性激活">15.神经网络—非线性激活</h2><p>Padding Layers  几乎用不到  基本都可以通过卷积层中的padding实现</p><p>==Non—linear  Activations==</p><p><strong>Relu</strong></p><p><img src="ReLu1.png" alt="ReLu1"></p><p><img src="ReLu2.png" alt="ReLu2"></p><p><img src="ReLu3.png" alt="ReLu3"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span>  nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> ReLU<br><br><span class="hljs-built_in">input</span>=torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">0.5</span>],<br>                    [<span class="hljs-number">2</span>,-<span class="hljs-number">0.6</span>]])<br><br><span class="hljs-built_in">input</span>=torch.reshape(<span class="hljs-built_in">input</span>,(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NNRelu</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NNRelu,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.Relu1=ReLU()  <span class="hljs-comment">##无需输入</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x=<span class="hljs-variable language_">self</span>.Relu1(x)<br>        <span class="hljs-keyword">return</span> x<br><br>relu_results=NNRelu()<br>outputs=relu_results(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(outputs)<br></code></pre></td></tr></table></figure><p><strong>Sigmod</strong></p><p><img src="Sigmod1.png" alt="Sigmod1"></p><p><img src="Sigmod2.png" alt="Sigmod2"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span>  torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span>  torch.nn <span class="hljs-keyword">import</span> Sigmoid<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>data_set=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br>dataloader=DataLoader(data_set,batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NNsigmoild</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NNsigmoild,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.Sig=Sigmoid()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,output</span>):<br>        output=<span class="hljs-variable language_">self</span>.Sig(output)<br>        <span class="hljs-keyword">return</span> output<br><br>Mysigmoid=NNsigmoild()<br>writer=SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>step=<span class="hljs-number">0</span><br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    outputs=Mysigmoid(imgs)<br>    writer.add_images(<span class="hljs-string">&quot;Output_sigmoid&quot;</span>,outputs,step)<br>    step+=<span class="hljs-number">1</span><br><br>writer.close()<br><br><br><br></code></pre></td></tr></table></figure><p>==非线性变换的目的：为模型引入非线性特征，提高模型的泛化能力==</p><h2 id="16-神经网络—线性层及其他层的介绍">16.神经网络—线性层及其他层的介绍</h2><p><a href="https://pytorch.org/docs/1.8.1/nn.html">https://pytorch.org/docs/1.8.1/nn.html</a></p><p><img src="%E5%85%B6%E4%BB%96%E5%B1%82.png" alt="其他层"></p><p><strong>Linear(也就是常说的全连接层)</strong></p><p><img src="%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82.png" alt="全连接层"></p><p><img src="Linear.png" alt="Linear"></p><p><img src="Linear%E5%8F%82%E6%95%B0.png" alt="Linear参数"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">##将图片通过liner转换成1x3 </span><br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span>  torch.nn <span class="hljs-keyword">import</span> Linear<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>data_set=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br>dataloader=DataLoader(data_set,batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Haijun</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.Line=Linear(<span class="hljs-number">196608</span>,<span class="hljs-number">10</span>)  <span class="hljs-comment">#分别是输入和输出的样本量</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,<span class="hljs-built_in">input</span></span>):<br>        output=<span class="hljs-variable language_">self</span>.Line(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">return</span> output<br><br>Trans=Haijun()<br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    <span class="hljs-comment"># outputs=torch.reshape(imgs,(1,1,1,-1))</span><br>    outputs=torch.flatten(imgs)  <span class="hljs-comment">##flatten 可以直接展开一行操作  但是reshape可以指定大小</span><br>    <span class="hljs-built_in">print</span>(outputs.shape)<br>    outputs=Trans(outputs)<br>    <span class="hljs-built_in">print</span>(outputs.shape)<br></code></pre></td></tr></table></figure><h2 id="17-神经网络—搭建实战及Sequential序列的使用">17.神经网络—搭建实战及Sequential序列的使用</h2><p><strong>将神经网络各层连接</strong></p><p><img src="%E8%BF%9E%E6%8E%A5%E5%90%84%E5%B1%82.png" alt="连接各层"></p><p><strong>CIFAR训练模型</strong></p><p><img src="%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.png" alt="训练模型"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Haijun</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># self.cov1=Conv2d(in_channels=3,out_channels=32,kernel_size=5,stride=1,padding=2)</span><br>        <span class="hljs-comment"># self.maxpool1=MaxPool2d(2)</span><br>        <span class="hljs-comment"># self.cov2=Conv2d(32,32,5,1,2)</span><br>        <span class="hljs-comment"># self.maxpool2=MaxPool2d(2)</span><br>        <span class="hljs-comment"># self.cov3=Conv2d(32,64,5,1,2)</span><br>        <span class="hljs-comment"># self.maxpool3=MaxPool2d(2)</span><br>        <span class="hljs-comment"># self.flatten=Flatten()</span><br>        <span class="hljs-comment"># self.Lin1=Linear(1024,64)</span><br>        <span class="hljs-comment"># self.Lin2=Linear(64,10)</span><br><br>        <br>        <span class="hljs-comment">##  使用Sequential可以很有效的简化代码</span><br>        <span class="hljs-variable language_">self</span>.model1=Sequential(<br>            Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Flatten(),<br>            Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>),<br>            Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br>    <span class="hljs-keyword">def</span>  <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,output</span>):<br>        <span class="hljs-comment"># output=self.cov1(output)</span><br>        <span class="hljs-comment"># output=self.maxpool1(output)</span><br>        <span class="hljs-comment"># output=self.cov2(output)</span><br>        <span class="hljs-comment"># output=self.maxpool2(output)</span><br>        <span class="hljs-comment"># output=self.cov3(output)</span><br>        <span class="hljs-comment"># output=self.maxpool3(output)</span><br>        <span class="hljs-comment"># output=self.flatten(output)</span><br>        <span class="hljs-comment"># output=self.Lin1(output)</span><br>        <span class="hljs-comment"># output=self.Lin2(output)</span><br><br>        output=<span class="hljs-variable language_">self</span>.model1(output)<br><br>        <span class="hljs-keyword">return</span> output<br><br>haijun=Haijun()<br><span class="hljs-built_in">input</span>=torch.ones((<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>))  <span class="hljs-comment">##测试用代码  生成64batch 3通道 32x32  其中元素都为1的矩阵</span><br>output=haijun(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(output.shape)<br><br><span class="hljs-comment">##利用TensorBoard可视化</span><br><br>writer=SummaryWriter(<span class="hljs-string">&quot;./logs_seq&quot;</span>)<br>writer.add_graph(haijun,<span class="hljs-built_in">input</span>)<br>writer.close()<br></code></pre></td></tr></table></figure><p><img src="%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt="可视化"></p><h2 id="18-损失函数与反向传播">18.损失函数与反向传播</h2><p><strong>Loss Function作用：</strong><br>1、计算实际输出和目标之间的差距<br>2、为我们更新输出提供一定的依据（反向传播）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> L1Loss<br><br>inputs=torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],dtype=torch.float32)<br>targets=torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>],dtype=torch.float32)<br><br>inputs=torch.reshape(inputs,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))<br>outputs=torch.reshape(targets,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))  <span class="hljs-comment">##inputs 和 outputs输入输出大小一定要相同</span><br><br><span class="hljs-comment">##L1 function  绝对值</span><br>loss=L1Loss()<br><span class="hljs-comment">##Mse  平方差</span><br>loss_mse=nn.MSELoss()<br><br>results=loss(inputs,outputs)<br><span class="hljs-built_in">print</span>(results)<br>results2=loss_mse(inputs,outputs)<br><span class="hljs-built_in">print</span>(results2)<br></code></pre></td></tr></table></figure><p><strong>交叉熵：Crossentropyloss   分类问题</strong></p><p><img src="%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.png" alt="交叉熵损失"></p><p>分类问题输出：output[x1,x2,x3]  分别对应是每一类的概率</p><p>​Target=0（对应与第0类，对应与x1）只有当target完全命中的时候 -x[class]才会比较小  从而Loss比较小</p><p>交叉熵=—x1+ln(exp(x1)+exp(x2)+exp(x3))</p><p><img src="%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B12.png" alt="交叉熵损失2"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">x=torch.tensor([<span class="hljs-number">0.1</span>,<span class="hljs-number">0.2</span>,<span class="hljs-number">0.3</span>])<br>y=torch.tensor([<span class="hljs-number">1</span>])  <span class="hljs-comment">#(batch_size)</span><br>x=torch.reshape(x,(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))  <span class="hljs-comment">#转化为（batch_size,classes)</span><br>loss_cross=nn.CrossEntropyLoss()<br>result=loss_cross(x,y)<br><span class="hljs-built_in">print</span>(result)<br><br>结果：<br>tensor(<span class="hljs-number">1.1019</span>)<br><br><br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    outputs=haijun(imgs)<br>    results=loss(outputs,targets)<br>    <span class="hljs-built_in">print</span>(results)<br>    <br>    results.backword()  <span class="hljs-comment">#反向传播 生成梯度 后续利用优化器</span><br></code></pre></td></tr></table></figure><h2 id="19-优化器">19.优化器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, target <span class="hljs-keyword">in</span> dataset:<br>    optimizer.zero_grad()  <span class="hljs-comment">#梯度清0  防止梯度有初始值对下一轮训练造成影响</span><br>    output = model(<span class="hljs-built_in">input</span>)  <span class="hljs-comment">#模型训练的结果</span><br>    loss = loss_fn(output, target)  <span class="hljs-comment">#计算损失函数</span><br>    loss.backward()   <span class="hljs-comment">#误差反向传播 </span><br>    optimizer.step()  <span class="hljs-comment">#优化器根据梯度调整模型中的各项参数</span><br>    <br>    <br><span class="hljs-comment">##不同的优化器除了params(参数)，lr(学习速率)外还有许多其他要设置的参数，根据不同情况设置</span><br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#利用随机梯度下降</span><br><br>haijun=Haijun()<br>loss=nn.CrossEntropyLoss()<br>optimizer=optim.SGD(haijun.parameters(),lr=<span class="hljs-number">0.01</span>)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):  <span class="hljs-comment">#epoch为训练轮次</span><br>    running_loss=<span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>        imgs,targets=data<br>        outputs=haijun(imgs)<br>        loss_cross=loss(outputs,targets)<br>        optimizer.zero_grad()<br>        loss_cross.backward()<br>        optimizer.step()<br>        running_loss=running_loss+loss_cross  <br>    <span class="hljs-built_in">print</span>(running_loss)  <span class="hljs-comment">#每一轮总的loss和</span><br></code></pre></td></tr></table></figure><h2 id="20-现有模型的使用与修改">20.现有模型的使用与修改</h2><p>各模型地址：<a href="https://pytorch.org/vision/0.9/models.html#id2">https://pytorch.org/vision/0.9/models.html#id2</a></p><p>分类模型VGG举例(其预训练在ImageNet上进行)</p><p><img src="ImageNet.png" alt="ImageNet"></p><p>ImageNet需要scipy，且目前已不支持download=True下载</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><br><span class="hljs-comment"># dataset=torchvision.datasets.ImageNet(&#x27;./dataset&#x27;,split=&#x27;train&#x27;,transform=torchvision.transforms.ToTensor(),download=True)</span><br><br>vgg16_false=torchvision.models.vgg16(weights=<span class="hljs-literal">None</span>)  <span class="hljs-comment">#只是加载网络模型  目前使用weights 而非download 默认为None 不适用预训练权重</span><br><span class="hljs-comment"># vgg16_true=torchvision.models.vgg16(download=True)    #从ImageNet上下载训练好的模型参数</span><br><br><span class="hljs-built_in">print</span>(vgg16_false)<br><br><br></code></pre></td></tr></table></figure><p><img src="%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.png" alt="网络模型"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#修改现有网络模型 以Vgg16举例</span><br><br><span class="hljs-comment"># 在vgg16中的classifier层中增加一层线性层</span><br>vgg16_false.classifier.add_module(<span class="hljs-string">&#x27;add_linear&#x27;</span>,nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">10</span>))<br><br><br><span class="hljs-comment">#修改vgg16中的calssifier层中最后一层线性层</span><br>vgg16_false.classifier[<span class="hljs-number">6</span>]=nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><h2 id="21-网络模型的保存与读取">21.网络模型的保存与读取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><br><span class="hljs-comment">#保存方式1</span><br>torch.save(vgg16,<span class="hljs-string">&quot;vgg16_model1.pth&quot;</span>)  <span class="hljs-comment">#保存的是模型的结构+参数</span><br>model1=torch.load(<span class="hljs-string">&quot;vgg16_model1.pth&quot;</span>)  <span class="hljs-comment">#加载</span><br><br><span class="hljs-comment">#保存方式2</span><br>torch.save(vgg16.state_dict(),<span class="hljs-string">&quot;vgg16_model2.pth&quot;</span>)  <span class="hljs-comment">#保存的仅仅是模型参数  以字典的形式</span><br>model2=torch.load(<span class="hljs-string">&quot;vgg16_model2.pth&quot;</span>) <span class="hljs-comment">#加载（字典结构显示）</span><br><br><span class="hljs-comment">#将字典加载成模型</span><br>vgg16=torchvision.models.vgg16(weights=<span class="hljs-literal">None</span>)<br>vgg16.load_state_dict(torch.load(<span class="hljs-string">&quot;vgg16_model2.pth&quot;</span>))  <span class="hljs-comment">#此时Vgg16显示的就是结构+参数</span><br><br></code></pre></td></tr></table></figure><h2 id="22-模型训练">22.模型训练</h2><p><strong>以CIFAR10为数据集</strong></p><p>==train.py==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#准备数据集</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> *<br><br>trian_data=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br>test_data=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment">#length长度</span><br>train_data_size=<span class="hljs-built_in">len</span>(trian_data)<br>test_data_size=<span class="hljs-built_in">len</span>(test_data)<br><br><span class="hljs-comment">#数据集长度</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练数据集长度：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(train_data_size))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;测试数据集长度：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(test_data_size))<br><br><span class="hljs-comment">#利用dataloader加载数据集</span><br>trian_dataloader=DataLoader(trian_data,batch_size=<span class="hljs-number">64</span>)<br>test_dataloader=DataLoader(test_data,batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-comment">#创建网络模型</span><br>Haijun=haijun()<br><br><span class="hljs-comment">#创建损失函数</span><br>loss_fn=nn.CrossEntropyLoss()<br><br><span class="hljs-comment">#优化器</span><br>learning_rate=<span class="hljs-number">1e-2</span><br>optim=torch.optim.SGD(Haijun.parameters(),lr=learning_rate)<br><br><span class="hljs-comment">#设置训练网络的一些参数</span><br>total_train_step=<span class="hljs-number">0</span>  <span class="hljs-comment">#训练次数</span><br>total_test_step=<span class="hljs-number">0</span>  <span class="hljs-comment">#测试次数</span><br>epoch=<span class="hljs-number">10</span>  <span class="hljs-comment">#训练轮数</span><br><br><span class="hljs-comment">#添加Tensorboard</span><br>writer=SummaryWriter(<span class="hljs-string">&quot;./Mylogs&quot;</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-------------------------第&#123;&#125;论训练开始----------------------------&quot;</span>.<span class="hljs-built_in">format</span>(i))<br><br>    <span class="hljs-comment">#训练步骤开始</span><br>    <span class="hljs-comment">#Haijun.train()  调用模型的特定结构  例如drop_out层  batchnorm层</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> trian_dataloader:<br>        imgs,targets=data<br>        outputs=Haijun(imgs)<br>        loss=loss_fn(outputs,targets)<br>        <span class="hljs-comment">#优化器优化模型</span><br>        optim.zero_grad()<br>        loss.backward()<br>        optim.step()<br><br>        total_train_step=total_train_step+<span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> total_train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练次数：&#123;&#125;，Loss=&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_train_step,loss.item()))  <span class="hljs-comment">##loss.item() 将tensor(1)转换成数字1显示</span><br>            writer.add_scalar(<span class="hljs-string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)<br><br>    <span class="hljs-comment">#测试步骤开始</span><br>    <span class="hljs-comment">#Haijun.eval()  调用模型结构  例如drop_out  batchnorm层</span><br>    total_test_loss=<span class="hljs-number">0</span><br>    total_accuacy=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment">#去除梯度的影响</span><br>        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_dataloader:<br>            imgs,targets=data<br>            outputs=Haijun(imgs)<br>            loss=loss_fn(outputs,targets)<br>            total_test_loss=total_test_loss+loss.item()<br>            accuacy=(outputs.argmax(<span class="hljs-number">1</span>)==targets).<span class="hljs-built_in">sum</span>()<br>            total_accuacy=total_accuacy+accuacy<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_test_loss))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的Accuacy：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_accuacy/test_data_size))<br>    writer.add_scalar(<span class="hljs-string">&quot;test_loss&quot;</span>,total_test_loss,total_test_step)<br>    writer.add_scalar(<span class="hljs-string">&quot;test_accuacy&quot;</span>,total_accuacy/test_data_size,total_test_step)<br>    total_train_step=total_train_step+<span class="hljs-number">1</span><br><br>    <span class="hljs-comment">#保存文件</span><br>    torch.save(Haijun,<span class="hljs-string">&quot;Haijun_&#123;&#125;.pth&quot;</span>.<span class="hljs-built_in">format</span>(i))<br>    <span class="hljs-comment">#torch.save(Haijun.state_dict(),&quot;Haijun_&#123;&#125;&quot;.format(i))  官方推荐的保存方式</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型已保存&quot;</span>)<br><br>writer.close()<br></code></pre></td></tr></table></figure><p>==model.py==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 搭建神经网络模型</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">haijun</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, output</span>):<br>        output = <span class="hljs-variable language_">self</span>.model(output)<br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    Haijun=haijun()<br>    <span class="hljs-built_in">input</span>=torch.ones((<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>))  <span class="hljs-comment">##64batch_size  3通道 32x32位  全为1的矩阵</span><br>    output=Haijun(<span class="hljs-built_in">input</span>)<br>    <span class="hljs-built_in">print</span>(output.shape)<br></code></pre></td></tr></table></figure><p><img src="train_loss.png" alt="train_loss"></p><p><img src="testL_loss.png" alt="testL_loss"></p><p><img src="test_accuacy.png" alt="test_accuacy"></p><h2 id="23-CUDA加速">23.CUDA加速</h2><p>==第一种cpu加速方式：1、网络模型     2、数据（输入、标注）    3、损失函数    调用.cuda()==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#计算时间代码</span><br><span class="hljs-keyword">import</span> time  <span class="hljs-comment">#记时</span><br><br><span class="hljs-comment">#模型加速</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():  <span class="hljs-comment">#判断cuda()方法是否可用</span><br>    Haijun=Haijun.cuda()  <span class="hljs-comment">#网络模型的cuda方法</span><br>    <br><span class="hljs-comment">#损失函数加速</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    loss_fn=loss_fn.cuda()  <span class="hljs-comment">#损失函数的cuda()方法</span><br>    <br><span class="hljs-comment">#数据加速</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> trian_dataloader:<br>        imgs,targets=data<br>        <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>            imgs=imgs.cuda()  <span class="hljs-comment">#数据的cuda()方法</span><br>            targets=targets.cuda()<br></code></pre></td></tr></table></figure><p>时间对比：</p><p><img src="%E8%BE%93%E5%87%BA5.png" alt="输出5"></p><p><img src="%E8%BE%93%E5%87%BA6.png" alt="输出6"></p><p>利用Goole colab云GPU加速</p><p><a href="https://colab.research.google.com/">https://colab.research.google.com/</a></p><p><code>!nvidia-smi</code></p><p><img src="%E8%BE%93%E5%87%BA7.png" alt="输出7"></p><p><img src="CUDA.png" alt="CUDA"></p><p>==第二种cpu加速方式：1、网络模型     2、数据（输入、标注）    3、损失函数==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#调用CPU</span><br>device=torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)  <span class="hljs-comment">#定义训练的设备</span><br>device=torch.device(<span class="hljs-string">&quot;cuda&quot;</span><span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span><span class="hljs-string">&quot;cpu&quot;</span>)  <span class="hljs-comment">#有cuda用cuda  无cuda用cpu</span><br><br>device=torch.device(<span class="hljs-string">&quot;cuda&quot;</span>)<br>       torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span>)  <span class="hljs-comment">#指定第一张显卡</span><br>       torch.device(<span class="hljs-string">&quot;cuda:1&quot;</span>)  <span class="hljs-comment">#指定第二张显卡</span><br><br>Haijun.to(device)<br>loss_fn.to(device)<br><span class="hljs-comment">#针对非数据类型 上述写法即可不需要赋值操作  下面的写法也正确</span><br><span class="hljs-comment">#Haijun=Haijun.to(device)</span><br><span class="hljs-comment">#loss_fn=loss_fn.to(device)  </span><br><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> trian_dataloader:<br>        imgs,targets=data<br>        imgs=imgs.to(device)  <span class="hljs-comment">#数据的cuda()方法</span><br>        targets=targets.to(device)<br></code></pre></td></tr></table></figure><h2 id="24-完整的模型验证demo">24.完整的模型验证demo</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br>image_path=<span class="hljs-string">&quot;./data/1.png&quot;</span><br>image=Image.<span class="hljs-built_in">open</span>(image_path)<br><span class="hljs-built_in">print</span>(Image)<br><br>image=image.convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)  <span class="hljs-comment">#png多一个透明通道  转换成三通道</span><br><br>transform=torchvision.transforms.Compose([torchvision.transforms.Resize((<span class="hljs-number">32</span>,<span class="hljs-number">32</span>)),<br>                                          torchvision.transforms.ToTensor()])<br><br><br>image=transform(image)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">haijun</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, output</span>):<br>        output = <span class="hljs-variable language_">self</span>.model(output)<br>        <span class="hljs-keyword">return</span> output<br>Haijun=haijun()<br><br>model=torch.load(<span class="hljs-string">&quot;Haijun_9.pth&quot;</span>)<br><span class="hljs-comment">#model=torch.load(&quot;Haijun_9.pth&quot;,map_location=torch.device(&#x27;cpu&#x27;))  #GPU训练的Haijun_9.pth  yaozai</span><br><span class="hljs-built_in">print</span>(model)<br><br>image=torch.reshape(image,(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>))<br>model.<span class="hljs-built_in">eval</span>()<br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    output=model(image)<br><span class="hljs-built_in">print</span>(output)<br><br><span class="hljs-built_in">print</span>(output.argmax(<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Linux/UNIX文件系统</title>
    <link href="/Diffcc/Diffcc.github.io/2025/06/07/20250607/"/>
    <url>/Diffcc/Diffcc.github.io/2025/06/07/20250607/</url>
    
    <content type="html"><![CDATA[<p><img src="linux.jpg" alt="linux"></p><p>文件系统给是对文件和目录的组织集合，</p><p>本文探究<strong>Linux</strong>下的设备文件系统，ext系列系统，日志文件系统设计</p><span id="more"></span><h2 id="设备文件系统">设备文件系统</h2><h3 id="设备文件">设备文件</h3><p>在Linux内核中，每种设备类型（真正存在or虚拟抽象）的都有与之对应的设备驱动程序，内核通过<strong>设备驱动程序</strong>的API接口，实现对于设备的操控，虽然每个设备都有差异，但是接口类似，进而能够很好的承接系统调用本身。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">/dev <span class="hljs-comment">#dev目录下设备文件</span><br><br><span class="hljs-comment">#超级用户使用mknod来创建设备文件</span><br></code></pre></td></tr></table></figure><p><img src="dev.png" alt="dev"></p><p>输入输出设备可分为两大类：<strong>块设备（<em>Block Device</em>）<strong>和</strong>字符设备（<em>Character Device</em>）</strong></p><ul><li>字符型设备基于每个字符来处理数据。终端、键盘、鼠标都属于字符型设备。</li><li>块设备每次处理一块设备。块的大小取决于设备类型，硬盘，USB都属于块设备。</li></ul><h3 id="磁盘设备">磁盘设备</h3><p>磁盘由盘面，磁道，柱面和扇区构成，数据的读/写按柱面进行，而不按盘面进行。也就是说，一个磁道写满数据后，就在同一柱面的下一个盘面来写，<em><strong>*一个柱面写满后，才移到下一个扇区开始写数据*</strong></em>。读数据也按照这种方式进行，这样就提高了硬盘的读/写效率。</p><p><img src="disk.png" alt="disk"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">fdisk -l <span class="hljs-comment">#列出所有磁盘的分区</span><br></code></pre></td></tr></table></figure><p>磁盘分区可以容纳任何类型的信息，但通常只包含<strong>下面之一</strong>：</p><ul><li>文件系统：存放常规文件。</li><li>数据区域： 充当裸设备。</li><li>交换区域：内存管理之用 特权进程通过***swapon()<em><strong>和</strong></em>swapoff()***启动和停止磁盘分区操作。</li></ul><p>详情可参考：<a href="https://blog.csdn.net/hguisu/article/details/7408047">https://blog.csdn.net/hguisu/article/details/7408047</a></p><h3 id="ext系列文件系统">ext系列文件系统</h3><p>**ext2（second extended file system）**扩展文件系统二世是Linux上使用最广泛的文件系统</p><p>ext2文件系统以<em><strong>block</strong></em>为基本单位，包括引导块，超级块，i节点表和数据块部分：</p><ul><li>引导块可以理解为Linux系统的<strong>init()</strong>，用来引导操作系统的信息。</li><li><strong>超级块</strong>则是标记了i节点表的大小，逻辑块的大小信息。</li><li><em><strong>i-list</strong></em>维护了文件类型、属主、硬链接数、<strong>指向文件数据块</strong>的指针等信息，在ext2中，每个i节点包括了15个指针，前12个指针用于直接索引文件数据库的位置，保证直接访问一击必中，后四个文件块通过指向一个<strong>指针块</strong>，分化为指向更多的数据块，以保证容纳大体量的文件，同时可以将未指向数据块的指针块中的指针标记未0，则无需未<strong>文件黑洞</strong>分配空字节数据块。</li><li><strong>数据块</strong>构成了文件和目录，用于存放数据。</li></ul><p><img src="inode.png" alt="inode"></p><p><img src="ext2.png" alt="ext2"></p><p><strong>ext3</strong>文件系统允许<code>journaling</code>日志，<code>journaling</code>日志是在文件系单独的区域存储，每当文件系统意外崩溃，采用<code>journaling</code>日志可以进行恢复，<strong>和ext2文件系统不同的是多出了<code>journaling</code>日志的功能</strong>。</p><ul><li>ext3提供三种journal日志模式，分别是<code>writeback</code>、<code>ordered</code>、<code>journal</code>.<code>writeback</code>仅仅会记录元数据的日志，数据可以直接写到磁盘，但是不保证数据比元数据先落盘，这也是性能最好的，但是<a href="https://cloud.tencent.com/product/dsgc?from_column=20065&amp;from=20065">数据安全</a>性最低；<code>ordered</code>也是仅仅是记录元数据块的日志，这种模式是将文件的数据相关的元数据和数据在一个事务中，当元数据写入到磁盘时候，把对应的数据也写到磁盘，这样是先数据刷盘，再做元数据日志。<code>journal</code>提供数据和元数据的日志，所有的文件数据都先写到日志，然后在写到磁盘，数据需要写2次，性能是最差的。</li><li>ext2中在目录项中查找文件时间的复杂度是<code>O(n)</code>，ext3中采用了<code>h-trees</code>查找效率提高了很多</li></ul><p><img src="ext3.png" alt="ext3"></p><p><strong>ext4</strong>是从ext3 fork而来,针对ext4最大的feature就是ext4采用<code>extents</code>机制，替代了<code>indirect block</code>寻址的方式。ext4尽量会把<a href="https://cloud.tencent.com/product/cos?from_column=20065&amp;from=20065">数据存储</a>在连续的block区域内，为了实现这个ext4需要知道三个信息，第一是文件的初始化block.其次是块的数量，最后是磁盘上初始化块的数据，这些信息统一抽象以<code>struct ext4_extent</code>呈现。</p><p><img src="ext4.png" alt="ext4"></p><p>详情可参考：<a href="https://cloud.tencent.com/developer/article/2074590">https://cloud.tencent.com/developer/article/2074590</a></p><h3 id="虚拟文件系统">虚拟文件系统</h3><p>除了ext系列文件系统，还有诸如Reiserfs，VFAT，NFS等文件系统，为了能和各种文件系统打交道，应用程序通过<strong>虚拟文件系统（VFS</strong>）这一层抽象层中定义的通用接口（<em><strong>open()、read()、write()、lseek()</strong></em>….），来实现对不同文件系统的访问。</p><p>不同的文件系统对比可参考：<a href="https://zhuanlan.zhihu.com/p/689551298">https://zhuanlan.zhihu.com/p/689551298</a></p><p><img src="%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94.png" alt="文件系统性能对比"></p><h3 id="日志文件系统">日志文件系统</h3><p><strong>日志文件系统</strong>的作用在于避免了系统崩溃之后，为了确保文件系统的完整，需要遍历整个文件系统，来检查一致性（<em><strong>fcsk</strong></em>）。</p><p>日志文件系统会将更新操作记录在专门的磁盘日志文件中，可利用日志重做（<em><strong>redo</strong></em>）任何不完整更新（日志系统总能够保证将<strong>文件元数据事务</strong>作为一个完整单元提交至磁盘）。</p><h3 id="文件挂载">文件挂载</h3><p>文件挂载（Mount）的作用是将存储设备（如硬盘分区、光盘、网络存储等）或虚拟文件系统（如<code>proc</code>、<code>sysfs</code>等）关联到目录树的某个位置（挂载点），使得用户和程序可以<strong>通过文件系统接口访问这些资源</strong>。</p><p><strong>Linux/UNIX</strong>所有的文件系统中的文件都位于单根目录树下（<em><strong>”/“</strong></em>），其他的文件系统都挂载在根目录下，被视为整个目录层级的子树（<em><strong>subtree</strong></em>）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 将device文件系统挂载在directory所指定的目录下</span><br>mount device directory<br><br><span class="hljs-comment"># mount用于列出当前已挂载的文件系统，unmount用于卸载</span><br>mount<br></code></pre></td></tr></table></figure><p><img src="mount.png" alt="mount"></p><p>用于查看当前已挂载和可挂载的文件信息：<code>/proc/mounts</code>，<code>/etc/mtab</code>，<code>/etc/fstab</code></p><p><img src="proc_mounts.png" alt="proc_mounts"></p><ul><li><strong>文件系统类型</strong>: <code>sysfs</code>（虚拟文件系统，用于导出内核对象信息）</li><li><strong>挂载点</strong>: <code>/sys</code></li><li><strong>文件系统类型</strong>: <code>sysfs</code></li><li><strong>挂载标志</strong>:<ul><li><code>rw</code>: 以读写方式挂载</li><li><code>nosuid</code>: 忽略文件系统中的SUID/SGID权限位（禁用setuid/setgid效果）</li><li><code>nodev</code>: 不允许解释文件系统上的设备文件（禁用设备文件）</li><li><code>noexec</code>: 禁止直接执行该文件系统上的程序</li><li><code>relatime</code>: 仅在访问时间早于修改时间时更新访问时间（优化性能）</li></ul></li><li><strong>数字字段</strong>:<ul><li><code>0</code>: dump备份标志（0表示不备份）</li><li><code>0</code>: fsck检查顺序（0表示不检查）</li></ul></li></ul><h4 id="高级挂载特性">高级挂载特性</h4><p><strong>多个挂载点挂在文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount /dev/sda12 /test1<br>mount /dev/sda12 /test2<br><br><span class="hljs-comment">#在test1挂载点下操作，test2下完全可见</span><br></code></pre></td></tr></table></figure><p><strong>多次挂载同一挂载点</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">每次新挂载会隐藏之前可见于挂载点下的目录子树<br>mount /dev/sda12 /testfs<br>mount /dev/sda12 /testfs<br><br><span class="hljs-comment">#卸载最后一次挂载，上次挂载内容可见</span><br></code></pre></td></tr></table></figure><p><strong>基于每次挂载的挂载标志</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#mountflag可以基于每次挂载来设置</span><br></code></pre></td></tr></table></figure><p><strong>绑定挂载</strong></p><p><strong>两处同时可见，类似于硬链接，但可以针对目录</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">MS_BIND<br>mount --<span class="hljs-built_in">bind</span><br></code></pre></td></tr></table></figure><p><strong>递归绑定挂载</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">MS_BIND OR MS_REC<br></code></pre></td></tr></table></figure><h3 id="虚拟内存文件系统-tmpfs">虚拟内存文件系统: tmpfs</h3><p>tmpfs 是 Linux 内核中的一个虚拟文件系统，它将数据<strong>存储在内存中</strong>而不是硬盘上。使用 tmpfs 可以快速访问数据，因为数据存储在 <em><strong>RAM</strong></em> 中，而不需要进行实际的磁盘 <em><strong>I/O</strong></em> 操作。</p><p>可使用如下命令挂载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Bash">mount -t tmpfs <span class="hljs-built_in">source</span> target <span class="hljs-comment">#source:名称 target:挂载点</span><br></code></pre></td></tr></table></figure><p>除了用于用户应用程序之外，tmpfs文件系统还有下面两个用途:</p><ul><li>由内核内部挂载隐形tmpfs文件系统，用于实现<em><strong>System V</strong></em>共享内存和共享匿名内存映射</li><li>挂在于***/dev/shm*** 的tmpfs文件系统 ，为<em><strong>glibc</strong></em>用以实现POSIX共享内存和POSIX信号量</li></ul><p>系统可通过<code>statvfs()</code> 和 <code>fstatvfs()</code>库函数获取已挂载文件系统的有关信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#include &lt;sys/statvfs.h&gt;</span><br><br>int statvfs(const char*pathname, struct statvfs* statvfsbuf);<br>int fstatvfs(int fd, struct statvfs* statvfsbuf);<br><br><span class="hljs-comment">#statvfsbuf为指向statvfs结构体的缓冲区，内含文件系统的信息</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Linux系统编程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
