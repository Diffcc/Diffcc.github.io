<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>TransformerEngine加速库</title>
    <link href="/Diffcc/Diffcc.github.io/2025/08/04/20250804/"/>
    <url>/Diffcc/Diffcc.github.io/2025/08/04/20250804/</url>
    
    <content type="html"><![CDATA[<p>NVIDIA TransformerEngine是一个专注于优化Transformer模型训练和推理性能的高效工具库。该项目通过提供FP8混合精度支持、优化的注意力机制实现以及分布式训练集成等功能，显著提升了大规模语言模型的训练效率。</p><span id="more"></span><p>TransformerEngine针对Transformer里面的Kernel进行了重写, 以支持FP8特征, 扩展了Pytorch的功能.</p><p>使用方式也是比较简单，使用该拓展额外包的一层 Module 来搭建网络，即可，最后再包一层<a href="https://zhida.zhihu.com/search?content_id=250278123&amp;content_type=Article&amp;match_order=1&amp;q=%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83&amp;zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NTQ0NTA3ODQsInEiOiLmt7flkIjnsr7luqborq3nu4MiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoyNTAyNzgxMjMsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.zBU5ymd_pDNpWuxjHBihRgeQrGGH2jKN4QJhV_lNvM8&amp;zhida_source=entity">混合精度训练</a>作用域：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> transformer_engine.pytorch <span class="hljs-keyword">as</span> te<br><span class="hljs-keyword">from</span> transformer_engine.common <span class="hljs-keyword">import</span> recipe<br><br><span class="hljs-comment"># Set dimensions.</span><br>in_features = <span class="hljs-number">768</span><br>out_features = <span class="hljs-number">3072</span><br>hidden_size = <span class="hljs-number">2048</span><br><br><span class="hljs-comment"># Initialize model and inputs.</span><br>model = te.Linear(in_features, out_features, use_bias=<span class="hljs-literal">True</span>)<br>inp = torch.randn(hidden_size, in_features, device=<span class="hljs-string">&quot;cuda&quot;</span>)<br><br><span class="hljs-comment"># 创建FP8训练的配置</span><br>fp8_recipe = recipe.DelayedScaling(margin=<span class="hljs-number">0</span>, interval=<span class="hljs-number">1</span>, fp8_format=recipe.Format.E4M3)<br><br><span class="hljs-comment"># FP8的autocast</span><br><span class="hljs-keyword">with</span> te.fp8_autocast(enabled=<span class="hljs-literal">True</span>, fp8_recipe=fp8_recipe):<br>    out = model(inp)<br><br>loss = out.<span class="hljs-built_in">sum</span>()<br>loss.backward()<br></code></pre></td></tr></table></figure><p>在混合精度训练的情况下, 因为FP16本身的数值范围还是足够大的, <strong>在AMP(Automatic Mixed Prescision)下, 只在最后一个Loss做一个scaling</strong>, 这个步骤足以保证整个模型在运算过程中不会产生溢出.</p><p>而FP8相比FP16减少了更多有效位, 因此不能简单复用FP16下的策略, 需要给每个FP8 Tensor单独设置一个合适的scale factor. Transformer Engine需要动态地对输入范围进行调整, 如图所示:</p><p>![H100 TransformerEngine](H100 TransformerEngine.PNG)</p><p><img src="FP8%E9%87%8F%E5%8C%96%E7%AE%A1%E7%90%86%E5%99%A8.PNG" alt="FP8量化管理器" /></p><p>具体到每一个算子实现动态范围调整的原理很简单, 通过记录历史的abs max值, 来去调整最终缩放的范围.</p><p>其主要的 Kernel 实现都放在了 <a href="https://github.com/NVIDIA/TransformerEngine/tree/main/transformer_engine/common">common</a> 目录下，我们以 gelu 这个 kernel 为例，最终它会调用到 <code>vectorized_pointwise.h</code>这个文件，我们主要看 <code>unary_kernel</code></p><h4 id="unary-kernel">unary_kernel</h4><p>这个核函数模板跟常规的 elementwise 向量化模板是类似的。</p><p>首先会让每个线程获取到 scale 值</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs cpp">ComputeType s = <span class="hljs-number">0</span>;<br><span class="hljs-function"><span class="hljs-keyword">if</span> <span class="hljs-title">constexpr</span> <span class="hljs-params">(is_fp8&lt;OutputType&gt;::value)</span> </span>&#123;<br>    <span class="hljs-comment">// 获取scale值</span><br>    <span class="hljs-keyword">if</span> (scale != <span class="hljs-literal">nullptr</span>) s = *scale;<br>    <span class="hljs-comment">// 将scale取倒数写回scale_inv</span><br>    <span class="hljs-keyword">if</span> (blockIdx.x == <span class="hljs-number">0</span> &amp;&amp; threadIdx.x == <span class="hljs-number">0</span> &amp;&amp; scale_inv != <span class="hljs-literal">nullptr</span>) &#123;<br>      <span class="hljs-built_in">reciprocal</span>&lt;ComputeType&gt;(scale_inv, s);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>其中在循环里，线程会不断更新他运算结果的最大值，并且最终运算结果要乘上 scale 值：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 实际运算发生</span><br>ComputeType temp = <span class="hljs-built_in">OP</span>(val, p);<br><span class="hljs-function"><span class="hljs-keyword">if</span> <span class="hljs-title">constexpr</span> <span class="hljs-params">(is_fp8&lt;OutputType&gt;::value)</span> </span>&#123;<br>  __builtin_assume(max &gt;= <span class="hljs-number">0</span>);<br>  max = <span class="hljs-built_in">fmaxf</span>(<span class="hljs-built_in">fabsf</span>(temp), max);<br><br>  <span class="hljs-comment">// 缩放</span><br>  temp = temp * s;<br>&#125;<br></code></pre></td></tr></table></figure><p>当 Kernel 主体运算完毕后，再也 warp 为单位做一个 reduce_max，获取到线程束内的最大值，再通过 atomicMax 原子操作，不断更新全局最大值：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-keyword">if</span> <span class="hljs-title">constexpr</span> <span class="hljs-params">(is_fp8&lt;OutputType&gt;::value)</span> </span>&#123;<br>  <span class="hljs-comment">/* warp tile amax reduce*/</span><br>  max = <span class="hljs-built_in">reduce_max</span>&lt;unary_kernel_threads / THREADS_PER_WARP&gt;(max, warp_id);<br><br>  <span class="hljs-keyword">if</span> (threadIdx.x == <span class="hljs-number">0</span> &amp;&amp; amax != <span class="hljs-literal">nullptr</span>) &#123;<br>      <span class="hljs-built_in">static_assert</span>(std::is_same&lt;ComputeType, <span class="hljs-type">float</span>&gt;::value);<br>      <span class="hljs-comment">// 更新全局最大值</span><br>      <span class="hljs-built_in">atomicMaxFloat</span>(amax, max);<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>其他 layernorm 等 Kernel 也是诸如类似的逻辑，这里就不再展开了.</p><h4 id="Python-API">Python API</h4><h5 id="DelayedScaling">DelayedScaling</h5><p>从前面的示例代码我们可以看到一个比较重要的 API 是<code>DelayedScaling</code>，我们可以根据官方文档查看各个参数含义：</p><ul><li>margin 计算 scale 的偏移量</li><li>interval 控制计算 scale factor 的频率</li><li>fp8_format 使用 FP8 的格式，FP8 有 E4M3 和 E5M2，但是现在不支持纯 E5M2 的格式训练</li><li>amax_history_len 记录 abs maxval 的历史窗口大小</li><li>amax_compute_algo 在窗口里选择 absmax 的算法，'max’则是选择历史窗口里最大值，'most_recent’则是选择近期的值，当然你也可以传一个自定义的函数</li></ul><p>相关代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-meta">@torch.jit.script</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_default_get_amax</span>(<span class="hljs-params"></span><br><span class="hljs-params">    amax_history: torch.Tensor,</span><br><span class="hljs-params">    amax_compute_algo: <span class="hljs-built_in">str</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Tuple</span>[torch.Tensor, torch.Tensor]:<br>    <span class="hljs-string">&quot;&quot;&quot;Default function to obtain amax from history.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> amax_compute_algo == <span class="hljs-string">&quot;max&quot;</span>:<br>        amax = torch.<span class="hljs-built_in">max</span>(amax_history, dim=<span class="hljs-number">0</span>).values<br>    <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># amax_compute_algo == &quot;most_recent&quot;</span><br>        amax = amax_history[<span class="hljs-number">0</span>]<br><br>    amax_history = update_amax_history(amax_history)<br>    <span class="hljs-keyword">return</span> amax_history, amax<br></code></pre></td></tr></table></figure><ul><li>scaling_factor_compute_algo 计算 scale factor 的算法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-meta">@torch.jit.script</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_default_sf_compute</span>(<span class="hljs-params"></span><br><span class="hljs-params">    amax: torch.Tensor,</span><br><span class="hljs-params">    scale: torch.Tensor,</span><br><span class="hljs-params">    fp8_max: <span class="hljs-built_in">float</span>,</span><br><span class="hljs-params">    margin: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params"></span>) -&gt; torch.Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;Default function to convert amax to scaling factor.&quot;&quot;&quot;</span><br>    exp = torch.floor(torch.log2(fp8_max / amax)) - margin<br>    sf = torch.<span class="hljs-built_in">round</span>(torch.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>, torch.<span class="hljs-built_in">abs</span>(exp)))<br>    sf = torch.where(amax &gt; <span class="hljs-number">0.0</span>, sf, scale)<br>    sf = torch.where(torch.isfinite(amax), sf, scale)<br>    sf = torch.where(exp &lt; <span class="hljs-number">0</span>, <span class="hljs-number">1</span> / sf, sf)<br><br>    <span class="hljs-keyword">return</span> sf<br></code></pre></td></tr></table></figure><ul><li>override_linear_precision 由 3 个 bool 值，分别控制 fprop 前向，dgrad，wgrad 三个矩阵乘是否用更高的精度来计算，默认都为 False</li></ul><h5 id="TransformerEngineBaseModule">TransformerEngineBaseModule</h5><p>相关的 Kernel 除了要完成自己的计算任务，也得实时维护 amax 这些值，因此也需要对应修改 nn.Module，这里 TransformerEngine 继承了 nn.Module，并且增加了一些 buffer 维护的机制，这些 buffer 用于存储动态 scale 的信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerEngineBaseModule</span>(torch.nn.Module, ABC):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        ...<br>        <span class="hljs-variable language_">self</span>.fp8 = <span class="hljs-literal">False</span><br>        <span class="hljs-variable language_">self</span>.fp8_meta = &#123;&#125;<br>        <span class="hljs-variable language_">self</span>.fp8_meta[<span class="hljs-string">&quot;fp8_group&quot;</span>] = <span class="hljs-literal">None</span><br>        <span class="hljs-variable language_">self</span>.fp8_meta[<span class="hljs-string">&quot;recipe&quot;</span>] = get_default_fp8_recipe()<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fp8_init</span>(<span class="hljs-params">self, num_gemms: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize fp8 related metadata and tensors during fprop.&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># If fp8 isn&#x27;t enabled, turn off and return.</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> is_fp8_enabled():<br>            <span class="hljs-variable language_">self</span>.fp8 = <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">return</span><br><br>        <span class="hljs-comment"># FP8 is already enabled and recipe is the same, don&#x27;t do anything.</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.fp8 <span class="hljs-keyword">and</span> get_fp8_recipe() == <span class="hljs-variable language_">self</span>.fp8_meta[<span class="hljs-string">&quot;recipe&quot;</span>]:<br>            <span class="hljs-keyword">return</span><br><br>        <span class="hljs-comment"># Set FP8, recipe, and other FP8 metadata</span><br>        <span class="hljs-variable language_">self</span>.fp8 = <span class="hljs-literal">True</span><br>        <span class="hljs-variable language_">self</span>.fp8_meta[<span class="hljs-string">&quot;recipe&quot;</span>] = get_fp8_recipe()<br>        <span class="hljs-variable language_">self</span>.fp8_meta[<span class="hljs-string">&quot;num_gemms&quot;</span>] = num_gemms<br>        <span class="hljs-variable language_">self</span>.fp8_meta[<span class="hljs-string">&quot;fp8_group&quot;</span>] = get_fp8_group()<br><br>        <span class="hljs-comment"># Set FP8_MAX per tensor according to recipe</span><br>        <span class="hljs-variable language_">self</span>.fp8_meta[<span class="hljs-string">&quot;fp8_max_fwd&quot;</span>] = <span class="hljs-variable language_">self</span>.fp8_meta[<span class="hljs-string">&quot;recipe&quot;</span>].fp8_format.value.max_fwd<br>        <span class="hljs-variable language_">self</span>.fp8_meta[<span class="hljs-string">&quot;fp8_max_bwd&quot;</span>] = <span class="hljs-variable language_">self</span>.fp8_meta[<span class="hljs-string">&quot;recipe&quot;</span>].fp8_format.value.max_bwd<br><br>        <span class="hljs-comment"># Allocate scales and amaxes</span><br>        <span class="hljs-variable language_">self</span>.init_fp8_meta_tensors()<br></code></pre></td></tr></table></figure><p>而相关 Module 如 LayerNormMLP 继承该 Module，并且传入 fp8_meta 信息更新：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LayerNormMLP</span>(<span class="hljs-title class_ inherited__">TransformerEngineBaseModule</span>):<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">...</span>): <br>  out = _LayerNormMLP.apply(<br>            ..., <br>            <span class="hljs-variable language_">self</span>.fp8,<br>            <span class="hljs-variable language_">self</span>.fp8_meta,<br>        )<br></code></pre></td></tr></table></figure><h4 id="FP8-datatype">FP8 datatype</h4><p><img src="%E4%B8%8D%E5%90%8C%E6%95%B0%E5%80%BC%E7%B1%BB%E5%9E%8B.png" alt="不同数值类型" /></p><p>FP8数据格式有两种, 分别是E4M3和E5M2:</p><ol><li>E4M3有1位符号位, 4位指数位, 3位尾数位, 可以存储的范围是+/-448和nan</li><li>E5M2有1位符号位, 5位指数位, 2位尾数位, 可以存储的范围是+/-57344,+/-inf和nan</li></ol><p>通常在NN网络中, 前向的激活和权重需要更高的精度, 所以使用E4M3, 而在反向传播过程中, 网络中流动的梯度通常对精度损失不那么敏感, 但却需要更高的动态范围, 因此, 反向使用E5M2来存储是最佳选择.</p><p>H100的Tensor Core支持这些类型的任意组合作为输入, 使得我们能够为每种张量选择其中最适合的精度进行存储.</p><h4 id="FP16混合精度训练">FP16混合精度训练</h4><p>用于FP16训练的混合精度方案包含两个组成部分：<strong>选择哪些运算应以FP16执行</strong>，以及<strong>动态损失缩放:</strong></p><ol><li>选择哪些运算以FP16精度执行，需要分析该运算的输出相对于其输入的数值行为，以及预期的性能增益。这使得我们可以将诸如矩阵乘法、卷积和归一化层等运算构成了神经网络计算量的大头, 它们对数值误差的容忍度较高, 并且在FP16下有巨大的性能提升, 标记为“安全的”（适合FP16,），而将像求范数（norm）或指数（exp）这类运算标记为需要高精度的操作, 这些运算必须保持在FP32下以确保精度。</li><li>动态损失缩放能够避免在训练过程中梯度的上溢和下溢。在反向传播中计算出的梯度值可能非常小。FP16的数值表示范围有限，如果梯度值小到超出了FP16能表示的最小正数，它就会被“下溢”成0。一旦梯度变为0，模型的权重就不会更新，训练就停滞了。对此, 在反向传播之前，将计算出的损失（Loss）<strong>乘以一个很大的缩放因子</strong>（例如1024）。根据链式法则，所有的梯度也会被同等放大1024倍。这样，原本那些可能因太小而下溢的梯度，现在被放大了，就能安全地落在FP16的表示范围内。在优化器更新权重之前，再将梯度除以相同的缩放因子（1024），将其还原回真实的数值。这个缩放因子不是固定的。训练框架会自动监控梯度是否溢出：如果梯度在多次迭代中都没有溢出，就尝试增大缩放因子以保留更小的梯度细节；如果发生了溢出（梯度值被放大后超出了FP16的最大范围），就减小缩放因子(缩放因子往往使用2的幂次方的移位操作, 不会引入任何计算误差)。</li></ol><p><img src="%E6%95%B0%E5%80%BC%E8%8C%83%E5%9B%B4.png" alt="数值范围" /></p><h4 id="FP8混合精度训练">FP8混合精度训练</h4><p>尽管FP8类型提供的动态范围足以存储任一特定的激活值或梯度，但却不足以<strong>同时</strong>容纳所有这些值。这使得在FP16训练中行之有效的<strong>单一损失缩放因子策略</strong>，在FP8训练中变得不可行，因而需要为每个FP8张量使用<strong>各自不同的缩放因子</strong>。</p><p>为给定的FP8张量选择合适的缩放因子，存在多种策略：</p><ul><li><strong>即时缩放 (**<strong>just-in-time</strong> <strong>scaling)</strong>：该策略根据当前正在生成的张量的</strong>绝对值最大值 (amax)** 来选择缩放因子。在实践中，这是不可行的，因为它需要对数据进行多次处理——算子先以更高精度生成并写出输出，然后找到该输出的绝对值最大值，最后将这个最大值应用到所有数值上以获得最终的FP8输出。这会产生大量额外开销，严重削弱了使用FP8带来的收益。</li><li><strong>延迟缩放 (delayed scaling)</strong>：该策略根据在<strong>过去若干次迭代</strong>中观察到的绝对值最大值来选择缩放因子。这能够完全发挥FP8的计算性能，但要求将最大值的历史记录作为FP8算子的额外参数进行存储。</li></ul><h4 id="MXFP8">MXFP8</h4><p>MXFP8是NVIDIA在<strong>Blackwell架构</strong>中引入的一种<strong>增强版FP8格式</strong>。它的核心目标是解决传统FP8在保证数值精度和处理大动态范围数据时的矛盾，通过一种更精细化的<strong>块缩放（Block Scaling）机制</strong>，实现了更高的精度和效率。</p><p>其思想类似于QLoRA: 也就是一个Tensor我们划分为不同的块, 每个块使用自己的scale, 这样会让总体的数值往E4M3的范围靠拢, 但是如果Block数量太多, scale还是采用原始FP8下的FP32格式来存储, 也会占很大的空间, 所以MXFP8的每一块的缩放因子都采用E8M0格式来存储</p><p><img src="MXFP8.png" alt="MXFP8" /></p><p>![Block Scaling](Block Scaling.png)</p><p><img src="E8M0.png" alt="E8M0" /></p><p>分块量化会带来一个问题, 反向计算梯度过程中, 需要对原始的输入矩阵进行转置, 原先按照&quot;行主序&quot;生成的量化scale, 转置之后, 共享同一个缩放因子的连续块就会被打散到不同的新块里面, 原先的scale就不适用了, 采用的常规的方法是: 要想得到一个有效的、转置后的MXFP8矩阵，只能先将原始MXFP8数据<strong>反量化</strong>回高精度格式（如FP32），对高精度数据进行转置，然后再<strong>重新量化</strong>成新的MXFP8格式（生成新的块和新的缩放因子）。</p><p>这个“反量化 -&gt; 重新量化”的过程被称为<strong>双重量化 (double</strong> <strong>quantization)</strong>。每一次量化都可能引入微小的误差，进行两次这样的操作会累积误差，导致明显的<strong>精度损失</strong>，这是在训练中极力要避免的。</p><p>Transformer Engine采用的是从源头避免重复量化:</p><p>下图中的Cast模块在前向过程中会从源头计算两个MXFP8的版本:</p><ol><li>原始的非转置的MXFP8格式, 用于前向</li><li>转置后的MXFP8格式, 不会被立刻使用, 而是被暂存起来, 直接向下传递给反向传播阶段</li></ol><p>反向的输入梯度也会经过Cast模块生成两个MXFP8版本.</p><p>![TransformerEngine MXFP8前向反向优化](TransformerEngine MXFP8前向反向优化.png)</p><p>FP8操作使用<code>fp8_autocast</code>的上下文管理器自动进行, 其中的自动化操作包括:</p><ul><li>所有 FP8 安全操作的输入均转换为 FP8</li><li>Amax 历史记录已更新</li><li>计算新的缩放因子并准备进行下一次迭代</li></ul><p>当模型在<code>fp8_autocast</code>区域内运行时，尤其是在多 GPU 训练中，需要进行一些通信以同步缩放因子和 amax 历史记录。为了在不增加太多开销的情况下执行该通信，<code>fp8_autocast</code>上下文管理器会在执行通信之前聚合张量</p><p>由于这种聚合，反向调用需要在<code>fp8_autocast</code>上下文管理器之外进行.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">with</span> fp8_autocast():<br>    output = model(data)<br>    loss = criterion(output, target)<br><br><span class="hljs-comment"># 在上下文管理器外面调用 backward()</span><br>loss.backward()  <br></code></pre></td></tr></table></figure><p>如果 <code>loss.backward()</code> 在 <code>with</code> 块内部，那么反向传播会先开始。但此时，用于反向传播的缩放因子等信息可能还没有在各个GPU之间同步，这会导致计算错误。</p><p>因此，必须先退出 <code>with</code> 块，让它完成所有必要的聚合和通信(<code>fp8_autocast</code>被设计为在退出with时, 自动触发&quot;打包聚合和通信&quot;的操作)，确保所有GPU都拿到了最新的、同步好的缩放因子。然后，再执行 <code>loss.backward()</code>，此时的计算才是安全和正确的。</p><p>注: FP8操作下的和FP32下的输出数值不相等, 并不是量化带来的误差, 量化只是乘以一个数, 再除以一个数, 并不会带来误差, 是<strong>在计算开始前，将高精度的FP32输入（包括权重）转换为低精度FP8时，就已经发生了不可逆的精度损失.</strong></p>]]></content>
    
    
    <categories>
      
      <category>高性能计算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>大模型量化技术</title>
    <link href="/Diffcc/Diffcc.github.io/2025/07/12/20250712/"/>
    <url>/Diffcc/Diffcc.github.io/2025/07/12/20250712/</url>
    
    <content type="html"><![CDATA[<p>现有的基于Transformer架构的大模型，参数量规模极大，往往一个普通的模型动不动就几个G占用，对于边缘端设备来说，内存和计算能力都相对较低的情况下，压缩模型，同时尽量保证较高的推理性能尤为重要。</p><p>DeepSeek模型详解：<a href="https://cloud.tencent.com/developer/article/2497217">https://cloud.tencent.com/developer/article/2497217</a></p><p>Qwen3模型优化：<a href="https://www.zhihu.com/question/1914286810902827620/answer/1914361315604031301">https://www.zhihu.com/question/1914286810902827620/answer/1914361315604031301</a></p><p>参考链接：<a href="https://zhuanlan.zhihu.com/p/11886909512">https://zhuanlan.zhihu.com/p/11886909512</a></p><span id="more"></span><p>本文主要涉及模型压缩中的量化技术（Quantization），对于其他压缩技术，例如剪枝（Pruning）、知识蒸馏（Knowledge Distillation）、低秩分解（Low-Rank Factorization）等，读者可自行了解。</p><h3 id="量化原理">量化原理</h3><p>常见的数据类型有如下几种（Float32，TF32，FP16，BF16）等，其中后缀数字表明1个符号位 + 指数位 + 尾数位，通俗的理解，指数位代表该数的量级，也就是范围，而尾数位则代表了该数的有效尾数，所以我们在比较Float类型的数字是否相等时，往往取二者的差值小于某个小数阈值，而不同的尾数个数，对于阈值的要求也不一样。</p><p><img src="%E7%B2%BE%E5%BA%A6%E5%AF%B9%E6%AF%94.png" alt="精度对比" /></p><p>而模型量化则是将模型的参数从高精度的数据类型（例如Float32）转换为低精度的数据类型（如INT8，FP8），进而减少模型的参数量大小。</p><p><img src="%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F.png" alt="数据格式" /></p><h4 id="量化对象">量化对象</h4><p>模型量化的对象包括以下几个方面：</p><ul><li>权重（weight）：weight的量化最为常见，权重的减小可以减少模型的内存占用，权重由于在训练完成之后固定，其数值范围和数值无关，可以离线完成量化，通常比较简单。</li><li>激活（Activation）：activation在整个模型中占据了内存的大头，所以量化激活可以更近一步减少内存占用，同时结合权重量化可以充分利用整数计算获得模型推理性能的提升，但是由于激活输出会随着输入的改变而改变，所以需要统计数据的动态范围，更难量化。</li><li>KV Cache：在 Transformer 推理（如 LLM）中，<strong>Key-Value（KV）缓存</strong> 用于存储注意力机制计算时的历史键值对，避免重复计算，显著提升生成速度。但 KV 缓存会占用大量显存（尤其是长上下文场景）。因此，量化KV Cache对提高模型长序列生成的吞吐量至关重要。</li><li>梯度（Gradients）：梯度量化主要用于训练场景，量化梯度可以减少浮点数梯度在分布式计算中的通信开销，同时减少反向传播的开销。</li></ul><h4 id="量化形式">量化形式</h4><p>根据量化数表示的原始数据是否均匀，可以将量化方法划分为对称量化和非对称量化，实际网络中的权重和激活往往是不均匀的，理应采用非对称量化减少精度损失，但是由于非对称量化的计算复杂度较高，通常采用线性量化：</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction: ltr; display: block; text-align: center; margin: 1em 0; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="34.615ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 15299.8 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(737.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1793.6,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2226.6,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2524.6,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2869.6,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(3372.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(3761.6,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(4212.6,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(4697.6,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(5269.6,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(5869.6,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(6389.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(6778.6,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(7229.6,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mi" transform="translate(7729.6,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(8198.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(8809.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(9810,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(10275,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(10719.7,0)"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1223,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(12537.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(12982.4,0)"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1407,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(14910.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; overflow: hidden; width: 100%;"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>q</mi><mo>=</mo><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi><mo stretchy="false">(</mo><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo stretchy="false">(</mo><mi>r</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>s</mi><mo stretchy="false">)</mo><mo>+</mo><mi>z</mi><mo>,</mo><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container><p>原始浮点数<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi></math></mjx-assistive-mml></mjx-container> 除以量化间隔<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.023ex" role="img" focusable="false" viewBox="0 -442 469 452" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi></math></mjx-assistive-mml></mjx-container>, 得到缩放后的值<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="3.213ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1420 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(451,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mi" transform="translate(951,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>s</mi></math></mjx-assistive-mml></mjx-container>, 这一步将<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi></math></mjx-assistive-mml></mjx-container>映射到一个更小的范围，便于后续的整数表示，加上偏置<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi></math></mjx-assistive-mml></mjx-container>, 调整数据零点的位置，如果<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.2ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2298.6 748" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(742.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="mn" transform="translate(1798.6,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi><mo>=</mo><mn>0</mn></math></mjx-assistive-mml></mjx-container>, 表示堆成量化，数据的零点居中；反之，则为非对称量化，对称量化可以避免量化算子在推理中计算<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi></math></mjx-assistive-mml></mjx-container>相关的部分，降低推理时的时间复杂度；非对称量化可以根据实际数据的分布确定最大值和最大值，更加充分利用量化数据信息，使得量化导致的损失更低。<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.335ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3684 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(451,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(936,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1508,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2108,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(2628,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(3017,0)"><path data-c="B7" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(3295,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo stretchy="false">(</mo><mo>·</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>表示取整操作，得到最接近的整数值，<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.962ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2635 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(731,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1076,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(1579,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(1968,0)"><path data-c="B7" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(2246,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi><mo stretchy="false">(</mo><mo>·</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>表示截断操作，将取整之后的结果限制在<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="10.74ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4747.1 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1223,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(2096.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(2540.7,0)"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1407,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(4469.1,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">[</mo><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><mo stretchy="false">]</mo></math></mjx-assistive-mml></mjx-container>范围内，防止超出量化之后的数值表示范围。</p><p><img src="%E5%AF%B9%E7%A7%B0%E9%87%8F%E5%8C%96%E5%92%8C%E9%9D%9E%E5%AF%B9%E7%A7%B0%E9%87%8F%E5%8C%96.png" alt="对称量化和非对称量化" /></p><h4 id="量化粒度">量化粒度</h4><p>根据量化参数<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.023ex" role="img" focusable="false" viewBox="0 -442 469 452" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi></math></mjx-assistive-mml></mjx-container>的共享范围（即量化粒度），量化方法可以按照如下粒度划分：</p><ul><li>per-tensor(per-layer)量化：每层或每个张量只有一个缩放因子，张量内的所有值都被这个缩放因子量化。</li><li>per-channel量化：卷积核中的每个通道都有不同的缩放因子。</li><li>per-token量化：针对激活而言，针对每一行进行量化。在LLM中，通常和per-channel量化配合使用，如：逐token量化激活，逐channel量化权重。</li><li>per-group/group-wise: 分组量化，以组为单位，分组量化的特殊情况是，将每个密集的矩阵都视为一个组，每个矩阵都有自己的量化范围。而更为普遍的情况是将每个密集矩阵按照输出神经元进行分割，每个连续的N输出神经元为一个组。比如：GPTQ、AWQ中使用128个元素为一组进行量化。有些地方也称为子通道分组（Sub-channel-wise）量化，即将通道划分为更小的子组，以实现更细粒度的精度控制。它的粒度处于 per-tensor 和 per-channel 之间。比如：group_size=128对应一个量化系数，共有 ⌊T/group_size⌋ * ⌊C0/group_size⌋ 个。当 group_size=1 时，逐组量化与逐层量化等价；当 group_size=num_filters（如：dw（Depthwise）将卷积核变成单通道）时，逐组量化与逐通道量化等价。</li></ul><p><img src="%E4%B8%8D%E5%90%8C%E9%87%8F%E5%8C%96%E6%96%B9%E5%BC%8F.jpg" alt="不同量化方式" /></p><p>下图中<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.928ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 852 683" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>X</mi></math></mjx-assistive-mml></mjx-container>表示激活Tensor，<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="2.371ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1048 705" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></mjx-assistive-mml></mjx-container>表示权重Tensor。</p><p><img src="%E9%87%8F%E5%8C%96%E7%B2%92%E5%BA%A6.png" alt="量化粒度" /></p><h4 id="静态量化和动态量化">静态量化和动态量化</h4><p><strong>对于激活而言， 量化有动态量化和静态量化之分。</strong></p><p>通常，<strong>对于激活而言</strong>，静态量化是指如果采用具有代表性的校准数据集来为其生成缩放因子和零点，这些参数在模型的整个生命周期中保持不变。静态量化的优点在于推理时的计算效率较高，因为它不需要在运行时动态计算量化参数。然而，由于量化参数是固定的，静态量化可能会引入一些量化误差，从而影响模型的精度</p><p>而动态量化是指在每次前向传递期间计算激活的最小值和最大值，以提供动态的缩放因子以实现高精度。动态量化的优点在于它可以更准确地表示模型的激活值，因为它考虑了运行时的实际数据分布。然而，这种方法的缺点是可能会增加计算开销，因为需要在运行时计算量化参数。动态量化适合于那些对模型精度要求较高的应用场景，尤其是当模型的输入数据分布变化较大时。</p><p>目前，常见的是对激活<strong>使用静态量化</strong>，其中最小/最大范围是在离线校准阶段计算的。但由于LLM中激活范围差异巨大，将<strong>导致准确度显著下降</strong>。</p><h4 id="离线量化和在线量化">离线量化和在线量化</h4><p>离线量化是指模型上线前进行量化并生成缩放因子，而在线量化是指模型运行时进行量化。</p><p>动态与静态量化的区别在于是否使用校准集，而离线与在线量化的区别则是量化的时机不同。简单理解就是说<strong>离线静态量化</strong>是指在模型上线推理前使用校准集生成缩放因子，对权重和激活进行量化。<strong>在线动态量化</strong>是指在模型上线推理时，在每次前向传播过程中实时生成缩放因子，对模型对权重和激活进行量化。 而<strong>离线动态量化</strong>通常是指<strong>对权重在运行前先进行量化，对激活在运行时进行动态量化。</strong></p><h4 id="量化阶段">量化阶段</h4><p>根据应用量化压缩模型的阶段，可以将模型量化分为：</p><ul><li>量化感知训练（Quantization Aware Training, QAT）：在模型训练过程中加入伪量化算子，通过训练时统计输入输出的数据范围可以提升量化后模型的精度，适用于对模型精度要求较高的场景；其量化目标无缝地集成到模型的训练过程中。这种方法使LLM在训练过程中适应低精度表示，增强其处理由量化引起的精度损失的能力。这种适应旨在量化过程之后保持更高性能。</li><li>量化感知微调（Quantization-Aware Fine-tuning，<a href="https://zhida.zhihu.com/search?content_id=251400333&amp;content_type=Article&amp;match_order=1&amp;q=QAF&amp;zhida_source=entity">QAF</a>）：在微调过程中对LLM进行量化。主要目标是确保经过微调的LLM在量化为较低位宽后仍保持性能。通过将量化感知整合到微调中，以在模型压缩和保持性能之间取得平衡。</li><li>训练后量化（Post Training Quantization, PTQ）：在LLM训练完成后对其参数进行量化，只需要少量校准数据，适用于追求高易用性和缺乏训练资源的场景。主要目标是减少LLM的存储和计算复杂性，而无需对LLM架构进行修改或进行重新训练。PTQ的主要优势在于其简单性和高效性。但PTQ可能会在量化过程中引入一定程度的精度损失。</li></ul><h5 id="量化感知训练">量化感知训练</h5><p>量化感知训练是在训练过程中模拟量化，<strong>利用伪量化算子将量化带来的精度损失计入训练误差，使得优化器能在训练过程中尽量减少量化误差</strong>，得到更高的模型精度。量化感知训练的具体流程如下：</p><ul><li><strong>初始化</strong>：设置权重和激活值的范围<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.113ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 1818.1 636" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1223,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.363ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 1928.4 636" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1407,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>的初始值；</li><li><strong>构建模拟量化网络</strong>：在需要量化的权重和激活值后插入伪量化算子；</li><li><strong>量化训练</strong>：重复执行以下步骤直到网络收敛，<strong>计算量化网络层的权重和激活值的范围</strong><mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.113ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 1818.1 636" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1223,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.363ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 1928.4 636" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1407,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>，并<strong>根据该范围将量化损失带入到前向推理和后向参数更新的过程中</strong>；</li><li><strong>导出量化网络</strong>：获取<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.113ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 1818.1 636" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1223,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.363ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 1928.4 636" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1407,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>，并计算量化参数<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.023ex" role="img" focusable="false" viewBox="0 -442 469 452" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi></math></mjx-assistive-mml></mjx-container>；<strong>将量化参数代入量化公式中，转换网络中的权重为量化整数值；删除伪量化算子，在量化网络层前后分别插入量化和反量化算子</strong>。</li></ul><p>伪量化算子的数学形式与实际量化公式类似，但增加了梯度传播机制。例如，对权重或激活值 x<em>x</em> 的伪量化过程如下:</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction: ltr; display: block; text-align: center; margin: 1em 0; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.667ex;" xmlns="http://www.w3.org/2000/svg" width="38.237ex" height="2.364ex" role="img" focusable="false" viewBox="0 -750 16900.6 1045" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(550,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1079,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1600,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2066,0)"><path data-c="5F" d="M0 -62V-25H499V-62H0Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2566,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(3026,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(3598,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(4127,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(4727,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(4530.5,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(5586.3,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(6019.3,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(6317.3,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(6662.3,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(7165.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(7554.3,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(8005.3,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(8490.3,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(9062.3,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(9662.3,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(10182.3,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(572,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mi" transform="translate(1072,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(1763.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2763.4,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(13633,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(14633.2,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(15098.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(15709.4,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(16431.6,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; overflow: hidden; width: 100%;"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>x</mi><mrow data-mjx-texclass="ORD"><mi>f</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi mathvariant="normal">_</mi><mi>q</mi><mi>u</mi><mi>a</mi><mi>n</mi><mi>t</mi></mrow></msub><mo>=</mo><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi><mo stretchy="false">(</mo><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mrow data-mjx-texclass="ORD"><mi>x</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>s</mi><mo>+</mo><mi>z</mi></mrow><mo>−</mo><mi>z</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>s</mi></math></mjx-assistive-mml></mjx-container><p>在前向传播时，<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.706ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3406 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(451,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(936,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1508,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2108,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(2628,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(3017,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo stretchy="false">(</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.333ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2357 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(731,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1076,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(1579,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(1968,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi><mo stretchy="false">(</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>模拟量化， 对输入值进行缩放、取整、截断，再反缩放回原始范围。</p><p>在反向传播时，梯度直接传递（STE），忽略<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.706ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3406 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(451,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(936,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1508,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2108,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(2628,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(3017,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo stretchy="false">(</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.333ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2357 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(731,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1076,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(1579,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(1968,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi><mo stretchy="false">(</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>的不可微性：</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction: ltr; display: block; text-align: center; margin: 1em 0; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.667ex;" xmlns="http://www.w3.org/2000/svg" width="23.983ex" height="2.364ex" role="img" focusable="false" viewBox="0 -750 10600.3 1045" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(520,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1201,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mi" transform="translate(1701,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2221,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(3070.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(4126.6,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(4646.6,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5327.6,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mi" transform="translate(5827.6,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(6347.6,0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(550,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1079,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1600,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2066,0)"><path data-c="5F" d="M0 -62V-25H499V-62H0Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2566,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(3026,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(3598,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(4127,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(4727,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width: 3;"/></g></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; overflow: hidden; width: 100%;"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>d</mi><mi>L</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>d</mi><mi>x</mi><mo>=</mo><mi>d</mi><mi>L</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>d</mi><mrow data-mjx-texclass="ORD"><msub><mi>x</mi><mrow data-mjx-texclass="ORD"><mi>f</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi mathvariant="normal">_</mi><mi>q</mi><mi>u</mi><mi>a</mi><mi>n</mi><mi>t</mi></mrow></msub></mrow></math></mjx-assistive-mml></mjx-container><h5 id="训练后量化">训练后量化</h5><p>训练后量化也可以分成两种，权重量化和全量化。</p><ul><li>权重量化<strong>仅量化模型的权重</strong>以压缩模型的大小，在推理时将权重反量化为原始的float32数据，后续推理流程与普通的float32模型一致。权重量化的好处是不需要校准数据集，不需要实现量化算子，且模型的精度误差较小，由于实际推理使用的仍然是float32算子，所以推理性能不会提高。</li><li>全量化<strong>不仅会量化模型的权重，还会量化模型的激活值</strong>，在模型推理时执行量化算子来加快模型的推理速度。为了量化激活值，需要用户提供一定数量的校准数据集用于统计每一层激活值的分布，并对量化后的算子做校准。<strong>校准数据集可以来自训练数据集或者真实场景的输入数据，需要数量通常非常小</strong>。</li></ul><p>在量化激活值时会<strong>以校准数据集为输入，执行推理流程然后统计每层激活值的数据分布并得到相应的量化参数</strong>。具体的操作流程如下：</p><ul><li>使用直方图统计的方式<strong>得到原始float32数据的统计分布</strong> <mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.667ex;" xmlns="http://www.w3.org/2000/svg" width="2.52ex" height="2.213ex" role="img" focusable="false" viewBox="0 -683 1113.9 978" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(675,-150) scale(0.707)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width: 3;"/></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>P</mi><mi>f</mi></msub></math></mjx-assistive-mml></mjx-container>；</li><li>在给定的搜索空间中选取若干个<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.113ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 1818.1 636" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1223,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.363ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 1928.4 636" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1407,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>分别<strong>对激活值量化</strong>，得到量化后的数据<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="2.713ex" height="2.242ex" role="img" focusable="false" viewBox="0 -704 1199.3 991.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(824,-150) scale(0.707)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>Q</mi><mi>q</mi></msub></math></mjx-assistive-mml></mjx-container>；</li><li>使用直方图统计得到<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="2.713ex" height="2.242ex" role="img" focusable="false" viewBox="0 -704 1199.3 991.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(824,-150) scale(0.707)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>Q</mi><mi>q</mi></msub></math></mjx-assistive-mml></mjx-container>的<strong>统计分布</strong>;</li><li>计算每个<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="2.824ex" height="2.032ex" role="img" focusable="false" viewBox="0 -704 1248.1 898" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(824,-150) scale(0.707)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>Q</mi><mi>a</mi></msub></math></mjx-assistive-mml></mjx-container>与<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.667ex;" xmlns="http://www.w3.org/2000/svg" width="2.52ex" height="2.213ex" role="img" focusable="false" viewBox="0 -683 1113.9 978" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(675,-150) scale(0.707)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width: 3;"/></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>P</mi><mi>f</mi></msub></math></mjx-assistive-mml></mjx-container>的统计分布差异，并<strong>找到差异性最低的一个</strong>对应的 <mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.113ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 1818.1 636" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1223,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.363ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 1928.4 636" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(479,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1407,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>q</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>来计算相应的量化参数，常见的用于度量分布差异的指标包括KL散度(Kullback-Leibler Divergence)、对称KL散度(Symmetric Kullback-Leibler Divergence)和JS散度(Jenson-Shannon Divergence)</li></ul><p>除此之外，由于量化存在固有误差，还需要<strong>校正量化误差</strong>。以矩阵乘为例<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.777ex;" xmlns="http://www.w3.org/2000/svg" width="17.894ex" height="2.949ex" role="img" focusable="false" viewBox="0 -960 7909.2 1303.3" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(806.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="munderover" transform="translate(1862.6,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(1089,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width: 3;"/></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="msub" transform="translate(4315.9,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g><g data-mml-node="msub" transform="translate(5358.8,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(6480,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(7480.2,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi><mo>=</mo><munderover><mo data-mjx-texclass="OP">∑</mo><mrow data-mjx-texclass="ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow data-mjx-texclass="ORD"><mi>N</mi></mrow></munderover><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi></math></mjx-assistive-mml></mjx-container>，<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.62ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 716 454" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi></math></mjx-assistive-mml></mjx-container>表示权重，<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math></mjx-assistive-mml></mjx-container>表示激活值，<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>b</mi></math></mjx-assistive-mml></mjx-container>表示偏置。</p><p>首先需要对量化的均值做校正，对float32算子和量化算子输出的每个通道求平均，假设某个通道i的float32算子输出均值为<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.937ex" height="1.355ex" role="img" focusable="false" viewBox="0 -441 856 598.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>a</mi><mi>i</mi></msub></math></mjx-assistive-mml></mjx-container>，量化算子反量化输出均值为<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="2.672ex" height="1.647ex" role="img" focusable="false" viewBox="0 -441 1181.2 728.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(562,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(460,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>a</mi><mrow data-mjx-texclass="ORD"><mi>q</mi><mi>i</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>，将这个通道两个均值的差<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="7.375ex" height="1.969ex" role="img" focusable="false" viewBox="0 -583 3259.6 870.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(1078.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(2078.4,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(562,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(460,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>a</mi><mi>i</mi></msub><mo>−</mo><msub><mi>a</mi><mrow data-mjx-texclass="ORD"><mi>q</mi><mi>i</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>加到对应的通道上即可使得最终的输出均值和float32一致。</p><p>另外，还需要<strong>保证量化后的分布和量化前是一致的</strong>，设某个通道权重数据的均值、方差为<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.989ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2647.2 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(764,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(1153,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(2258.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>E</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>c</mi></msub><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>、<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="13.771ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6086.8 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(278,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(556,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(1883.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2883.6,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(3647.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(4036.6,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(5141.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(5530.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(5808.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo><msub><mi>w</mi><mi>c</mi></msub><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>c</mi></msub><mo stretchy="false">)</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo></math></mjx-assistive-mml></mjx-container>，量化后的均值和方差为<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.989ex" height="2.4ex" role="img" focusable="false" viewBox="0 -811 2647.2 1061" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(764,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1153,0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(552.6,17) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(2258.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>E</mi><mo stretchy="false">(</mo><mrow data-mjx-texclass="ORD"><mover><msub><mi>w</mi><mi>c</mi></msub><mo stretchy="false">^</mo></mover></mrow><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>、<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="13.771ex" height="2.4ex" role="img" focusable="false" viewBox="0 -811 6086.8 1061" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(278,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(556,0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(552.6,17) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(1883.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2883.6,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(3647.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4036.6,0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(552.6,17) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(5141.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(5530.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(5808.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mrow data-mjx-texclass="ORD"><mover><msub><mi>w</mi><mi>c</mi></msub><mo stretchy="false">^</mo></mover></mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mrow data-mjx-texclass="ORD"><mover><msub><mi>w</mi><mi>c</mi></msub><mo stretchy="false">^</mo></mover></mrow><mo stretchy="false">)</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo></math></mjx-assistive-mml></mjx-container>，对权重如下校正：</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction: ltr; display: block; text-align: center; margin: 1em 0; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.791ex;" xmlns="http://www.w3.org/2000/svg" width="17.404ex" height="2.713ex" role="img" focusable="false" viewBox="0 -849.5 7692.7 1199" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(441.3,17) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(1383,0)"><path data-c="2190" d="M944 261T944 250T929 230H165Q167 228 182 216T211 189T244 152T277 96T303 25Q308 7 308 0Q308 -11 288 -11Q281 -11 278 -11T272 -7T267 2T263 21Q245 94 195 151T73 236Q58 242 55 247Q55 254 59 257T73 264Q121 283 158 314T215 375T247 434T264 480L267 497Q269 503 270 505T275 509T288 511Q308 511 308 500Q308 493 303 475Q293 438 278 406T246 352T215 315T185 287T165 270H929Q944 261 944 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(2660.7,0)"><g data-mml-node="mi"><path data-c="1D701" d="M296 643Q298 704 324 704Q342 704 342 687Q342 682 339 664T336 633Q336 623 337 618T338 611Q339 612 341 612Q343 614 354 616T374 618L384 619H394Q471 619 471 586Q467 548 386 546H372Q338 546 320 564L311 558Q235 506 175 398T114 190Q114 171 116 155T125 127T137 104T153 86T171 72T192 61T213 53T235 46T256 39L322 16Q389 -10 389 -80Q389 -119 364 -154T300 -202Q292 -204 274 -204Q247 -204 225 -196Q210 -192 193 -182T172 -167Q167 -159 173 -148Q180 -139 191 -139Q195 -139 221 -153T283 -168Q298 -166 310 -152T322 -117Q322 -91 302 -75T250 -51T183 -29T116 4T65 62T44 160Q44 287 121 410T293 590L302 595Q296 613 296 643Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(471,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3487.9,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="28" d="M152 251Q152 646 388 850H416Q422 844 422 841Q422 837 403 816T357 753T302 649T255 482T236 250Q236 124 255 19T301 -147T356 -251T403 -315T422 -340Q422 -343 416 -349H388Q359 -325 332 -296T271 -213T212 -97T170 56T152 251Z" style="stroke-width: 3;"/></g></g><g data-mml-node="msub" transform="translate(3945.9,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(441.3,17) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(5273.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(6273.5,0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(7234.7,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="29" d="M305 251Q305 -145 69 -349H56Q43 -349 39 -347T35 -338Q37 -333 60 -307T108 -239T160 -136T204 27T221 250T204 473T160 636T108 740T60 807T35 839Q35 850 50 850H56H69Q197 743 256 566Q305 425 305 251Z" style="stroke-width: 3;"/></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; overflow: hidden; width: 100%;"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mrow data-mjx-texclass="ORD"><mover><mi>w</mi><mo stretchy="false">^</mo></mover></mrow><mrow data-mjx-texclass="ORD"><mi>c</mi></mrow></msub><mo stretchy="false">←</mo><msub><mi>ζ</mi><mrow data-mjx-texclass="ORD"><mi>c</mi></mrow></msub><mrow data-mjx-texclass="ORD"><mo minsize="1.2em" maxsize="1.2em">(</mo></mrow><msub><mrow data-mjx-texclass="ORD"><mover><mi>w</mi><mo stretchy="false">^</mo></mover></mrow><mrow data-mjx-texclass="ORD"><mi>c</mi></mrow></msub><mo>+</mo><msub><mi>u</mi><mrow data-mjx-texclass="ORD"><mi>c</mi></mrow></msub><mrow data-mjx-texclass="ORD"><mo minsize="1.2em" maxsize="1.2em">)</mo></mrow></math></mjx-assistive-mml></mjx-container><p>其中：</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction: ltr; display: block; text-align: center; margin: 1em 0; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="19.936ex" height="2.4ex" role="img" focusable="false" viewBox="0 -811 8811.5 1061" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(1239,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2294.7,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(3058.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(3447.7,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(4552.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(5164.1,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(6164.4,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(6928.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(7317.4,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(441.3,17) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(8422.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; overflow: hidden; width: 100%;"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>u</mi><mi>c</mi></msub><mo>=</mo><mi>E</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>c</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><msub><mrow data-mjx-texclass="ORD"><mover><mi>w</mi><mo stretchy="false">^</mo></mover></mrow><mi>c</mi></msub><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container><mjx-container class="MathJax" jax="SVG" display="true" style="direction: ltr; display: block; text-align: center; margin: 1em 0; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -2.448ex;" xmlns="http://www.w3.org/2000/svg" width="19.655ex" height="5.751ex" role="img" focusable="false" viewBox="0 -1460 8687.5 2542" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D701" d="M296 643Q298 704 324 704Q342 704 342 687Q342 682 339 664T336 633Q336 623 337 618T338 611Q339 612 341 612Q343 614 354 616T374 618L384 619H394Q471 619 471 586Q467 548 386 546H372Q338 546 320 564L311 558Q235 506 175 398T114 190Q114 171 116 155T125 127T137 104T153 86T171 72T192 61T213 53T235 46T256 39L322 16Q389 -10 389 -80Q389 -119 364 -154T300 -202Q292 -204 274 -204Q247 -204 225 -196Q210 -192 193 -182T172 -167Q167 -159 173 -148Q180 -139 191 -139Q195 -139 221 -153T283 -168Q298 -166 310 -152T322 -117Q322 -91 302 -75T250 -51T183 -29T116 4T65 62T44 160Q44 287 121 410T293 590L302 595Q296 613 296 643Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(471,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(1105,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="mfrac" transform="translate(2160.7,0)"><g data-mml-node="mrow" transform="translate(276,710)"><g data-mml-node="mo"><path data-c="2225" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(500,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(1827.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2827.6,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(3591.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(3980.6,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(5085.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(5474.8,0)"><path data-c="2225" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mrow" transform="translate(220,-771)"><g data-mml-node="mo"><svg width="500" height="1122" y="-311" x="28" viewBox="0 -140.3 500 1122"><path data-c="2225" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z" transform="scale(1,1.683)" style="stroke-width: 3;"/></svg></g><g data-mml-node="msub" transform="translate(556,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(441.3,17) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(1883.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2883.6,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(3647.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(4036.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(441.3,17) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(5141.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(5530.8,0)"><svg width="500" height="1122" y="-311" x="28" viewBox="0 -140.3 500 1122"><path data-c="2225" d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z" transform="scale(1,1.683)" style="stroke-width: 3;"/></svg></g></g><rect width="6286.8" height="60" x="120" y="220"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; overflow: hidden; width: 100%;"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>ζ</mi><mrow data-mjx-texclass="ORD"><mi>c</mi></mrow></msub><mo>=</mo><mfrac><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN" symmetric="true">‖</mo><msub><mi>w</mi><mrow data-mjx-texclass="ORD"><mi>c</mi></mrow></msub><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow data-mjx-texclass="ORD"><mi>c</mi></mrow></msub><mo stretchy="false">)</mo><mo data-mjx-texclass="CLOSE" symmetric="true">‖</mo></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN" symmetric="true">‖</mo><msub><mrow data-mjx-texclass="ORD"><mover><mi>w</mi><mo stretchy="false">^</mo></mover></mrow><mrow data-mjx-texclass="ORD"><mi>c</mi></mrow></msub><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><msub><mrow data-mjx-texclass="ORD"><mover><mi>w</mi><mo stretchy="false">^</mo></mover></mrow><mrow data-mjx-texclass="ORD"><mi>c</mi></mrow></msub><mo stretchy="false">)</mo><mo data-mjx-texclass="CLOSE" symmetric="true">‖</mo></mrow></mfrac></math></mjx-assistive-mml></mjx-container><h3 id="量化方法">量化方法</h3><p>LLM主要有三种类型量化：</p><p>针对仅权重量化：</p><ul><li>对于 W8A16 量化，代表方法有 MinMax</li><li>对于 W6A16 量化，代表方法有 FP6-LLM</li><li>对于 W4A16 量化，代表方法有 AWQ、GPTQ、SpQR、OmniQuant、QuIP#</li><li>对于 W3A16 量化，代表方法有 GPTQ、SpQR、OmniQuant、QuIP#</li><li>对于 W2A16 量化，代表方法有 OmniQuant、QuIP、QuIP#</li></ul><p>针对权重激活量化：</p><ul><li>对于 W8A8 量化，代表方法有 LLM.int8()、SmoothQuant、ZeroQuant</li><li>对于 W6A6 量化，代表方法有 OmniQuant</li><li>对于 W4A8 量化，代表方法有 QoQ</li><li>对于 W4A4 量化，代表方法有 Atom 、QuaRot、OmniQuant</li></ul><p>针对 KV Cache量化：</p><ul><li>KV8：INT8（LMDeploy、TensorRT-LLM）、FP8（TensorRT-LLM、vLLM）</li><li>KV4：Atom、QuaRot、QoQ</li><li>KV3：KVQuant</li><li>KV2：KVQuant、KIVI</li></ul><p>（<strong>注：W4A16 = 4-bit 整数量化权重 + 16-bit 浮点激活值，KV4 中的 4 表示 4-bit 量化</strong>）</p><p><img src="%E9%87%8F%E5%8C%96%E8%87%B3INT8.PNG" alt="量化至INT8" /></p><p>图中展示的是量化到 int8，因此 Round to Grid 实际上是先乘以 127(=2**(8-1)-1)，然后 round 到最近的整数</p><p>![BLOCK Scale](BLOCK Scale.PNG)</p><p>图为了避免outlier的影响, 将输入Tensor分割成一个一个block, 每个block进行单独量化, 有单独的scale和zero, 精度损失更少</p><h3 id="GPTQ-W4A16-W8A16">GPTQ(W4A16, W8A16)</h3><p>GPTQ只针对权重量化, 不针对激活</p><p>GPTQ属于离线量化方法, 也就是在已训练的模型上, 使用少量或者不使用额外的数据, 对模型量化过程进行校准:  QLoRA是<strong>训练后动态量化方法</strong>, 不使用校准集, 直接对每一层layer通过量化公式进行转换. GPTQ是<strong>训练后校正量化方法</strong>, 需要使用有代表性的数据集, 根据模型每一层layer的输入输出调整量化权重.</p><p>简单来说, GPTQ对某个block内所有参数逐个量化, 每个参数量化后, 需要适当调整这个block内其他未量化的参数, 以弥补量化造成的精度损失, GPTQ量化需要准备校准数据集.</p><p>GPTQ是从OBD-&gt;OBS-&gt;OBQ上发展过来的:</p><ol><li>OBD的思想在于计算海森矩阵(对目标函数进行泰勒展开, 系数即为海森矩阵), 去除其中对目标影响较小的参数(剪枝)</li><li>OBS考虑了泰勒展开的交叉项, 即各个参数不独立, 删除一个权重, 其他需要相应调整, 计算方式为计算海森矩阵的逆, 计算每个参数对目标的影响, 按照影响大小确定剪枝的次序, 每次剪枝一个参数, 其他参数也需要相应更新. <strong>GPTQ算法中也延续了此思想: 对某个Block内的所有参数进行量化, 每个参数量化之后, 需要适当调整这个block内其他未量化的参数, 以弥补量化造成的精度损失.</strong></li><li>OBC计算海森矩阵时,考虑参数矩阵的同一行相互相关, 不同行参数不相关, 海森矩阵只需要在每一行内单独计算.</li><li>OBQ和OBC是同一篇文章,在OBC基础上做剪枝</li></ol><p>GPTO在OBQ基础上做了一些算法和性能上的优化:</p><ol><li>GPTO采用贪心策略, 即按顺序做参数量化, 而不是先量化对目标影响较小的参数(这项改进使得参数矩阵每一行的量化可以做并行的矩阵计算)</li><li>Lazy Batch-Updates: 延迟一部分参数的更新(每次量化一个参数, 其他所有未量化的参数都需要全部更新一遍), 缓解带宽压力</li><li>Cholesky Reformulation, 用Chklesky分解求海森矩阵的逆, 减少计算量.</li></ol><p><img src="%E9%9D%9E%E5%BB%B6%E8%BF%9F%E6%9B%B4%E6%96%B0.PNG" alt="非延迟更新" /></p><p><strong>思路:</strong> 由于参数量化是一列一列按次序进行的，第 i 列的参数的量化结果受到前 i-1 列量化的影响，但第 i 列的量化结果不影响前面列的量化。因此我们不需要每次量化前面的列，就更新一遍第 i 列的参数，而是可以先记录第 i 列的更新量，在量化到第 i 列时，再一次性更新参数，这样就可以减少 IO 的次数。</p><p><strong>具体实现:</strong> 将参数矩阵按每 128 列划分为一个个 group，量化某一列时，group 内的参数立即更新，而 group 后面的列只记录更新量，延迟更新。当一个 group 的参数全部量化完成，再统一对后面的所有参数做一次更新。这就是 Lazy Batch-Updates。</p><p><img src="%E5%BB%B6%E8%BF%9F%E6%9B%B4%E6%96%B0.PNG" alt="延迟更新" /></p><p>Lazy Batch-Updates 不减少实际的计算量，但它能有效解决吞吐的瓶颈问题。</p><h3 id="QLoRA">QLoRA</h3><p>QLoRA同时结合模型量化Quant和LoRA参数微调两种方法, 可以在单张48GB的GPU上对一个65B的大模型做finetune</p><p>QLoRA量化的创新点在于两部分:</p><ol><li>采用新的NF(NormalFloat)数据类型, 对于正态分布权重而言信息理论上最优的数据类型</li></ol><p>下图是INT4和NF4数据类型的对比, INT4的格点分布是均匀的, 然而模型的权重通常服从均值为0的正态分布, 因此格点的分布和数据分布不一致, NF4的格点按照正态分布的分位数截取, 格点分布两端稀疏, 中间密集, 格点分布和数据分布一致.</p><p><img src="INT4%E5%AF%B9%E6%AF%94NF4.png" alt="INT4对比NF4" /></p><ol start="2"><li>Double Quant, 对于量化后的scale数据进一步量化</li></ol><p>QLoRA以Block_size=64的Block进行量化, 每个Block计算一个scale, 以FP32存储, 需要的额外显存为32/64=0.5bits, Block数量过多时, Scale的显存占用也会很多, 于是QLoRA对Scale进一步量化为FP8, 取Double Quant 的Block_size = 64, 此时每个参数做量化需要的额外显存 8/64 + 32/(64<em>256) = 0.127bits(其中第二级的缩放因子即FP32, 本身仍然以FP32浮点数存储, 是用来饭量化256个FP8的, 所以第二项相当于32-bit平摊到256</em>64的参数上).</p><h3 id="AWQ-W4A16-W8A16">AWQ(W4A16, W8A16)</h3><p>思路: “激活感知权重量化”, 权重对于LLM的性能并不同等重要, 存在约(0.1% - 1%)显著权重对于大模型的性能影响太大, 跳过这1%的显著权重(salient weight)不进行量化, 可以大大减少量化误差.</p><p><img src="AWQ%E6%BF%80%E6%B4%BB%E6%84%9F%E7%9F%A5%E6%9D%83%E9%87%8D%E9%87%8F%E5%8C%96.png" alt="AWQ激活感知权重量化" /></p><p>AWQ思路是对关键权重先乘一个方法系数再量化进行一个保护。首先，识别关键权重的方法是分析**Activation分布（**通过校验数据集找到没被激活抑制的权重所在位置，</p><p>这些位置（也就是激活值大的位置))于最终输出的影响更大 所以是<strong>显著权重</strong>。显著权重乘一个放大系数scale后，量化误差会较之前变小，并且对于显著权重也给予不同的保护粒度.</p><h3 id="SmoothQuant-W8A8">SmoothQuant(W8A8)</h3><p>weight和activation同时量化的方法, LLM的weight和activation同时量化时, weight的分布大部分是比较平坦的(意味着比较容易量化), activation的值分布在某几个channels内, 而channel内的值分布方差不大, 波动较大.</p><p><img src="SmoothQuant.png" alt="SmoothQuant" /></p><p>SmoothQuant 提出了一种数学上等价的逐通道缩放变换（per-channel scaling transformation），可显著平滑通道间的幅度，从而使模型易于量化。</p><p>SmoothQuant对<strong>input activation的每个channel除以一个平滑因子</strong><mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.09ex;" xmlns="http://www.w3.org/2000/svg" width="7.315ex" height="2.04ex" role="img" focusable="false" viewBox="0 -861.5 3233.3 901.5" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(746.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msup" transform="translate(1691.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z" style="stroke-width: 3;"/></g></g><g data-mml-node="TeXAtom" transform="translate(755,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(748,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi><mo>∈</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><msub><mi>C</mi><mi>i</mi></msub></mrow></msup></math></mjx-assistive-mml></mjx-container>, 同时在对应的<strong>相反方向缩放权重</strong>来保持矩阵相乘数学上相等。</p><p><mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="40.955ex" height="2.943ex" role="img" focusable="false" viewBox="0 -1051 18102.1 1301" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(1040.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(2096.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(2485.6,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(3559.8,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mtext" transform="translate(4060,0)"><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" style="stroke-width: 3;"/><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(556,0)" style="stroke-width: 3;"/><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(834,0)" style="stroke-width: 3;"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(1334,0)" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(5894,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(6283,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="msup" transform="translate(6752,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(422,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mn" transform="translate(778,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(8127.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(8738.9,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(9239.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mtext" transform="translate(9628.1,0)"><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" style="stroke-width: 3;"/><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(556,0)" style="stroke-width: 3;"/><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(834,0)" style="stroke-width: 3;"/><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(1334,0)" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(11462.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(11851.1,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(12320.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(12931.3,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(13431.6,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(14479.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(15146.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(16202.1,0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(515.3,257) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(17054.1,0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(550,257) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Y</mi><mo>=</mo><mo stretchy="false">(</mo><mi>X</mi><mo>⋅</mo><mtext>diag</mtext><mo stretchy="false">(</mo><mi>s</mi><msup><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo><mo>⋅</mo><mo stretchy="false">(</mo><mtext>diag</mtext><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><mrow data-mjx-texclass="ORD"><mover><mi>X</mi><mo stretchy="false">^</mo></mover></mrow><mrow data-mjx-texclass="ORD"><mover><mi>W</mi><mo stretchy="false">^</mo></mover></mrow></math></mjx-assistive-mml></mjx-container></p><p><mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.09ex;" xmlns="http://www.w3.org/2000/svg" width="10.553ex" height="2.04ex" role="img" focusable="false" viewBox="0 -861.5 4664.3 901.5" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(1129.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msup" transform="translate(2074.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z" style="stroke-width: 3;"/></g></g><g data-mml-node="TeXAtom" transform="translate(755,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(704,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(1482,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(748,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>X</mi><mo>∈</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>T</mi><mo>×</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></msup></math></mjx-assistive-mml></mjx-container>在channel维度（列）上每个元素除以<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.801ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 796 599.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>s</mi><mi>i</mi></msub></math></mjx-assistive-mml></mjx-container>, <mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.09ex;" xmlns="http://www.w3.org/2000/svg" width="11.695ex" height="2.04ex" role="img" focusable="false" viewBox="0 -861.5 5169.2 901.5" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(1325.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msup" transform="translate(2270.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z" style="stroke-width: 3;"/></g></g><g data-mml-node="TeXAtom" transform="translate(755,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(748,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(1042,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(1820,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(748,-150) scale(0.707)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width: 3;"/></g></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi><mo>∈</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><msub><mi>C</mi><mi>i</mi></msub><mo>×</mo><msub><mi>C</mi><mi>o</mi></msub></mrow></msup></math></mjx-assistive-mml></mjx-container>则在每行上每个元素乘以<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.801ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 796 599.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>s</mi><mi>i</mi></msub></math></mjx-assistive-mml></mjx-container>，这样<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.726ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 763 683" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Y</mi></math></mjx-assistive-mml></mjx-container>在数学上完全是相等的，另外在LLM里，input <mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.928ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 852 683" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>X</mi></math></mjx-assistive-mml></mjx-container>通常来自于上一层的输出（譬如linear层，layer norm层），因此可以<strong>提前将平滑因子融合到前面层，这样不会增加INT8 kernel开销。</strong></p><p>对激活的Channel上找到最大异常值, 所有的激活值除以平滑因子, 将激活平滑, 这时候的激活就变的容易量化, 为了保证输出结果不变, 权重<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="2.371ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1048 705" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></mjx-assistive-mml></mjx-container>会对应乘以一个scale, 由于权重<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="2.371ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1048 705" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></mjx-assistive-mml></mjx-container>本身就比较平滑, 所以乘以这个比例之后, 只不过数值范围变大了一些, 但是通常仍然在可控范围之内, 相当于把激活的量化难度转移到权重.</p><p>为了减小量化误差，可以为所有 channels 增加有效量化 bits。当所有 channels 都拥有相同的最大值时，有效量化 bits 将会最大。一种比较直接的做法是，让<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="33.494ex" height="2.363ex" role="img" focusable="false" viewBox="0 -750 14804.4 1044.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(1121.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(2176.9,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width: 3;"/><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(833,0)" style="stroke-width: 3;"/><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(1333,0)" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(4037.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(4426.9,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(4704.9,0)"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(861,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(5907.2,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(6185.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(6574.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width: 3;"/></g><g data-mml-node="mstyle" transform="translate(6852.2,0)"><g data-mml-node="mspace"/></g><g data-mml-node="mi" transform="translate(8018.9,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(8708.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="mn" transform="translate(9764.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(10264.4,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width: 3;"/></g><g data-mml-node="mn" transform="translate(10709.1,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(11209.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(11653.8,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(12992.4,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(13437.1,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(748,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(14526.4,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>s</mi><mi>j</mi></msub><mo>=</mo><mo data-mjx-texclass="OP" movablelimits="true">max</mo><mo stretchy="false">(</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo><msub><mi>X</mi><mi>j</mi></msub><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mo stretchy="false">)</mo><mo>,</mo><mstyle scriptlevel="0"><mspace width="1em"/></mstyle><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>C</mi><mi>j</mi></msub><mo>,</mo></math></mjx-assistive-mml></mjx-container> <mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.462ex;" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>j</mi></math></mjx-assistive-mml></mjx-container> 代表 <mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.462ex;" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>j</mi></math></mjx-assistive-mml></mjx-container>-th input channel。通过除以 <mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="1.908ex" height="1.666ex" role="img" focusable="false" viewBox="0 -442 843.3 736.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width: 3;"/></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>s</mi><mi>j</mi></msub></math></mjx-assistive-mml></mjx-container> 后，activation channels 都将有相同的最大值，比较容易量化。但是这种做法会把量化难度全部转向权重，导致一个比较大的精度损失。另一种做法是让<mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="18.405ex" height="2.363ex" role="img" focusable="false" viewBox="0 -750 8134.9 1044.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(1121.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="mn" transform="translate(2176.9,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2676.9,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(3343.6,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width: 3;"/><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(833,0)" style="stroke-width: 3;"/><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(1333,0)" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(5204.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(5593.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(5871.6,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(7189.9,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(7467.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(7856.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>s</mi><mi>j</mi></msub><mo>=</mo><mn>1</mn><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mo data-mjx-texclass="OP" movablelimits="true">max</mo><mo stretchy="false">(</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo><msub><mi>W</mi><mi>j</mi></msub><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mo stretchy="false">)</mo><mo>,</mo></math></mjx-assistive-mml></mjx-container> 这样权重易量化，但是 activation 量化误差会很大。因此我们需要在 weight 和 activation 中分离量化难度，让彼此均容易被量化。</p><p>论文引入了一个超参「迁移强度」 <mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 640 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></mjx-assistive-mml></mjx-container> 来控制从 activation 迁移多少难度到 weight：</p><p><mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="31.061ex" height="2.552ex" role="img" focusable="false" viewBox="0 -833.9 13729 1128.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(1121.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(2176.9,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width: 3;"/><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(833,0)" style="stroke-width: 3;"/><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(1333,0)" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(4037.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(4426.9,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(4704.9,0)"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(861,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(5907.2,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="msup" transform="translate(6185.2,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(422,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(7109.8,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(7776.4,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width: 3;"/><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(833,0)" style="stroke-width: 3;"/><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(1333,0)" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(9637.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(10026.4,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(10304.4,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(11622.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width: 3;"/></g><g data-mml-node="msup" transform="translate(11900.8,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(422,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(500,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(1278,0)"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z" style="stroke-width: 3;"/></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>s</mi><mi>j</mi></msub><mo>=</mo><mo data-mjx-texclass="OP" movablelimits="true">max</mo><mo stretchy="false">(</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo><msub><mi>X</mi><mi>j</mi></msub><mo data-mjx-texclass="ORD" stretchy="false">|</mo><msup><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mi>α</mi></mrow></msup><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mo data-mjx-texclass="OP" movablelimits="true">max</mo><mo stretchy="false">(</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo><msub><mi>W</mi><mi>j</mi></msub><mo data-mjx-texclass="ORD" stretchy="false">|</mo><msup><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mn>1</mn><mo>−</mo><mi>α</mi></mrow></msup></math></mjx-assistive-mml></mjx-container></p><p><img src="%E8%BF%81%E7%A7%BB%E5%BC%BA%E5%BA%A6%E9%87%8F%E5%8C%96%E9%9A%BE%E5%BA%A6%E8%BD%AC%E7%A7%BB.png" alt="迁移强度量化难度转移" /></p>]]></content>
    
    
    <categories>
      
      <category>高性能计算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>CUDA基础</title>
    <link href="/Diffcc/Diffcc.github.io/2025/06/11/20250611/"/>
    <url>/Diffcc/Diffcc.github.io/2025/06/11/20250611/</url>
    
    <content type="html"><![CDATA[<p>可编程处理单元**（Graphic Processor Unit）<strong>日益发展成为高并行、多线程、多核处理的计算利器，2006年11月，NVIDIA推出了</strong>CUDA**——一种通用并行计算平台和编程模型，通过NVIDIA GPU中的并行计算引擎，以比CPU更高效的方式解决复杂的计算问题。</p><p>本文基于CUDA官方文档：<a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html">1. Preface — CUDA C++ Best Practices Guide 12.9 documentation</a></p><span id="more"></span><h2 id="GPU结构">GPU结构</h2><p>CPU和GPU在计算单元上存在以下的不同：</p><table><thead><tr><th style="text-align:left"><strong>组件</strong></th><th style="text-align:left"><strong>CPU</strong></th><th style="text-align:left"><strong>GPU</strong></th></tr></thead><tbody><tr><td style="text-align:left"><strong>控制单元</strong></td><td style="text-align:left">1个强大的控制单元（Control）</td><td style="text-align:left">极简控制逻辑（图中未单独标注）</td></tr><tr><td style="text-align:left"><strong>ALU数量</strong></td><td style="text-align:left">少量复杂ALU（通常4-8核）</td><td style="text-align:left">大量简单ALU（数千个流处理器）</td></tr><tr><td style="text-align:left"><strong>线程能力</strong></td><td style="text-align:left">支持少量并行线程（超线程优化）</td><td style="text-align:left">支持数万并发线程</td></tr></tbody></table><p>CPU通过单个控制单元管理少量的ALU，GPU无集中标注的控制单元，突出多ALU（<strong>Arithmetic Logic Unit，算术逻辑单元</strong>）并行；此外CPU利用大容量多级缓存（L1/L2/L3）实现不同层级的信息缓存，GPU的共享缓存则很少量，两者都连接DRAM（<strong>Dynamic Random-Access Memory，动态随机存取存储器，用于临时存储计算机运行时所需的程序和数据，断电后数据丢失</strong>），但GPU的显存带宽通常是CPU的5-10倍。</p><p><img src="CPU%E5%92%8CGPU%E7%9A%84%E5%AF%B9%E6%AF%94.png" alt="CPU和GPU的对比" /></p><p>总的来说，GPU通过<strong>高度并行运算</strong>，使用大量ALU组成流式多处理器（SM）来隐藏内存访问延迟，而不是像CPU一样利用大量的数据缓存和流控制来确保低延迟。</p><p>在CUDA并行编程中，有三个关键抽象为开发者提供高效的并行计算能力：</p><ol><li><strong>线程组层次结构（Hierarchy of Thread Groups）</strong></li><li><strong>共享内存（Shared Memory）</strong></li><li><strong>屏障同步（Barrier Synchronization）</strong></li></ol><p>线程组通过网格（Grid）、线程块（Block）、线程（Thread）的分层结构，利用共享内存<code>__shared__</code>进行快速数据交换，同时通过<code>__syncthreads()</code>确保数据的一致性，即块内所有的线程必须执行到当前点才能继续。</p><p><img src="%E5%85%B3%E9%94%AE%E6%8A%BD%E8%B1%A1%E7%9A%84%E5%85%B3%E7%B3%BB.png" alt="关键抽象的关系" /></p><h2 id="CUDA编程模型">CUDA编程模型</h2><p>首先需要明白CUDA 编程中的<em><strong>kernels</strong></em>的概念，<em><strong>kernel</strong></em>可以理解为CUDA扩展的下的函数实现，以C++为例（本文都采用C++作为示例），常规C++函数往往只能按序执行一次，而在<em><strong>kernels</strong></em>下，这些函数调用由N个不同的CUDA线程并行执行N次。</p><p>内核使用 <code>__global__</code>说明符定义，使用<code>&lt;&lt;&lt;block数、每个block的thread数&gt;&gt;&gt;</code>指定总的CUDA线程数（总线程数：numBlocks * threadPerBlock）,执行<em><strong>kernel</strong></em>的每个线程都有一个唯一的线程ID，即<code>threadIdx</code>。</p><p><strong>threadIdx</strong>是一个可以包含三个维度的向量，可以通过下表中的计算公式获得：</p><table><thead><tr><th style="text-align:left"><strong>维度</strong></th><th style="text-align:left"><strong>全局索引公式</strong></th></tr></thead><tbody><tr><td style="text-align:left">1D</td><td style="text-align:left"><code>tid = blockIdx.x * blockDim.x + threadIdx.x</code></td></tr><tr><td style="text-align:left">2D</td><td style="text-align:left"><code>row = blockIdx.y * blockDim.y + threadIdx.y</code> <code>col = blockIdx.x * blockDim.x + threadIdx.x</code></td></tr><tr><td style="text-align:left">3D</td><td style="text-align:left"><code>x = blockIdx.x * blockDim.x + threadIdx.x</code> <code>y = blockIdx.y * blockDim.y + threadIdx.y</code> <code>z = blockIdx.z * blockDim.z + threadIdx.z</code></td></tr></tbody></table><ul><li><code>threadIdx.x</code>：线程在块内的 x 维索引（从 0 开始）</li><li><code>blockIdx.x</code>：块在网格中的 x 维索引</li><li><code>blockDim.x</code>：每个块的 x 维线程数</li><li><code>gridDim.x</code>：网格中 x 维的块数</li></ul><p>对于一维块，线程索引和ID是相同的;对于大小为 （Dx， Dy） 的二维块，索引为 （x， y） 的线程 ID 为 （x + y Dx）;对于大小为 （Dx， Dy， Dz） 的三维块，索引为 （x， y， z） 的线程ID 为 （x + y Dx + z Dx Dy），可通过上表联立计算threadIdx得到。</p><p><img src="Grid_Block_Thread.png" alt="Grid_Block_Thread" /></p><p>同样的，通过<code>blockIdx</code>确定block所在的索引（可以理解为行），<code>blockDim</code>确定对应的维度，值得注意的是</p><p><code>&lt;&lt;&lt;…&gt;&gt;&gt;</code>可以被指定为int 或者是 <code>dim3</code>，用于表示二维块或者网格。</p><p>以下看几个具体的编程示例:</p><p><strong>两个一维的数组相加</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">VecAdd</span><span class="hljs-params">(<span class="hljs-type">float</span>* A, <span class="hljs-type">float</span>* B, <span class="hljs-type">float</span>* C)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> i = theadIdx.x; <span class="hljs-comment">//应为blockIdx.x + blockDim.x + threadIdx.x 但是此处block数为1</span><br>    C[i] = A[i] + B[i];<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    ...<br>    VecAdd&lt;&lt;&lt;<span class="hljs-number">1</span>, N&gt;&gt;&gt;(A, B, C);    <br>    ...<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>两个二维数组相加</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatAdd</span><span class="hljs-params">(<span class="hljs-type">float</span> A[N][N], <span class="hljs-type">float</span> B[N][N], <span class="hljs-type">float</span> C[N][N])</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> i = threadIdx.x; <span class="hljs-comment">//此处应为blockIdx.x * blockDim.x + threadIdx.x</span><br>    <span class="hljs-type">int</span> j = threadIdx.y; <span class="hljs-comment">//此处应为blockIdx.y * blockDim.y + threadIdx.y 但是block为1</span><br>    c[i][j] = A[i][j] + B[i][j];<br>&#125;<br><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> numBlocks = <span class="hljs-number">1</span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">threadsPerBlock</span><span class="hljs-params">(N,N)</span></span>;<br>    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B. C);<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>更为一般的情况</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatAdd</span><span class="hljs-params">(<span class="hljs-type">float</span> A[N][N], <span class="hljs-type">float</span> B[N][N], <span class="hljs-type">float</span> C[N][N])</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">int</span> j = blockIdx.y * blockDim.y + threadIdx.y;<br>    <span class="hljs-keyword">if</span>(i &lt; N ** j &lt; N)<br>    c[i][j] = A[i][j] + B[i][j];<br>&#125;<br><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-function">dim3 <span class="hljs-title">threadsPerBlock</span><span class="hljs-params">(<span class="hljs-number">16</span>,<span class="hljs-number">16</span>)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">numBlocks</span><span class="hljs-params">(N / threadsPerBlock.x, N / threadPerBlock.y)</span></span>;<br>    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B. C);<br>&#125;<br></code></pre></td></tr></table></figure><p>在边缘端设备Jetson Orin Nano 8G、Cuda-11.4运行<code>hello.cu</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span><span class="hljs-comment">//它声明了你在 CUDA kernel 函数中会使用的一些 内置变量</span></span><br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">()</span> </span>&#123;<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;hello world&quot;</span>);<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>kernel &lt;&lt;&lt;<span class="hljs-number">1</span>, <span class="hljs-number">1</span>&gt;&gt;&gt; ();<br>cudaError_t err = <span class="hljs-built_in">cudaDeviceSynchronize</span>();<br><span class="hljs-keyword">if</span> (err != cudaSuccess) &#123;<br>std::cerr &lt;&lt; <span class="hljs-string">&quot;CUDA Error: &quot;</span> &lt;&lt; <span class="hljs-built_in">cudaGetErrorString</span>(err) &lt;&lt; std::endl;<br>&#125;<br> <br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvcc hello.cu  -o hello<br><br>./hello<br>hello world<br></code></pre></td></tr></table></figure><h2 id="CUDA内存结构">CUDA内存结构</h2><p>CUDA线程在执行期间会访问到来自多个内存空间的数据，值得注意的是：</p><ul><li>每个线程都有自己独立的<strong>私有内存</strong>（类似于进程中线程的独立栈空间）</li><li>同一个线程块中含有<strong>共享内存（shared memory）</strong>，对该块中的所有线程可见，同时与该块具有相同的生命周期</li><li>除此之外，所有线程都可以访问相同的**全局内存（global memory）**以及只读的内存空间：<strong>常量内存（constant memory）<strong>和</strong>纹理内存（texture memory）</strong></li></ul><p><img src="%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84.png" alt="内存结构" /></p><p>当执行一个C++程序时，其中的线性化流程会在<strong>Host（主设备）<strong>上运行，其余的可以并行的程序将会在一个个独立的</strong>Device（从设备）<strong>上运行（例如</strong>kernel</strong>），<em><strong>CUDA runtime</strong></em>用于管理<strong>kernel</strong>可见的全局内存、常量内存以及纹理内存，同时负责device的内存分配和释放，以及<strong>host</strong>与<strong>device</strong>之间的数据传输。</p><p>不同的GPU设备具备不同的计算能力，称之为“SM version”, NVIDIA GPU架构全览如下：</p><table><thead><tr><th style="text-align:left"><strong>架构名称</strong></th><th style="text-align:left"><strong>发布时间</strong></th><th style="text-align:left"><strong>计算能力 (SM)</strong></th><th style="text-align:left"><strong>代表产品</strong></th><th style="text-align:left"><strong>制程工艺</strong></th><th style="text-align:left"><strong>核心创新</strong></th></tr></thead><tbody><tr><td style="text-align:left"><strong>Tesla</strong></td><td style="text-align:left">2006</td><td style="text-align:left">SM 1.x</td><td style="text-align:left">GeForce 8800 GTX</td><td style="text-align:left">90nm</td><td style="text-align:left">首款支持CUDA的架构</td></tr><tr><td style="text-align:left"><strong>Fermi</strong></td><td style="text-align:left">2010</td><td style="text-align:left">SM 2.x</td><td style="text-align:left">GTX 480, Tesla C2050</td><td style="text-align:left">40nm</td><td style="text-align:left">首次支持FP64、ECC显存、L1/L2缓存</td></tr><tr><td style="text-align:left"><strong>Kepler</strong></td><td style="text-align:left">2012</td><td style="text-align:left">SM 3.x</td><td style="text-align:left">GTX 680, Tesla K80</td><td style="text-align:left">28nm</td><td style="text-align:left">动态并行、Hyper-Q、GPU Boost</td></tr><tr><td style="text-align:left"><strong>Maxwell</strong></td><td style="text-align:left">2014</td><td style="text-align:left">SM 5.x</td><td style="text-align:left">GTX 980, Tesla M40</td><td style="text-align:left">28nm</td><td style="text-align:left">SMM设计、能效比提升2倍</td></tr><tr><td style="text-align:left"><strong>Pascal</strong></td><td style="text-align:left">2016</td><td style="text-align:left">SM 6.x</td><td style="text-align:left">GTX 1080 Ti, Tesla P100</td><td style="text-align:left">16nm</td><td style="text-align:left">NVLink 1.0、FP16支持、HBM2显存</td></tr><tr><td style="text-align:left"><strong>Volta</strong></td><td style="text-align:left">2017</td><td style="text-align:left">SM 7.0/7.2</td><td style="text-align:left">Tesla V100, Titan V</td><td style="text-align:left">12nm</td><td style="text-align:left">首代Tensor Core、独立线程调度</td></tr><tr><td style="text-align:left"><strong>Turing</strong></td><td style="text-align:left">2018</td><td style="text-align:left">SM 7.5</td><td style="text-align:left">RTX 2080 Ti, Tesla T4</td><td style="text-align:left">12nm</td><td style="text-align:left">RT Core、第二代Tensor Core（INT8/INT4）、GDDR6</td></tr><tr><td style="text-align:left"><strong>Ampere</strong></td><td style="text-align:left">2020</td><td style="text-align:left">SM 8.x</td><td style="text-align:left">RTX 3090, Tesla A100</td><td style="text-align:left">7nm/8nm</td><td style="text-align:left">第三代Tensor Core（TF32）、MIG、结构化稀疏</td></tr><tr><td style="text-align:left"><strong>Hopper</strong></td><td style="text-align:left">2022</td><td style="text-align:left">SM 9.x</td><td style="text-align:left">H100</td><td style="text-align:left">4nm</td><td style="text-align:left">第四代Tensor Core（FP8）、Transformer引擎、机密计算</td></tr><tr><td style="text-align:left"><strong>Ada Lovelace</strong></td><td style="text-align:left">2022</td><td style="text-align:left">SM 8.9</td><td style="text-align:left">RTX 4090</td><td style="text-align:left">5nm</td><td style="text-align:left">DLSS 3、光追性能翻倍、AV1编码</td></tr><tr><td style="text-align:left"><strong>Blackwell</strong></td><td style="text-align:left">2024*</td><td style="text-align:left">SM 10.x*</td><td style="text-align:left">B100 (预计)</td><td style="text-align:left">3nm*</td><td style="text-align:left">FP6支持、新一代NVLink*（*为预测特性）</td></tr></tbody></table><p>可以通过在代码中嵌入如下命令显示对应的 “SM_version”</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C++">cudaDeviceProp prop;<br><span class="hljs-built_in">cudaGetDeviceProperties</span>(&amp;prop, <span class="hljs-number">0</span>);<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Arch: SM_%d%d\n&quot;</span>, prop.major, prop.minor);<br><br><span class="hljs-comment">//Jetson Orin Nano 8G</span><br>jetson@unbutu:~/mhj/CUDA_ws$ ./hello <br>Arch: SM_87<br></code></pre></td></tr></table></figure><h2 id="CUDA编程接口">CUDA编程接口</h2><p>CUDA C++由<strong>C++ Language Extensions</strong> 和 <strong>CUDA Runtime</strong>组成，其中<strong>C++ Language Extensions</strong>即C++形式下的CUDA编程模型，例如C++形式下的<strong>kernels</strong>，而<strong>CUDA Runtime</strong> 则用于更底层的功能，运行在更低级的API上，例如在<strong>host</strong> 和 device之间传输数据，分配和释放内存，管理多个<strong>device</strong>等，往往使用C 和 C++混合实现。</p><h3 id="NVCC编译">NVCC编译</h3><p>NVCC的编译过程分为两个主要阶段：</p><ul><li><strong>主机代码(Host Code)处理</strong>：处理CPU端的C++代码</li><li><strong>设备代码(Device Code)处理</strong>：处理GPU端的CUDA代码</li></ul><p>主要分为以下几个阶段：</p><ul><li><strong>预处理阶段</strong>：首先调用C/C++预处理器处理<code>.cu</code>文件，处理所有<code>#include</code>、<code>#define</code>等预处理指令，展开宏定义。</li><li><strong>代码分离</strong>：NVCC将代码分离为主机代码和设备代码，主机代码即普通的C++代码，由CPU执行，设备代码，即标记有<code>__global__</code>、<code>__device__</code>等限定符的函数，由GPU执行。</li><li><strong>设备代码编译</strong>：使用NVIDIA的专有编译器前端和后端将设备代码编译为PTX(Parallel Thread eXecution)虚拟汇编代码，或者是直接编译为cubin二进制格式(使用<code>-cubin</code>选项)。</li><li><strong>主机代码编译</strong>：生成修改过的主机代码，其中包含对CUDA Runtime API的调用，传递给系统的C++编译器(如gcc、clang等)</li><li><strong>链接阶段</strong>：将所有对象文件(主机和设备)链接在一起，链接CUDA运行时库，生成最终的可执行文件。</li></ul><p>编译时有如下选项：</p><ul><li><code>--cuda</code>：只生成主机代码，不进行设备代码编译</li><li><code>-ptx</code>：只生成PTX代码</li><li><code>-cubin</code>：生成cubin二进制文件</li><li><code>-fatbin</code>：生成fatbin文件(包含多种架构的二进制)</li><li><code>-keep</code>：保留中间文件用于调试</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvcc -<span class="hljs-built_in">arch</span>=sm_75 -o my_program myprogram.cu<br><br>-<span class="hljs-built_in">arch</span>=sm_75 是 -gencode <span class="hljs-built_in">arch</span>=compute_35,code= \&#x27;compute_35,sm_35\&#x27; 的简写<br>也是 -gencode <span class="hljs-built_in">arch</span>=compute_35,code=sm_35的简写<br></code></pre></td></tr></table></figure><p><strong>Just-In-Time (JIT)</strong> 编译是一种动态编译技术，它在程序运行时(而非之前)将代码编译为机器指令。这种技术结合了解释执行的灵活性和预先编译(AOT, Ahead-Of-Time)的高效性。CUDA中的JIT编译将PTX代码在运行时编译为特定GPU的机器码，允许代码在不同代GPU上运行。</p><p>在CUDA中，JIT编译通常发生在：</p><ol><li>应用程序加载PTX代码时</li><li>驱动程序将PTX即时编译为当前GPU的特定机器码</li><li>缓存编译结果供后续使用</li></ol><h3 id="CUDA-Runtime">CUDA Runtime</h3><p>CUDA Runtime由<code>cudart</code>库实现，cudart库通过<code>cudart.lib</code>和<code>libcudart.a</code>静态链接，或者由<code>cudart.dll</code>或<code>libcudart.so</code>动态链接。</p><p>CUDA Runtime在第一次调用Runtime function时进行初始化，不显式初始化，在初始化期间，Runtime会为系统中的每个设备创建一个CUDA上下文，CUDA 上下文是指GPU 上的虚拟执行环境，包含所有 GPU 状态（内存、模块、流等）的容器，类似于 CPU 编程中的进程概念。</p><p>使用Runtime API隐式的创建上下文</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-built_in">cudaMalloc</span>(&amp;devPtr, size); <span class="hljs-comment">// 第一次调用Runtime API时会自动创建上下文</span><br></code></pre></td></tr></table></figure><h3 id="Device-Memory">Device Memory</h3><p>每个Host和Device都有自己独立的内存，Runtime提供分配释放Host memory和Device memory以及二者之间数据的传输。</p><p>Device memory 可以进一步分配为 linear memory 和 CUDA arrays，CUDA arrays是不透明的内存布局，而linear memory则被分配到一个统一的地址空间中。</p><p>linear memory通过 <code>cudaMalloc()</code>分配，<code>cudaFree()</code>释放，利用<code>cudaMemcpy()</code>进行host memory和device memory之间的数据传输。</p><p>下列命令实现了在Device 空间做运算，最后将device空间的数据拷贝到host空间</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span><span class="hljs-comment">//它声明了你在 CUDA kernel 函数中会使用的一些 内置变量</span></span><br><br><span class="hljs-comment">//device code</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">vecAdd</span><span class="hljs-params">(<span class="hljs-type">float</span>* A, <span class="hljs-type">float</span>* B, <span class="hljs-type">float</span>* C, <span class="hljs-type">int</span> N)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;<br>    <span class="hljs-keyword">if</span>( i &lt; N)<br>        C[i] = A[i] + B[i];<br>&#125; <br><br><span class="hljs-comment">//host code</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> N = <span class="hljs-number">16</span>;<br>    <span class="hljs-type">size_t</span> size = N * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br><br>    <span class="hljs-comment">//host memory </span><br>    <span class="hljs-type">float</span>* h_A = (<span class="hljs-type">float</span>*)<span class="hljs-built_in">malloc</span>(size);<br>    <span class="hljs-type">float</span>* h_B = (<span class="hljs-type">float</span>*)<span class="hljs-built_in">malloc</span>(size);<br>    <span class="hljs-type">float</span>* h_C = (<span class="hljs-type">float</span>*)<span class="hljs-built_in">malloc</span>(size);<br>    <span class="hljs-comment">/******</span><br><span class="hljs-comment">     * 初始化输入矩阵</span><br><span class="hljs-comment">     */</span><br><br>    <span class="hljs-comment">//device memory</span><br>    <span class="hljs-type">float</span>* d_A;<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_A, size);<br>    <span class="hljs-type">float</span>* d_B;<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_B, size);<br>    <span class="hljs-type">float</span>* d_C;<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_C, size);<br><br>    <span class="hljs-comment">//copy data from host to device</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(d_A, h_A, size, cudaMemcpyHostToDevice);<br>    <span class="hljs-built_in">cudaMemcpy</span>(d_B, h_B, size, cudaMemcpyHostToDevice);<br><br>    <span class="hljs-comment">//kernel</span><br>    <span class="hljs-type">int</span> threadsPerBlock = <span class="hljs-number">256</span>;<br>    <span class="hljs-type">int</span> blocksPerGrid = <br>    (N + threadsPerBlock - <span class="hljs-number">1</span>) / threadsPerBlock;<br><br>    vecAdd&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(d_A, d_B, d_C, N);<br><br>    <span class="hljs-comment">//copy data from device to host</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(h_C, d_C, size, cudaMemcpyDeviceToHost);<br><br>    <span class="hljs-comment">//free device memory</span><br>    <span class="hljs-built_in">cudaFree</span>(d_A);<br>    <span class="hljs-built_in">cudaFree</span>(d_B);<br>    <span class="hljs-built_in">cudaFree</span>(d_C);<br><br>    <span class="hljs-comment">/***</span><br><span class="hljs-comment">     * free host memory</span><br><span class="hljs-comment">     */</span><br><br>&#125;<br></code></pre></td></tr></table></figure><p>进一步的，可以通过<code>cudaMallocPitch()</code> 和 <code>cudaMalloc3D()</code> 分配2D和3D的数组（对应的数据复制可以使用<code>cudaMemcpy2D()</code> 和 <code>cudaMemcpy3D()</code>）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaMallocPitch</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">void</span>** devPtr,      <span class="hljs-comment">// 输出参数，返回分配的内存地址指针</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span>* pitch,      <span class="hljs-comment">// 输出参数，返回实际分配的行间距（字节数）</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> width,       <span class="hljs-comment">// 请求分配的每行宽度（字节数）</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> height,      <span class="hljs-comment">// 请求分配的行数</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> elementSize  <span class="hljs-comment">// 可选参数，元素大小（默认为1，CUDA 12.0+新增）</span></span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span>;<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaMalloc3D</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    cudaPitchedPtr* pitchedDevPtr,  <span class="hljs-comment">// 输出参数，返回分配的内存信息</span></span></span><br><span class="hljs-params"><span class="hljs-function">    cudaExtent extent               <span class="hljs-comment">// 请求分配的3D空间范围</span></span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span>;<br><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaExtent</span> &#123;<br>    <span class="hljs-type">size_t</span> width;   <span class="hljs-comment">// 宽度（字节数）</span><br>    <span class="hljs-type">size_t</span> height;  <span class="hljs-comment">// 高度（元素行数）</span><br>    <span class="hljs-type">size_t</span> depth;   <span class="hljs-comment">// 深度（切片数）</span><br>&#125;;<br><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaPitchedPtr</span> &#123;<br>    <span class="hljs-type">void</span>* ptr;      <span class="hljs-comment">// 内存指针</span><br>    <span class="hljs-type">size_t</span> pitch;   <span class="hljs-comment">// 行间距（字节数）</span><br>    <span class="hljs-type">size_t</span> xsize;   <span class="hljs-comment">// 实际分配的宽度</span><br>    <span class="hljs-type">size_t</span> ysize;   <span class="hljs-comment">// 实际分配的高度</span><br>&#125;;<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span></span><br><br><span class="hljs-comment">//device code</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">Mykernel</span><span class="hljs-params">(<span class="hljs-type">float</span>* devPtr, <span class="hljs-type">size_t</span> pitch, <span class="hljs-type">int</span> width, <span class="hljs-type">int</span> height)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> r = <span class="hljs-number">0</span>; r &lt; height; ++r)<br>    &#123;<br>        <span class="hljs-type">float</span>* row = (<span class="hljs-type">float</span>*)((<span class="hljs-type">char</span>*)devPtr + r * pitch);<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> c = <span class="hljs-number">0</span>; c &lt; width; ++c)<br>            <span class="hljs-type">int</span> element = row[c];<br>    &#125;<br>&#125;<br><br><span class="hljs-comment">//host code</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> width = <span class="hljs-number">64</span>, height = <span class="hljs-number">64</span>;<br>    <span class="hljs-type">float</span>* devPtr;<br>    <span class="hljs-type">size_t</span> pitch;<br><br>    <span class="hljs-built_in">cudaMallocPitch</span>(&amp;devPtr, &amp;pitch, width, height);<br><br>    Mykernel&lt;&lt;&lt;<span class="hljs-number">100</span>, <span class="hljs-number">512</span>&gt;&gt;&gt;(devPtr, pitch, width, height);<br>&#125;<br></code></pre></td></tr></table></figure><p><img src="3D%E7%BB%93%E6%9E%84.jpg" alt="3D结构" /></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span></span><br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">Mykernel</span><span class="hljs-params">(cudaPitchedPtr devPitchPtr,</span></span><br><span class="hljs-params"><span class="hljs-function">                    <span class="hljs-type">int</span> width, <span class="hljs-type">int</span> height, <span class="hljs-type">int</span> depth)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">char</span>* devPtr = (<span class="hljs-type">char</span>*)devPitchPtr.ptr;<br>    <span class="hljs-type">size_t</span> pitch = devPitchPtr.pitch;<br>    <span class="hljs-type">size_t</span> slicePitch = height * pitch;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> z = <span class="hljs-number">0</span>; z &lt; depth; ++z)<br>    &#123;<br>        <span class="hljs-type">char</span>* slice = devPtr + z*slicePitch;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y = <span class="hljs-number">0</span>; y &lt; height; ++y)<br>        &#123;<br>            <span class="hljs-type">float</span>* row = (<span class="hljs-type">float</span>*)(slice + y* pitch);<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> x = <span class="hljs-number">0</span>; x &lt; width; ++x)<br>                <span class="hljs-type">float</span> element = row[x];<br>        &#125;<br>    &#125;<br><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> width = <span class="hljs-number">64</span>, height = <span class="hljs-number">64</span>, depth = <span class="hljs-number">64</span>;<br>    cudaExtent extent = <span class="hljs-built_in">make_cudaExtent</span>(width * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), height, depth);<br><br>    cudaPitchedPtr devPitchedPtr;<br>    <span class="hljs-built_in">cudaMalloc3D</span>(&amp;devPitchedPtr, extent);<br><br>    Mykernel&lt;&lt;&lt;<span class="hljs-number">100</span>, <span class="hljs-number">512</span>&gt;&gt;&gt;(devPitchedPtr, width, height, depth);<br>&#125;<br></code></pre></td></tr></table></figure><p>有如下的变量声明</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++">__device__ <span class="hljs-type">float</span> devData; <span class="hljs-comment">//设备端(Device)的全局变量</span><br>__constant__ <span class="hljs-type">float</span> constData; <span class="hljs-comment">//只读且缓存</span><br>__shared__ <span class="hljs-type">float</span> sharedData;  <span class="hljs-comment">// 更快但块内可见</span><br></code></pre></td></tr></table></figure><p>使用 <code>cudaMemcpyToSymbol</code>将host内存拷贝到device端</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaMemcpyToSymbol</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">void</span>* symbol, <span class="hljs-comment">// 设备端的符号(通常是全局变量或常量)</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">void</span>* src,    <span class="hljs-comment">// 主机端源数据指针</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> count,       <span class="hljs-comment">// 要拷贝的字节数</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> offset = <span class="hljs-number">0</span>,  <span class="hljs-comment">// 可选参数，符号地址的偏移量(默认为0)</span></span></span><br><span class="hljs-params"><span class="hljs-function">    cudaMemcpyKind kind = cudaMemcpyHostToDevice <span class="hljs-comment">// 拷贝方向</span></span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span>;<br></code></pre></td></tr></table></figure><p>使用 <code>cudaMemcpyFromSymbol</code>将device侧拷贝到host内存</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaMemcpyFromSymbol</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">void</span>* dst,          <span class="hljs-comment">// 主机端目标指针</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">void</span>* symbol, <span class="hljs-comment">// 设备端的符号</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> count,       <span class="hljs-comment">// 要拷贝的字节数</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">size_t</span> offset = <span class="hljs-number">0</span>,  <span class="hljs-comment">// 可选参数，符号地址的偏移量(默认为0)</span></span></span><br><span class="hljs-params"><span class="hljs-function">    cudaMemcpyKind kind = cudaMemcpyDeviceToHost <span class="hljs-comment">// 拷贝方向</span></span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span>;<br></code></pre></td></tr></table></figure><h3 id="Shared-Memory">Shared Memory</h3><p>共享内存使用<code>__shared__</code>说明符实现，其速度比全局内存快得多，下面代码是一个矩阵乘法的实现</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span></span><br><br><span class="hljs-keyword">typedef</span> <span class="hljs-keyword">struct</span><br>&#123;<br>    <span class="hljs-type">int</span> width;<br>    <span class="hljs-type">int</span> height;<br>    <span class="hljs-type">float</span>* elements;<br><br>&#125; Matrix;<br><br><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> BLOCK_SIZE 16</span><br><br><span class="hljs-comment">//Forward declaration</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatMulkernel</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix, <span class="hljs-type">const</span> Matrix, Matrix)</span></span>;<br><br><br><span class="hljs-comment">//Host code</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">MatMul</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix A, <span class="hljs-type">const</span> Matrix B, Matrix C)</span></span><br><span class="hljs-function"></span>&#123;<br>    Matrix d_A;<br>    d_A.width = A.width; d_A.height = A.height;<br>    <span class="hljs-type">size_t</span> size = A.width * A.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_A.elements, size);<br>    <span class="hljs-comment">//host to device</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(d_A.elements, A.elements, size, cudaMemcpyHostToDevice);<br><br>    Matrix d_B;<br>    d_B.width = B.width, d_B.height = B.height;<br>    size = B.width * B.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_B.elements, size);<br>    <span class="hljs-comment">//host to device</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(d_B.elements, B.elements, size, cudaMemcpyHostToDevice);<br><br><br>    Matrix d_C;<br>    d_C.width = C.width, d_C.height = C.height;<br>    size = C.width * C.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-comment">//only need to malloc memory in device </span><br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_C.elements, size);<br><br>    <span class="hljs-function">dim3 <span class="hljs-title">dimBlock</span><span class="hljs-params">(BLOCK_SIZE, BLOCK_SIZE)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">dimGrid</span><span class="hljs-params">(B.width / BLOCK_SIZE, A.height / BLOCK_SIZE)</span></span>;<br><br>    MatMulkernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C);<br><br>    <span class="hljs-comment">//read C from device</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(C.elements, d_C.elements, size, cudaMemcpyDeviceToHost);<br><br><br>    <span class="hljs-built_in">cudaFree</span>(d_A.elements);<br>    <span class="hljs-built_in">cudaFree</span>(d_B.elements);<br>    <span class="hljs-built_in">cudaFree</span>(d_C.elements);<br>&#125;<br><br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatMulkernel</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix A, <span class="hljs-type">const</span> Matrix B, Matrix C)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">float</span> Cvalue = <span class="hljs-number">0</span>;<br>    <span class="hljs-type">int</span> row = blockDim.y * blockIdx.y + threadIdx.y;<br>    <span class="hljs-type">int</span> col = blockDim.x * blockIdx.x + threadIdx.x;<br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; A.width; ++i)<br>    &#123;<br>        Cvalue += A.elements[row * A.width + i] * B.elements[i * B.width + col];<br>    &#125;<br>    C.elements[row * C.width + col] = Cvalue;<br>&#125;<br><br></code></pre></td></tr></table></figure><p><img src="Matrix_Multiplication.png" alt="Matrix_Multiplication" /></p><p>进一步采用共享内存优化，将大矩阵分解为 <code>BLOCK_SIZE × BLOCK_SIZE</code> 的子矩阵，每个线程块计算一个子矩阵 <code>Csub</code>，每个线程计算 <code>Csub</code> 的一个元素，<code>As</code> 和 <code>Bs</code> 存储在共享内存中，减少对全局内存的访问，每个线程块只需加载 <code>Asub</code> 和 <code>Bsub</code> 一次，然后所有线程共享这些数据。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span></span><br><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> BLOCK_SIZE 16</span><br><br><span class="hljs-keyword">typedef</span> <span class="hljs-keyword">struct</span><br>&#123;<br>    <span class="hljs-type">int</span> width;<br>    <span class="hljs-type">int</span> height;<br>    <span class="hljs-type">int</span> stride; <span class="hljs-comment">/// 矩阵的内存步长（通常等于width, 表示一行有多少个元素）</span><br>    <span class="hljs-type">float</span> * elements;<br>&#125; Matrix;<br><br><span class="hljs-comment">//Get matrix element in[row, col]</span><br><span class="hljs-function">__device__ <span class="hljs-type">float</span> <span class="hljs-title">GetElement</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix A, <span class="hljs-type">int</span> row, <span class="hljs-type">int</span> col)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">return</span> A.elements[row * A.stride + col];<br>&#125;<br><br><span class="hljs-comment">//Set matrix element in[row,col]</span><br><span class="hljs-function">__device__ <span class="hljs-type">void</span> <span class="hljs-title">SetElement</span><span class="hljs-params">(Matrix A, <span class="hljs-type">int</span> row, <span class="hljs-type">int</span> col, <span class="hljs-type">float</span> value)</span></span><br><span class="hljs-function"></span>&#123;<br>    A.elements[row * A.stride + col] = value;<br>&#125;<br><br><span class="hljs-comment">//Get subMatrix in [row, col] (row col describe block not element)</span><br><span class="hljs-function">__device__ Matrix <span class="hljs-title">GetSubMatrix</span><span class="hljs-params">(Matrix A, <span class="hljs-type">int</span> row, <span class="hljs-type">int</span> col)</span></span><br><span class="hljs-function"></span>&#123;<br>    Matrix Asub;<br>    Asub.width = BLOCK_SIZE;<br>    Asub.height = BLOCK_SIZE;<br>    Asub.stride = A.stride;<br>    Asub.elements = &amp;A.elements[A.stride * row * BLOCK_SIZE + BLOCK_SIZE * col];<br><br>    <span class="hljs-keyword">return</span> Asub;<br>&#125;<br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatMulkernel</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix, <span class="hljs-type">const</span> Matrix, Matrix)</span></span>;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">MatMul</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix A, <span class="hljs-type">const</span> Matrix B, Matrix C)</span></span><br><span class="hljs-function"></span>&#123;<br>    Matrix d_A;<br>    d_A.width = d_A.stride = A.width; d_A.height = A.height;<br>    <span class="hljs-type">size_t</span> size = d_A.width * d_A.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_A.elements, size);<br>    <span class="hljs-built_in">cudaMemcpy</span>(d_A.elements, A.elements, size, cudaMemcpyHostToDevice);<br><br><br>    Matrix d_B;<br>    d_B.width = d_B.stride = B.width; d_B.height = B.height;<br>    <span class="hljs-type">size_t</span> size = d_B.width * d_B.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_B.elements, size);<br>    <span class="hljs-built_in">cudaMemcpy</span>(d_B.elements, B.elements, size, cudaMemcpyHostToDevice);<br><br><br>    Matrix d_C;<br>    d_C.width = d_C.stride = C.width; d_C.height = C.height;<br>    <span class="hljs-type">size_t</span> size = d_C.width * d_C.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_C.elements, size);<br><br>    <span class="hljs-function">dim3 <span class="hljs-title">dimBlock</span><span class="hljs-params">(BLOCK_SIZE, BLOCK_SIZE)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">dimGrid</span><span class="hljs-params">(B.width / BLOCK_SIZE, A.height / BLOCK_SIZE)</span></span>;<br><br>    MatMulkernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C);<br><br>    <span class="hljs-comment">//read C from device</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(C.elements, d_C.elements, size, cudaMemcpyDeviceToHost);<br><br><br>    <span class="hljs-built_in">cudaFree</span>(d_A.elements);<br>    <span class="hljs-built_in">cudaFree</span>(d_B.elements);<br>    <span class="hljs-built_in">cudaFree</span>(d_C.elements);<br>    <br>&#125;<br><br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatMulkernel</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix A, <span class="hljs-type">const</span> Matrix B, Matrix C)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> blockRow = blockDim.y;<br>    <span class="hljs-type">int</span> blockCol = blockDim.x;<br><br>    <span class="hljs-comment">//Each Block computes one sub-matrix of C</span><br>    Matrix Csub = <span class="hljs-built_in">GetSubMatrix</span>(C, blockRow, blockCol);<br><br>    <span class="hljs-comment">//Thread row and col in Csub</span><br>    <span class="hljs-type">int</span> row = threadIdx.y;<br>    <span class="hljs-type">int</span> col = threadIdx.x;<br><br>    <span class="hljs-comment">//Each thread computes one element of Csub</span><br>    <span class="hljs-type">float</span> Cvalue = <span class="hljs-number">0</span>;<br><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> m = <span class="hljs-number">0</span>; m &lt; (A.width / BLOCK_SIZE); ++m)<br>    &#123;<br>        Matrix Asub = <span class="hljs-built_in">GetSubMatrix</span>(A, blockRow, m);<br>        Matrix Bsub = <span class="hljs-built_in">GetSubMatrix</span>(B, m, blockCol);<br><br>        <span class="hljs-comment">//shared_memory to store Asub and Bsub</span><br>        __shared__ <span class="hljs-type">float</span> As[BLOCK_SIZE][BLOCK_SIZE];<br>        __shared__ <span class="hljs-type">float</span> Bs[BLOCK_SIZE][BLOCK_SIZE];<br><br>        As[row][col] = <span class="hljs-built_in">GetElement</span>(A, row, col);<br>        Bs[row][col] = <span class="hljs-built_in">GetElement</span>(B, row, col);<br><br>        <span class="hljs-comment">//确保上述所有操作同步</span><br>        __syncthreads();<br><br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> e = <span class="hljs-number">0</span>; e &lt; BLOCK_SIZE; e++)<br>            Cvalue += As[row][e] * Bs[e][col];<br><br>        <br>        __syncthreads();<br>    &#125;<br><br>    <span class="hljs-built_in">SetElement</span>(C, row, col, Cvalue);<br>&#125;<br><br></code></pre></td></tr></table></figure><p>值得注意的是 <code>GetSubMatrix()</code>的逻辑，其中的<code>[row,col]</code>是以整个<strong>Block</strong>为单位，而不再是以单个element为单元, 同时矩阵的宽度和高度必须是 <code>BLOCK_SIZE</code> 的整数倍。</p><p><img src="subMatrix.jpg" alt="subMatrix" /></p><p><img src="Shared_memory.png" alt="Shared_memory" /></p><h3 id="Page-Locked-Host-Memory（页锁定主机内存）">Page-Locked Host Memory（页锁定主机内存）</h3><p>Page-Locked Host Memory（也称为 <strong>Pinned Memory</strong> 或 <strong>Non-Pageable Memory</strong>）是 CUDA 中一种特殊的主机（CPU）内存分配方式，它<strong>禁止操作系统对这块内存进行分页交换（Page-Out）</strong>，从而保证内存始终驻留在物理 RAM 中，不会因虚拟内存机制被换出到磁盘.</p><p>在默认情况下，主机内存是 <strong>Pageable（可分页的）</strong>，操作系统可以随时将不活跃的内存页换出到磁盘（Swap Space）。但在 GPU 计算中，这会带来两个问题：</p><ol><li><strong>异步内存拷贝的效率问题</strong><ul><li>CUDA 的 <code>cudaMemcpy</code> 默认是同步操作，但如果使用 <code>cudaMemcpyAsync</code>（异步拷贝），要求源或目标内存必须是 <strong>Page-Locked</strong>，否则无法保证 DMA（直接内存访问）的正确性。</li><li>如果内存可分页，GPU 驱动必须先临时锁定内存，再执行拷贝，这会降低性能。</li></ul></li><li><strong>零拷贝内存（Zero-Copy）支持</strong><ul><li>Page-Locked Memory 可以直接映射到 GPU 地址空间，允许 GPU 直接访问主机内存（避免显式拷贝），但要求内存必须是锁定的</li></ul></li></ol><p>所以页锁主机内存适用于<strong>频繁的 CPU-GPU 数据传输</strong>（如深度学习数据加载），<strong>异步内存拷贝（<code>cudaMemcpyAsync</code>）</strong>，以及<strong>Zero-Copy 内存（GPU 直接访问主机内存）</strong></p><p>使用<code>cudaHostAlloc</code> 、<code>cudaFreeHost</code> 进行分配和释放内存</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-type">float</span> *h_data;<br><span class="hljs-built_in">cudaHostAlloc</span>((<span class="hljs-type">void</span>**)&amp;h_data, size_in_bytes, cudaHostAllocDefault);<br><span class="hljs-comment">// ... 使用 h_data ...</span><br><span class="hljs-built_in">cudaFreeHost</span>(h_data);<br></code></pre></td></tr></table></figure><p><code>cudaHostAlloc</code>有如下的几个选项：</p><ol><li><p><strong>可移植内存（Portable Memory）</strong></p><p>Page-Locked内存通常仅对分配时当前的GPU设备有效，通过<code>cudaHostAllocPortable</code>标志分配的内存可被系统中<strong>所有GPU设备</strong>直接使用，适用于多GPU环境。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 分配可移植的Page-Locked内存</span><br><span class="hljs-built_in">cudaHostAlloc</span>(&amp;h_portable, N*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaHostAllocPortable);<br><br><span class="hljs-comment">// 在多个设备上使用</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> dev = <span class="hljs-number">0</span>; dev &lt; num_devices; dev++) &#123;<br>    <span class="hljs-built_in">cudaSetDevice</span>(dev);<br>    <span class="hljs-built_in">cudaMemcpyAsync</span>(d_data[dev], h_portable, N*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice, stream[dev]);<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p><strong>写合并内存（Write-Combining Memory）</strong></p><p>Page-Locked内存通常是<strong>可缓存的</strong>（Cacheable），会占用CPU的L1/L2缓存,通过<code>cudaHostAllocWriteCombined</code>分配的内存为<strong>写合并内存</strong>，释放L1/L2缓存资源供其他应用使用，适用于主机频繁写入、GPU读取的数据（如实时数据采集）。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 分配Write-Combining内存（仅主机写入）</span><br><span class="hljs-built_in">cudaHostAlloc</span>(&amp;h_wc, N*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaHostAllocWriteCombined);<br><br><span class="hljs-comment">// 主机填充数据（快速）</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; N; i++) h_wc[i] = data[i];<br><br><span class="hljs-comment">// 传输到GPU（高效）</span><br><span class="hljs-built_in">cudaMemcpyAsync</span>(d_data, h_wc, N*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice, stream);<br></code></pre></td></tr></table></figure></li><li><p><strong>映射内存（Mapped Memory）</strong></p><p>利用<strong>零拷贝（Zero-Copy）</strong>，实现主机内存直接映射到GPU地址空间，无需显式拷贝。</p><p>主机地址：通过<code>cudaHostAlloc</code>返回。</p><p>设备地址：通过<code>cudaHostGetDevicePointer</code>获取。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-type">float</span> *h_mapped;<br><span class="hljs-built_in">cudaHostAlloc</span>(&amp;h_mapped, size, cudaHostAllocMapped);<br><br><span class="hljs-type">float</span> *d_mapped;<br><span class="hljs-built_in">cudaHostGetDevicePointer</span>(&amp;d_mapped, h_mapped, <span class="hljs-number">0</span>);<br></code></pre></td></tr></table></figure><p>内核访问时自动触发传输，无需<code>cudaMemcpy</code>, 同时确保了数据传输与内核计算自动并行。</p><h3 id="异步并发执行">异步并发执行</h3><p>CUDA将以下操作视为独立任务，可并行执行：</p><ul><li><strong>主机计算</strong>（CPU任务）</li><li><strong>设备计算</strong>（GPU内核）</li><li><strong>主机→设备内存传输</strong>（H2D）</li><li><strong>设备→主机内存传输</strong>（D2H）</li><li><strong>设备内内存传输</strong>（D2D）</li><li><strong>多设备间内存传输</strong></li></ul><ol><li><p><strong>Host与Device并发执行</strong></p><p><strong>异步函数</strong>：通过非阻塞调用（如<code>cudaMemcpyAsync</code>、<code>kernel&lt;&lt;&lt;...&gt;&gt;&gt;</code>）让主机线程立即返回，无需等待设备完成。</p><p>注：<code>cudaMemcpyAsync</code> 是 CUDA 中用于<strong>异步内存拷贝</strong>的核心函数，它的主要作用是在<strong>不阻塞主机（CPU）线程</strong>的情况下，在主机（Host）与设备（Device）之间或设备内部传输数据。与同步版本的 <code>cudaMemcpy</code> 不同，<code>cudaMemcpyAsync</code> 允许 CPU 在数据传输的同时继续执行其他任务，从而实现<strong>计算与传输的重叠</strong>，提升程序的整体效率。</p></li><li><p><strong>并发内核执行</strong></p><p>设备需<strong>Compute Capability ≥ 2.0</strong>，且<code>concurrentKernels</code>属性为1。</p><ul><li><p><strong>上下文隔离</strong>：不同CUDA上下文的内核无法并发。</p></li><li><p><strong>资源竞争</strong>：占用大量纹理内存或本地内存的内核会降低并发性。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> can_concurrent;<br><span class="hljs-built_in">cudaDeviceGetAttribute</span>(&amp;can_concurrent, cudaDevAttrConcurrentKernels, dev);<br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>数据传输与内核执行重叠</strong></p><ul><li><p>设备需支持<code>asyncEngineCount &gt; 0</code>。</p></li><li><p><strong>必须使用Page-Locked主机内存</strong></p><ol><li><p><strong>H2D/D2H传输与内核执行重叠</strong>：</p><ul><li>异步拷贝（<code>cudaMemcpyAsync</code>）与内核并发。</li></ul></li><li><p><strong>设备内拷贝（D2D）与内核执行重叠</strong>：</p><ul><li><p>需<code>concurrentKernels</code>支持</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaStream_t stream1, stream2;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream1);<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream2);<br><br><span class="hljs-comment">// 异步H2D传输（流1）</span><br><span class="hljs-built_in">cudaMemcpyAsync</span>(d_data, h_data, size, cudaMemcpyHostToDevice, stream1);<br><br><span class="hljs-comment">// 内核执行（流2，与传输重叠）</span><br>kernel&lt;&lt;&lt;grid, block, <span class="hljs-number">0</span>, stream2&gt;&gt;&gt;(d_data);<br><br><span class="hljs-comment">// 异步D2H传输（流1）</span><br><span class="hljs-built_in">cudaMemcpyAsync</span>(h_result, d_data, size, cudaMemcpyDeviceToHost, stream1);<br></code></pre></td></tr></table></figure></li></ul></li></ol></li></ul></li><li><p><strong>并发数据传输</strong></p><ul><li>设备需<code>asyncEngineCount = 2</code>（Compute Capability ≥ 2.0）。</li><li><strong>必须使用Page-Locked内存</strong>。</li><li>同时进行H2D和D2H传输（双向重叠）</li></ul></li><li><p><strong>流管理</strong></p><p><strong>流</strong>是命令序列（如内核、内存拷贝），在<strong>同一流内顺序执行</strong>，<strong>不同流间可能并发</strong>，<strong>无显式同步时，流间执行顺序不确定</strong>。</p><p>流的创建与销毁</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaStream_t stream;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream);  <span class="hljs-comment">// 创建流</span><br><br><span class="hljs-comment">// 在流中执行操作</span><br>kernel&lt;&lt;&lt;grid, block, <span class="hljs-number">0</span>, stream&gt;&gt;&gt;(...);<br><span class="hljs-built_in">cudaMemcpyAsync</span>(..., stream);<br><br><span class="hljs-built_in">cudaStreamDestroy</span>(stream);  <span class="hljs-comment">// 销毁流（若流未完成，自动等待后释放资源）</span><br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaStream_t streams[<span class="hljs-number">2</span>];<br><span class="hljs-type">float</span> *h_pinned, *d_data;<br><span class="hljs-built_in">cudaMallocHost</span>(&amp;h_pinned, <span class="hljs-number">2</span> * size);  <span class="hljs-comment">// Page-Locked内存</span><br><span class="hljs-built_in">cudaMalloc</span>(&amp;d_data, <span class="hljs-number">2</span> * size);<br><br><span class="hljs-comment">// 创建流</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">2</span>; i++) <br>    <span class="hljs-built_in">cudaStreamCreate</span>(&amp;streams[i]);<br><br><span class="hljs-comment">// 异步操作（流间可能并发）</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">2</span>; i++) &#123;<br>    <span class="hljs-built_in">cudaMemcpyAsync</span>(d_data + i*size, h_pinned + i*size, <br>                    size, cudaMemcpyHostToDevice, streams[i]);<br>    kernel&lt;&lt;&lt;grid, block, <span class="hljs-number">0</span>, streams[i]&gt;&gt;&gt;(d_data + i*size);<br>    <span class="hljs-built_in">cudaMemcpyAsync</span>(h_pinned + i*size, d_data + i*size,<br>                    size, cudaMemcpyDeviceToHost, streams[i]);<br>&#125;<br><br><span class="hljs-comment">// 同步所有流</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">2</span>; i++) <br>    <span class="hljs-built_in">cudaStreamSynchronize</span>(streams[i]);<br><br><span class="hljs-comment">// 释放资源</span><br><span class="hljs-built_in">cudaFreeHost</span>(h_pinned);<br><span class="hljs-built_in">cudaFree</span>(d_data);<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">2</span>; i++) <br>    <span class="hljs-built_in">cudaStreamDestroy</span>(streams[i]);<br></code></pre></td></tr></table></figure><p>未指定流或流为<code>0</code>时使用<strong>全局NULL流</strong>（隐式同步所有操作），即默认的编译选项<code>--default-stream legacy</code>，也可以指定–<code>default-stream per-thread</code>或定义宏<code>CUDA_API_PER_THREAD_DEFAULT_STREAM</code>来使每个主机线程都有独立的默认流，支持并发。</p><p>流的同步方式有两种：</p><ol><li><p>显式同步</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaStreamSynchronize</span>(stream);  <span class="hljs-comment">// 等待流完成</span><br><span class="hljs-built_in">cudaDeviceSynchronize</span>();        <span class="hljs-comment">// 等待所有流完成</span><br><br><span class="hljs-built_in">cudaStreamWaitEvent</span>();<br><span class="hljs-built_in">cudaStreamQuery</span>(<br></code></pre></td></tr></table></figure></li><li><p>隐式同步</p><p>NULL流中的操作会同步所有其他流。</p></li></ol></li></ol><p>当主机线程在两个流的命令之间插入以下操作时，<strong>不同流的命令将失去并发性</strong>，导致串行执行:</p><ol><li><p><strong>页锁定主机内存分配</strong></p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss"><span class="hljs-built_in">cudaMallocHost</span>(&amp;ptr, size);  <span class="hljs-comment">// 阻塞所有流的并发</span><br></code></pre></td></tr></table></figure></li><li><p><strong>设备内存分配</strong></p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss"><span class="hljs-built_in">cudaMalloc</span>(&amp;d_ptr, size);    <span class="hljs-comment">// 强制同步</span><br></code></pre></td></tr></table></figure></li><li><p><strong>设备内存初始化（如cudaMemset）</strong></p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss"><span class="hljs-built_in">cudaMemset</span>(d_ptr, <span class="hljs-number">0</span>, size);  <span class="hljs-comment">// 隐式同步点</span><br></code></pre></td></tr></table></figure></li><li><p><strong>同一设备内存的拷贝（D2D）</strong></p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss"><span class="hljs-built_in">cudaMemcpy</span>(dst, src, size, cudaMemcpyDeviceToDevice); <span class="hljs-comment">// 内部同步</span><br></code></pre></td></tr></table></figure></li><li><p><strong>NULL流（默认流）中的任何操作</strong></p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss">kernel&lt;&lt;&lt;<span class="hljs-attribute">grid</span>, block&gt;&gt;&gt;();  <span class="hljs-comment">// 默认流操作会同步所有流</span><br></code></pre></td></tr></table></figure></li><li><p><strong>L1/Shared内存配置切换</strong>（针对Compute Capability 3.x/7.x设备）</p></li></ol><p>两个流之间的执行重叠量取决于向每个流发出命令的顺序，以及设备是否支持数据传输和内核执行的重叠，并发内核执行，或并发数据传输。</p><table><thead><tr><th style="text-align:left"><strong>场景</strong></th><th style="text-align:left"><strong>问题根源</strong></th><th style="text-align:left"><strong>优化方法</strong></th></tr></thead><tbody><tr><td style="text-align:left">不支持并发数据传输的设备</td><td style="text-align:left">H2D/D2H共用引擎</td><td style="text-align:left">无解，需升级硬件</td></tr><tr><td style="text-align:left">支持并发数据传输的设备</td><td style="text-align:left">任务提交顺序限制重叠</td><td style="text-align:left">分组提交（H2D→Kernel→D2H）</td></tr><tr><td style="text-align:left">计算能力≤3.0的设备</td><td style="text-align:left">内核启动依赖全局线程块进度</td><td style="text-align:left">提前提交所有内核，延迟D2H</td></tr><tr><td style="text-align:left">任何设备</td><td style="text-align:left">NULL流或隐式同步操作</td><td style="text-align:left">使用显式流，避免全局操作</td></tr></tbody></table><p>例如，我们在不支持并发数据传输的设备上，执行下面的逻辑：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 流0：H2D -&gt; Kernel -&gt; D2H</span><br><span class="hljs-comment">// 流1：H2D -&gt; Kernel -&gt; D2H （提交顺序导致串行化）</span><br></code></pre></td></tr></table></figure><p>流1的H2D必须等待流0的D2H完成（因为D2H和H2D共用传输引擎）,由于<strong>零重叠</strong>，所以流0和流1是完全串行执行。</p><p><strong><code>cudaLaunchHostFunc</code></strong> 函数在CUDA流的特定位置插入一个<strong>主机端函数</strong>，该函数会在流中<strong>此前所有命令完成</strong>后自动触发，由CUDA runtime调度，不会阻塞</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaLaunchHostFunc</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    cudaStream_t stream,          <span class="hljs-comment">// 目标流</span></span></span><br><span class="hljs-params"><span class="hljs-function">    cudaHostFn_t fn,              <span class="hljs-comment">// 回调函数指针</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">void</span>* userData                <span class="hljs-comment">// 传递给回调的用户数据</span></span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span>;<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">void</span> CUDART_CB <span class="hljs-title">MyCallback</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    cudaStream_t stream,  <span class="hljs-comment">// 关联的流</span></span></span><br><span class="hljs-params"><span class="hljs-function">    cudaError_t status,   <span class="hljs-comment">// 流中前置操作的状态（成功/错误）</span></span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">void</span>* data            <span class="hljs-comment">// 用户数据</span></span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span>;<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-built_in">cudaMemcpyAsync</span>(..., stream);  <span class="hljs-comment">// 操作1</span><br>kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;();     <span class="hljs-comment">// 操作2</span><br><span class="hljs-built_in">cudaLaunchHostFunc</span>(stream, MyCallback, data); <span class="hljs-comment">// 回调</span><br><span class="hljs-comment">// MyCallback 仅在操作1和2完成后执行</span><br></code></pre></td></tr></table></figure><p>需要注意的是回调函数调用CUDA API（如<code>cudaMemcpy</code>），可能等待自身完成，导致死锁。</p><p>在运行时，高优先级流中的待处理工作优先于低优先级流中的待处理工作</p><p>可以使用<code>cudaDeviceGetStreamPriorityRange()</code>获取当前的优先级范围，并且利用<code>cudaStreamCreateWithPriority()</code>设定流的优先级。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// get the range of stream priorities for this device</span><br><span class="hljs-type">int</span> priority_high, priority_low;<br><span class="hljs-built_in">cudaDeviceGetStreamPriorityRange</span>(&amp;priority_low, &amp;priority_high);<br><span class="hljs-comment">// create streams with highest and lowest available priorities</span><br>cudaStream_t st_high, st_low;<br><span class="hljs-built_in">cudaStreamCreateWithPriority</span>(&amp;st_high, cudaStreamNonBlocking, priority_high);<br><span class="hljs-built_in">cudaStreamCreateWithPriority</span>(&amp;st_low, cudaStreamNonBlocking, priority_low);<br></code></pre></td></tr></table></figure><h3 id="Graphs">Graphs</h3><p>传统流模型每次内核启动或内存拷贝都需要CPU驱动执行设置工作（如参数验证、GPU命令生成），对于短时内核，开销占比显著，而且无法让CUDA看到完整任务流，难以全局优化。</p><p>对此Graphs将整个工作流（包括操作和依赖）预先定义为<strong>图结构</strong>，后续可重复执行，也方便了实例化阶段完成大部分初始化，减少运行时开销。</p><p>以下是创建一个图的简单流程：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// Create the graph - it starts out empty</span><br><span class="hljs-built_in">cudaGraphCreate</span>(&amp;graph, <span class="hljs-number">0</span>);<br><span class="hljs-comment">// For the purpose of this example, we&#x27;ll create</span><br><span class="hljs-comment">// the nodes separately from the dependencies to</span><br><span class="hljs-comment">// demonstrate that it can be done in two stages.</span><br><span class="hljs-comment">// Note that dependencies can also be specified</span><br><span class="hljs-comment">// at node creation.</span><br><span class="hljs-built_in">cudaGraphAddKernelNode</span>(&amp;a, graph, <span class="hljs-literal">NULL</span>, <span class="hljs-number">0</span>, &amp;nodeParams);<br><span class="hljs-built_in">cudaGraphAddKernelNode</span>(&amp;b, graph, <span class="hljs-literal">NULL</span>, <span class="hljs-number">0</span>, &amp;nodeParams);<br><span class="hljs-built_in">cudaGraphAddKernelNode</span>(&amp;c, graph, <span class="hljs-literal">NULL</span>, <span class="hljs-number">0</span>, &amp;nodeParams);<br><span class="hljs-built_in">cudaGraphAddKernelNode</span>(&amp;d, graph, <span class="hljs-literal">NULL</span>, <span class="hljs-number">0</span>, &amp;nodeParams);<br><span class="hljs-comment">// Now set up dependencies on each node</span><br><span class="hljs-built_in">cudaGraphAddDependencies</span>(graph, &amp;a, &amp;b, <span class="hljs-number">1</span>); <span class="hljs-comment">// A-&gt;B</span><br><span class="hljs-built_in">cudaGraphAddDependencies</span>(graph, &amp;a, &amp;c, <span class="hljs-number">1</span>); <span class="hljs-comment">// A-&gt;C</span><br><span class="hljs-built_in">cudaGraphAddDependencies</span>(graph, &amp;b, &amp;d, <span class="hljs-number">1</span>); <span class="hljs-comment">// B-&gt;D</span><br><span class="hljs-built_in">cudaGraphAddDependencies</span>(graph, &amp;c, &amp;d, <span class="hljs-number">1</span>); <span class="hljs-comment">// C-&gt;D</span><br></code></pre></td></tr></table></figure><p><img src="Graph.png" alt="Graph" /></p><p>也可以同步捕获流来创建一个图：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaGraph_t graph;<br><br><span class="hljs-built_in">cudaStreamBeginCapture</span>(stream); <span class="hljs-comment">// 进入捕获模式,后续放入流的操作不会立即执行，而是被记录到内部图中</span><br>kernel_A&lt;&lt;&lt; ..., stream &gt;&gt;&gt;(...); <span class="hljs-comment">//被捕获为图节点</span><br>kernel_B&lt;&lt;&lt; ..., stream &gt;&gt;&gt;(...);<br><span class="hljs-built_in">libraryCall</span>(stream); <span class="hljs-comment">//// 库函数调用（需支持捕获）</span><br>kernel_C&lt;&lt;&lt; ..., stream &gt;&gt;&gt;(...);<br><span class="hljs-built_in">cudaStreamEndCapture</span>(stream, &amp;graph); <span class="hljs-comment">//// 返回构建的图graph</span><br></code></pre></td></tr></table></figure><p>支持普通流和每线程流（<code>cudaStreamPerThread</code>），<strong>不支持</strong>传统NULL流（<code>cudaStreamLegacy</code>），可以通过下述代码查询：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-built_in">cudaStreamIsCapturing</span>(stream, &amp;isCapturing);  <span class="hljs-comment">// 检查流是否处于捕获模式</span><br></code></pre></td></tr></table></figure><p>上图的依赖关系，我们可以通过引入事件来实现跨流的依赖：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// stream1 is the origin stream</span><br><span class="hljs-built_in">cudaStreamBeginCapture</span>(stream1);<br>kernel_A&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);<br><span class="hljs-comment">// Fork into stream2</span><br><span class="hljs-built_in">cudaEventRecord</span>(event1, stream1);<br><span class="hljs-built_in">cudaStreamWaitEvent</span>(stream2, event1);<br>kernel_B&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);<br>kernel_C&lt;&lt;&lt; ..., stream2 &gt;&gt;&gt;(...);<br><span class="hljs-comment">// Join stream2 back to origin stream (stream1)</span><br><span class="hljs-built_in">cudaEventRecord</span>(event2, stream2);<br><span class="hljs-built_in">cudaStreamWaitEvent</span>(stream1, event2);<br>kernel_D&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);<br><span class="hljs-comment">// End capture in the origin stream</span><br><span class="hljs-built_in">cudaStreamEndCapture</span>(stream1, &amp;graph);<br><span class="hljs-comment">// stream1 and stream2 no longer in capture mode</span><br></code></pre></td></tr></table></figure><p>可以这样理解上述的代码：首先创建了stream1这条流，从kernel_A 触发，生成对的event1事件，创建第二条流等待event1事件，也就是kernel_A的创建完成，那么stream2也自然建立了和kernel_A的关系，kernel_B延续stream1，kernel_C延续stream2, 之后建立第二个事件 也就是stream2（kernel_C的是否完成），同时stream1流等待event2，当stream1流和stream2流都通过时，也就是kernel_B和kernel_C都完成时，kernel_C延续stream1，从而实现了图示的效果。</p><p>注意下面的代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// stream1 is the origin stream</span><br><span class="hljs-built_in">cudaStreamBeginCapture</span>(stream1);<br>kernel_A&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);<br><br><span class="hljs-comment">// Fork into stream2</span><br><span class="hljs-built_in">cudaEventRecord</span>(event1, stream1);<br><span class="hljs-built_in">cudaStreamWaitEvent</span>(stream2, event1);<br>kernel_B&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);<br>kernel_C&lt;&lt;&lt; ..., stream2 &gt;&gt;&gt;(...);<br><br><span class="hljs-comment">// 错误依赖关系</span><br><span class="hljs-built_in">cudaEventRecord</span>(event2, stream1);<br><span class="hljs-built_in">cudaStreamWaitEvent</span>(stream2, event2);<br><br><span class="hljs-comment">//这里编程stream2 无影响</span><br>kernel_D&lt;&lt;&lt; ..., stream2 &gt;&gt;&gt;(...);<br><br><span class="hljs-comment">// End capture in the origin stream</span><br><span class="hljs-built_in">cudaStreamEndCapture</span>(stream1, &amp;graph);<br><span class="hljs-comment">// stream1 and stream2 no longer in capture mode</span><br></code></pre></td></tr></table></figure><p>由于先创建了stream1，所以stream1最大可能最先执行完，所以stream1需要等待stream2的完成，而不是stream2等待stream1，而最后的kernel_D无论基于哪个流，二者都已经同步完成，所以不影响。</p><p>需要注意的是：<code>cudaStreamBeginCapture</code>哪个流开始，<code>cudaStreamEndCapture</code>必须结束对应的流。</p><p>捕获模式下的流和事件仅是<strong>图构建的临时抽象</strong>，不代表实际GPU任务队列。尝试同步/查询会导致：</p><ul><li>逻辑矛盾（无法查询未提交执行的任务状态）</li><li>潜在死锁（捕获未完成却要求同步）</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaStreamBeginCapture</span>(stream);<br>kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;();<br><span class="hljs-built_in">cudaStreamSynchronize</span>(stream); <span class="hljs-comment">// 错误！流在捕获模式</span><br></code></pre></td></tr></table></figure><p>跨图合并也是禁止的：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 图1</span><br><span class="hljs-built_in">cudaStreamBeginCapture</span>(stream1);<br><span class="hljs-built_in">cudaEventRecord</span>(event1, stream1);<br><span class="hljs-built_in">cudaStreamEndCapture</span>(stream1, &amp;graph1);<br><br><span class="hljs-comment">// 图2（错误尝试合并）</span><br><span class="hljs-built_in">cudaStreamBeginCapture</span>(stream2);<br><span class="hljs-built_in">cudaStreamWaitEvent</span>(stream2, event1); <span class="hljs-comment">// 错误！event1属于不同图</span><br></code></pre></td></tr></table></figure><p>可以按照如下流程处理失效的情况：</p><p><img src="%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png" alt="处理流程" /></p><p>最后需要注意的是，图定义对象（<code>cudaGraph_t</code>）不能被多线程同时访问（包括创建/修改/销毁）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaGraph_t graph;<br><br><span class="hljs-comment">// 线程1</span><br><span class="hljs-built_in">cudaGraphAddKernelNode</span>(&amp;node1, graph, ...);<br><br><span class="hljs-comment">// 线程2（同时操作同一graph）</span><br><span class="hljs-built_in">cudaGraphAddMemcpyNode</span>(&amp;node2, graph, ...); <span class="hljs-comment">// 危险！</span><br></code></pre></td></tr></table></figure><p>这时候需要使用线程互斥锁保护图对象操作：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++">std::mutex graph_mutex;<br>&#123;<br>    <span class="hljs-function">std::lock_guard&lt;std::mutex&gt; <span class="hljs-title">lock</span><span class="hljs-params">(graph_mutex)</span></span>;<br>    <span class="hljs-built_in">cudaGraphAddKernelNode</span>(&amp;node, graph, ...);<br>&#125;<br></code></pre></td></tr></table></figure><p>可执行图实例（<code>cudaGraphExec_t</code>）不能并发执行同一实例,连续启动同一可执行图时，CUDA保证</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++">Launch <span class="hljs-number">1</span> → Launch <span class="hljs-number">2</span> → Launch <span class="hljs-number">3</span> （严格顺序）<br></code></pre></td></tr></table></figure><p>即使使用不同流，也无法实现同一图实例的并行执行：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaGraphLaunch</span>(execGraph, stream1);  <span class="hljs-comment">// 第一次启动</span><br><span class="hljs-built_in">cudaGraphLaunch</span>(execGraph, stream2);  <span class="hljs-comment">// 阻塞直到第一次完成</span><br></code></pre></td></tr></table></figure><p>可以采用如下的方式实现多线程共享图：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 主线程构建图</span><br>cudaGraph_t graph;<br><span class="hljs-built_in">buildGraph</span>(&amp;graph);<br><br><span class="hljs-comment">// 子线程使用（只读）</span><br>cudaGraphExec_t localExec;<br><span class="hljs-built_in">cudaGraphInstantiate</span>(&amp;localExec, graph, ...); <span class="hljs-comment">// 每个线程独立实例化</span><br><span class="hljs-built_in">cudaGraphLaunch</span>(localExec, localStream);<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaGraphExec_t execGraph1, execGraph2;<br><span class="hljs-built_in">cudaGraphInstantiate</span>(&amp;execGraph1, graph, ...);<br><span class="hljs-built_in">cudaGraphInstantiate</span>(&amp;execGraph2, graph, ...); <span class="hljs-comment">// 创建副本</span><br><br><span class="hljs-comment">// 真正并行执行</span><br><span class="hljs-built_in">cudaGraphLaunch</span>(execGraph1, stream1);<br><span class="hljs-built_in">cudaGraphLaunch</span>(execGraph2, stream2);<br></code></pre></td></tr></table></figure><p>CUDA 事件(Event)是 CUDA 编程中用于监控设备执行进度和精确计时的机制。它们允许应用程序在程序中的任意点异步记录事件，并查询这些事件的完成状态。</p><ul><li>事件完成意味着所有在该事件之前提交的任务（或指定流中的所有命令）已完成</li><li>流0(默认流)中的事件在所有流中所有前置任务和命令完成后才标记为完成</li><li>主要用于：计时操作、同步点、性能分析</li></ul><p><code>cudaEventCreate</code> 函数创建一个新的事件对象，可以通过 <code>cudaEvent_t</code> 类型的变量引用:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaEvent_t start, stop;<br><span class="hljs-built_in">cudaEventCreate</span>(&amp;start);<br><span class="hljs-built_in">cudaEventCreate</span>(&amp;stop);<br></code></pre></td></tr></table></figure><p>当不再需要事件时，应该销毁它以释放相关资源:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaEventDestroy</span>(start);<br><span class="hljs-built_in">cudaEventDestroy</span>(stop);<br></code></pre></td></tr></table></figure><p>事件最常用的场景是测量代码段的执行时间，特别是异步操作的执行时间:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 记录开始事件</span><br><span class="hljs-built_in">cudaEventRecord</span>(start, <span class="hljs-number">0</span>);  <span class="hljs-comment">// 0表示默认流</span><br><br><span class="hljs-comment">// 执行要计时的代码</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">2</span>; ++i) &#123;<br>    <span class="hljs-built_in">cudaMemcpyAsync</span>(inputDev + i * size, inputHost + i * size,<br>                   size, cudaMemcpyHostToDevice, stream[i]);<br>    MyKernel&lt;&lt;&lt;<span class="hljs-number">100</span>, <span class="hljs-number">512</span>, <span class="hljs-number">0</span>, stream[i]&gt;&gt;&gt;<br>              (outputDev + i * size, inputDev + i * size, size);<br>    <span class="hljs-built_in">cudaMemcpyAsync</span>(outputHost + i * size, outputDev + i * size,<br>                   size, cudaMemcpyDeviceToHost, stream[i]);<br>&#125;<br><br><span class="hljs-comment">// 记录结束事件</span><br><span class="hljs-built_in">cudaEventRecord</span>(stop, <span class="hljs-number">0</span>);<br><br><span class="hljs-comment">// 等待事件完成</span><br><span class="hljs-built_in">cudaEventSynchronize</span>(stop);<br><br><span class="hljs-comment">// 计算时间差</span><br><span class="hljs-type">float</span> elapsedTime;<br><span class="hljs-built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);<br></code></pre></td></tr></table></figure><h3 id="Multi-Device-System（多设备系统）">Multi-Device System（多设备系统）</h3><p>在具有多个GPU的系统中，CUDA提供了设备枚举功能来识别和查询可用设备：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-type">int</span> deviceCount;<br><span class="hljs-built_in">cudaGetDeviceCount</span>(&amp;deviceCount);  <span class="hljs-comment">// 获取系统中CUDA设备数量</span><br><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> device = <span class="hljs-number">0</span>; device &lt; deviceCount; ++device) &#123;<br>    cudaDeviceProp deviceProp;<br>    <span class="hljs-built_in">cudaGetDeviceProperties</span>(&amp;deviceProp, device);  <span class="hljs-comment">// 获取设备属性</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Device %d: %s\n&quot;</span>, device, deviceProp.name);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;  Compute Capability: %d.%d\n&quot;</span>, <br>           deviceProp.major, deviceProp.minor);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;  Total Global Memory: %.2f GB\n&quot;</span>, <br>           deviceProp.totalGlobalMem/<span class="hljs-number">1024.0</span>/<span class="hljs-number">1024.0</span>/<span class="hljs-number">1024.0</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>每个主机线程有&quot;当前设备&quot;的概念，内存分配和内核启动都在当前设备上执行，默认当前设备是设备0：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">//使用cudaSetDevice切换设备</span><br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">0</span>);  <span class="hljs-comment">// 切换到设备0</span><br><span class="hljs-type">float</span>* d_data0;<br><span class="hljs-built_in">cudaMalloc</span>(&amp;d_data0, size);  <span class="hljs-comment">// 在设备0上分配内存</span><br><br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">1</span>);  <span class="hljs-comment">// 切换到设备1</span><br><span class="hljs-type">float</span>* d_data1;<br><span class="hljs-built_in">cudaMalloc</span>(&amp;d_data1, size);  <span class="hljs-comment">// 在设备1上分配内存</span><br></code></pre></td></tr></table></figure><p>涉及到对于流的处理时，如果内核启动是向未与当前设备关联的流发出的，则内核启动将失败：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">0</span>); <span class="hljs-comment">// Set device 0 as current</span><br>cudaStream_t s0;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;s0); <span class="hljs-comment">// Create stream s0 on device 0</span><br>MyKernel&lt;&lt;&lt;<span class="hljs-number">100</span>, <span class="hljs-number">64</span>, <span class="hljs-number">0</span>, s0&gt;&gt;&gt;(); <span class="hljs-comment">// Launch kernel on device 0 in s0</span><br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">1</span>); <span class="hljs-comment">// Set device 1 as current</span><br>cudaStream_t s1;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;s1); <span class="hljs-comment">// Create stream s1 on device 1</span><br>MyKernel&lt;&lt;&lt;<span class="hljs-number">100</span>, <span class="hljs-number">64</span>, <span class="hljs-number">0</span>, s1&gt;&gt;&gt;(); <span class="hljs-comment">// Launch kernel on device 1 in s1</span><br><span class="hljs-comment">// This kernel launch will fail:</span><br>MyKernel&lt;&lt;&lt;<span class="hljs-number">100</span>, <span class="hljs-number">64</span>, <span class="hljs-number">0</span>, s0&gt;&gt;&gt;(); <span class="hljs-comment">// Launch kernel on device 1 in s0</span><br></code></pre></td></tr></table></figure><p>对于跨设备操作，遵循以下的规则：</p><table><thead><tr><th style="text-align:left">操作</th><th style="text-align:left">跨设备行为</th></tr></thead><tbody><tr><td style="text-align:left">cudaEventRecord</td><td style="text-align:left">事件和流必须在同一设备</td></tr><tr><td style="text-align:left">cudaEventElapsedTime</td><td style="text-align:left">两个事件必须在同一设备</td></tr><tr><td style="text-align:left">cudaEventSynchronize/Query</td><td style="text-align:left">支持跨设备操作</td></tr><tr><td style="text-align:left">cudaStreamWaitEvent</td><td style="text-align:left">支持跨设备同步</td></tr></tbody></table><p>对于支持PCIe拓扑或NVLink连接的设备之间可以进行点对点访问：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-type">int</span> canAccessPeer;<br><span class="hljs-built_in">cudaDeviceCanAccessPeer</span>(&amp;canAccessPeer, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>);  <span class="hljs-comment">// 检查设备0能否访问设备1</span><br><br><span class="hljs-keyword">if</span> (canAccessPeer) &#123;<br>    <span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">0</span>);<br>    <span class="hljs-built_in">cudaDeviceEnablePeerAccess</span>(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>);  <span class="hljs-comment">// 设备0启用对设备1的访问</span><br>    <br>    <span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">1</span>);<br>    <span class="hljs-built_in">cudaDeviceEnablePeerAccess</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);  <span class="hljs-comment">// 设备1启用对设备0的访问</span><br>&#125;<br></code></pre></td></tr></table></figure><p>P2P内存访问可直接访问对等设备内存，避免通过主机内存中转，使得指针在不同设备间保持有效性。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 设备0上分配内存</span><br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">0</span>);<br><span class="hljs-type">float</span>* p0;<br><span class="hljs-built_in">cudaMalloc</span>(&amp;p0, size);<br><br><span class="hljs-comment">// 设备1上直接使用设备0的内存</span><br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">1</span>);<br>MyKernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(p0);  <span class="hljs-comment">// 直接访问设备0的内存</span><br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 传统方法(通过主机中转)</span><br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">0</span>);<br><span class="hljs-built_in">cudaMemcpy</span>(hostPtr, dev0Ptr, size, cudaMemcpyDeviceToHost);<br><span class="hljs-built_in">cudaSetDevice</span>(<span class="hljs-number">1</span>);<br><span class="hljs-built_in">cudaMemcpy</span>(dev1Ptr, hostPtr, size, cudaMemcpyHostToDevice);<br><br><span class="hljs-comment">// P2P直接拷贝</span><br><span class="hljs-built_in">cudaMemcpyPeer</span>(dev1Ptr, <span class="hljs-number">1</span>, dev0Ptr, <span class="hljs-number">0</span>, size);  <span class="hljs-comment">// 设备0到设备1的直接拷贝</span><br></code></pre></td></tr></table></figure><p>异步拷贝方式如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaMemcpyPeerAsync</span>(dest, destDevice, src, srcDevice, size, stream);<br></code></pre></td></tr></table></figure><p>在Linux系统上应禁用IOMMU以获得最佳P2P性能，IOMMU（Input-Output Memory Management Unit）是一种硬件功能，它类似于CPU中的MMU（内存管理单元），但专门为I/O设备设计，用于管理设备对系统内存的访问。</p><table><thead><tr><th style="text-align:left">特性</th><th style="text-align:left">MMU</th><th style="text-align:left">IOMMU</th></tr></thead><tbody><tr><td style="text-align:left">服务对象</td><td style="text-align:left">CPU</td><td style="text-align:left">I/O设备</td></tr><tr><td style="text-align:left">主要功能</td><td style="text-align:left">虚拟地址→物理地址转换</td><td style="text-align:left">设备地址→物理地址转换</td></tr><tr><td style="text-align:left">保护目标</td><td style="text-align:left">进程间内存隔离</td><td style="text-align:left">设备间内存隔离</td></tr></tbody></table></li></ol><p>统一地址空间是CUDA架构中一项重要特性，它从根本上简化了多设备系统中的内存管理，在64位进程中，CUDA创建一个跨越<strong>主机内存</strong>和<strong>所有计算能力2.0+设备内存</strong>的单一虚拟地址空间：</p><ul><li>每个内存地址在系统中具有唯一性</li><li>指针值本身包含位置信息（主机/设备）</li><li>地址范围：通常为40位或48位虚拟地址空间（取决于GPU架构）</li></ul><p>利用GPU同一地址空间可以实现：</p><p>**指针属性的查询：**通过<code>cudaPointerGetAttributes()</code>可确定指针的实际位置</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaPointerAttributes attributes;<br><span class="hljs-built_in">cudaPointerGetAttributes</span>(&amp;attributes, ptr);<br><br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Memory type: %s\n&quot;</span>, <br>       attributes.type == cudaMemoryTypeHost ? <span class="hljs-string">&quot;Host&quot;</span> :<br>       attributes.type == cudaMemoryTypeDevice ? <span class="hljs-string">&quot;Device&quot;</span> : <span class="hljs-string">&quot;Managed&quot;</span>);<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Device ID: %d\n&quot;</span>, attributes.device);<br></code></pre></td></tr></table></figure><p>**智能内存的拷贝：**使用<code>cudaMemcpyDefault</code>自动判断拷贝方</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 自动识别src和dst位置（主机↔设备/设备↔设备）</span><br><span class="hljs-built_in">cudaMemcpy</span>(dst, src, size, cudaMemcpyDefault);<br></code></pre></td></tr></table></figure><p>便携式内存分配<code>cudaHostAlloc</code>分配的内存自动支持</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">float</span> *h_data;<br><span class="hljs-built_in">cudaHostAlloc</span>(&amp;h_data, size, cudaHostAllocDefault);<br><br><span class="hljs-comment">// 所有支持统一地址空间的设备均可直接访问</span><br>MyKernel&lt;&lt;&lt;...&gt;&gt;&gt;(h_data);  <span class="hljs-comment">// 无需获取设备指针</span><br></code></pre></td></tr></table></figure><p>和Linux的进程间通信一样，CUDA IPC（Inter-Process Communication）是一组允许不同进程间共享GPU内存和事件的高级功能，对于同一进程的所有线程天然共享设备指针和事件句柄，不同进程的地址空间相互独立，需显式IPC机制。</p><p>需要注意的是，<code>cudaMallocManaged</code>分配的内存不支持IPC，通信进程必须使用相同版本的CUDA驱动和运行时。</p><p>可以通过<strong>内存共享</strong>和事件共享来实现IPC通信：</p><p>对于<strong>内存共享</strong>，在GPU页表中建立跨进程映射，使用唯一的IPC句柄替代裸指针，同时采用引用计数，防止过早释放共享内存。</p><p>发送方进程执行：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">float</span>* d_data;<br><span class="hljs-built_in">cudaMalloc</span>(&amp;d_data, size);  <span class="hljs-comment">// 分配设备内存</span><br><br>cudaIpcMemHandle_t handle;<br><span class="hljs-built_in">cudaIpcGetMemHandle</span>(&amp;handle, d_data);  <span class="hljs-comment">// 获取IPC句柄</span><br><br><span class="hljs-comment">// 通过OS机制传递handle（如共享内存、管道、文件等）</span><br><span class="hljs-built_in">write_to_ipc_channel</span>(&amp;handle, <span class="hljs-built_in">sizeof</span>(handle));<br></code></pre></td></tr></table></figure><p>接收方进程执行：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaIpcMemHandle_t handle;<br><span class="hljs-built_in">read_from_ipc_channel</span>(&amp;handle, <span class="hljs-built_in">sizeof</span>(handle));  <span class="hljs-comment">// 获取句柄</span><br><br><span class="hljs-type">float</span>* foreign_d_data;<br><span class="hljs-built_in">cudaIpcOpenMemHandle</span>(&amp;foreign_d_data, handle, <br>                    cudaIpcMemLazyEnablePeerAccess);  <span class="hljs-comment">// 打开句柄</span><br><br><span class="hljs-comment">// 现在foreign_d_data可当作本地设备指针使用</span><br></code></pre></td></tr></table></figure><p>可以指定Handle的打开模式：</p><table><thead><tr><th style="text-align:left">模式标志</th><th style="text-align:left">说明</th></tr></thead><tbody><tr><td style="text-align:left"><code>cudaIpcMemLazyEnablePeerAccess</code></td><td style="text-align:left">按需自动建立P2P访问</td></tr><tr><td style="text-align:left"><code>cudaIpcMemLazyDisablePeerAccess</code></td><td style="text-align:left">禁用P2P访问</td></tr></tbody></table><p>对于<strong>事件共享</strong>，在设备层面创建跨进程可见对象，通过GPU硬件信号实现跨进程同步。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 进程A创建事件并共享</span><br>cudaEvent_t event;<br><span class="hljs-built_in">cudaEventCreate</span>(&amp;event, cudaEventInterprocess);<br>cudaIpcEventHandle_t event_handle;<br><span class="hljs-built_in">cudaIpcGetEventHandle</span>(&amp;event_handle, event);<br><br><span class="hljs-comment">// 进程B打开事件</span><br>cudaEvent_t foreign_event;<br><span class="hljs-built_in">cudaIpcOpenEventHandle</span>(&amp;foreign_event, event_handle);<br></code></pre></td></tr></table></figure><p>必须使用<code>cudaEventInterprocess</code>标志创建事件，此时的共享事件只能用于同步，而不能用于计时。</p><h3 id="CUDA-Error-Checking">CUDA Error Checking</h3><p>所有CUDA Runtime函数运行失败都会返回错误码，对于异步函数，因为该函数在设备完成任务之前返回，错误代码仅仅报告在运行任务之前host上发生的错误，不可能报告device上任何异步错误，需要后续进行同步操作检测。</p><p>对于同步函数：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaError_t err = <span class="hljs-built_in">cudaMemcpy</span>(dst, src, size, cudaMemcpyHostToDevice);<br><span class="hljs-comment">// 错误立即可知，包含设备执行错误</span><br></code></pre></td></tr></table></figure><p>对于异步函数：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++">MyKernel&lt;&lt;&lt;...&gt;&gt;&gt;(); <span class="hljs-comment">// 启动不返回错误</span><br><span class="hljs-comment">// 仅后续同步才能发现内核执行错误</span><br></code></pre></td></tr></table></figure><p>CUDA Errorshiyong如下API进行检测：</p><table><thead><tr><th style="text-align:left">函数</th><th style="text-align:left">返回值</th><th style="text-align:left">副作用</th><th style="text-align:left">适用场景</th></tr></thead><tbody><tr><td style="text-align:left"><code>cudaPeekAtLastError()</code></td><td style="text-align:left">当前错误码</td><td style="text-align:left">无</td><td style="text-align:left">检查错误但不破坏错误上下文</td></tr><tr><td style="text-align:left"><code>cudaGetLastError()</code></td><td style="text-align:left">当前错误码</td><td style="text-align:left">重置为cudaSuccess</td><td style="text-align:left">标准错误处理流程</td></tr></tbody></table><p>对于标准的内核错误，参照如下方式进行：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 清除之前可能存在的错误</span><br><span class="hljs-built_in">cudaGetLastError</span>();<br><br><span class="hljs-comment">// 启动内核</span><br>MyKernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(...);<br><br><span class="hljs-comment">// 检查启动参数错误</span><br>cudaError_t launchErr = <span class="hljs-built_in">cudaPeekAtLastError</span>();<br><span class="hljs-keyword">if</span> (launchErr != cudaSuccess) &#123;<br>    <span class="hljs-built_in">handleLaunchError</span>(launchErr);<br>&#125;<br><br><span class="hljs-comment">// 同步并检查执行错误</span><br>cudaError_t syncErr = <span class="hljs-built_in">cudaDeviceSynchronize</span>();<br><span class="hljs-keyword">if</span> (syncErr != cudaSuccess) &#123;<br>    <span class="hljs-built_in">handleRuntimeError</span>(syncErr);<br>&#125;<br></code></pre></td></tr></table></figure><p>对于流式错误检测：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs C++">cudaStream_t stream;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream);<br><br><span class="hljs-comment">// 启动多个异步操作</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; N; ++i) &#123;<br>    <span class="hljs-built_in">cudaGetLastError</span>(); <span class="hljs-comment">// 清除错误状态</span><br>    Kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(...);<br>    cudaError_t err = <span class="hljs-built_in">cudaStreamQuery</span>(stream);<br>    <span class="hljs-keyword">if</span> (err == cudaErrorNotReady) &#123;<br>        <span class="hljs-comment">// 正常情况，操作未完成</span><br>    &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (err != cudaSuccess) &#123;<br>        <span class="hljs-comment">// 真实错误处理</span><br>    &#125;<br>&#125;<br><br><span class="hljs-comment">// 最终错误检查</span><br>cudaError_t finalErr = <span class="hljs-built_in">cudaStreamSynchronize</span>(stream);<br></code></pre></td></tr></table></figure><p>如何处理错误，在CUDA 8+中定义了错误回调的机制：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> __host__ <span class="hljs-title">errorCallback</span><span class="hljs-params">(cudaStream_t stream, cudaError_t status, <span class="hljs-type">void</span>* userData)</span> </span>&#123;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Async error detected: %s\n&quot;</span>, <span class="hljs-built_in">cudaGetErrorString</span>(status));<br>&#125;<br><br><span class="hljs-comment">// 注册回调</span><br><span class="hljs-built_in">cudaStreamAddCallback</span>(stream, errorCallback, <span class="hljs-literal">nullptr</span>, <span class="hljs-number">0</span>);<br></code></pre></td></tr></table></figure><h3 id="Texture-and-Surface-Memory">Texture and Surface Memory</h3><p>纹理内存（texture memory）和表面内存（surface memory）类似于常量内存，是有缓存的全局变量，可见范围和生命周期一样，一般仅可读（表面内存可写），但内存容量更大。</p><h4 id="Texture-Memory">Texture Memory</h4><p>在旧式纹理API中使用如下方式静态引用纹理：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 声明纹理引用（必须在文件作用域）</span><br>texture&lt;<span class="hljs-type">float</span>, <span class="hljs-number">2</span>, cudaReadModeElementType&gt; texRef;<br><br><span class="hljs-comment">// 运行时绑定</span><br>cudaChannelFormatDesc desc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;<span class="hljs-type">float</span>&gt;();<br><span class="hljs-built_in">cudaBindTexture2D</span>(<span class="hljs-number">0</span>, texRef, devPtr, desc, width, height, pitch);<br><br><span class="hljs-comment">// 内核中使用</span><br><span class="hljs-type">float</span> val = <span class="hljs-built_in">tex2D</span>(texRef, x, y);<br></code></pre></td></tr></table></figure><p>CUDA3.0+可以在运行时动态创建纹理：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaTextureDesc</span><br>&#123;<br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureAddressMode</span> addressMode[<span class="hljs-number">3</span>];<br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureFilterMode</span> filterMode;<br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureReadMode</span> readMode;<br>    <span class="hljs-type">int</span> sRGB;<br>    <span class="hljs-type">int</span> normalizedCoords;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> maxAnisotropy;<br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureFilterMode</span> mipmapFilterMode;<br>    <span class="hljs-type">float</span> mipmapLevelBias;<br>    <span class="hljs-type">float</span> minMipmapLevelClamp;<br>    <span class="hljs-type">float</span> maxMipmapLevelClamp;<br>&#125;;<br><br><br><br><span class="hljs-comment">// 创建纹理对象</span><br>cudaResourceDesc resDesc;<br><span class="hljs-built_in">memset</span>(&amp;resDesc, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(resDesc));<br>resDesc.resType = cudaResourceTypeLinear;<br>resDesc.res.linear.devPtr = devPtr;<br>resDesc.res.linear.desc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;<span class="hljs-type">float</span>&gt;();<br>resDesc.res.linear.sizeInBytes = width * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br><br>cudaTextureDesc texDesc;<br><span class="hljs-built_in">memset</span>(&amp;texDesc, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(texDesc));<br>texDesc.addressMode[<span class="hljs-number">0</span>] = cudaAddressModeClamp;<br>texDesc.filterMode = cudaFilterModeLinear;<br>texDesc.readMode = cudaReadModeElementType;<br><br>cudaTextureObject_t texObj;<br><span class="hljs-built_in">cudaCreateTextureObject</span>(&amp;texObj, &amp;resDesc, &amp;texDesc, <span class="hljs-literal">NULL</span>);<br><br><span class="hljs-comment">// 内核中使用</span><br><span class="hljs-type">float</span> val = <span class="hljs-built_in">tex1Dfetch</span>&lt;<span class="hljs-type">float</span>&gt;(texObj, x);<br></code></pre></td></tr></table></figure><p>下面一段代码展示了利用纹理对象实现图像旋转的操作：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;cuda_runtime.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;device_launch_parameters.h&quot;</span></span><br><br><span class="hljs-comment">// rotate kernel</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">transformKernel</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, cudaTextureObject_t textObj, <span class="hljs-type">int</span> width, <span class="hljs-type">int</span> height, <span class="hljs-type">float</span> theta)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-comment">//归一化 纹理采样前的标准预处理步骤</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;<br><br>    <span class="hljs-type">float</span> u = x / (<span class="hljs-type">float</span>)width;<br>    <span class="hljs-type">float</span> v = y / (<span class="hljs-type">float</span>)height;<br><br>    <span class="hljs-comment">//旋转中心</span><br>    u -= <span class="hljs-number">0.5f</span>;<br>    v -= <span class="hljs-number">0.5f</span>;<br>    <span class="hljs-type">float</span> tu = u * <span class="hljs-built_in">cosf</span>(theta) - v * <span class="hljs-built_in">sinf</span>(theta) + <span class="hljs-number">0.5f</span>;<br>    <span class="hljs-type">float</span> tv = v * <span class="hljs-built_in">cosf</span>(theta) - u * <span class="hljs-built_in">sinf</span>(theta) + <span class="hljs-number">0.5f</span>;<br><br>    <span class="hljs-comment">//read from texture to global memory</span><br>    <span class="hljs-comment">//tex2D是纹理采样函数</span><br>    output[y * width + x] = <span class="hljs-built_in">tex2D</span>&lt;<span class="hljs-type">float</span>&gt;(textObj, tu, tv);<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">float</span>* h_data;<br>    <span class="hljs-type">int</span> width, height;<br>    <span class="hljs-type">float</span> angle;<br>    <span class="hljs-type">size_t</span> size = width * height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br><br>    <span class="hljs-comment">//创建32位单通道浮点格式的CUDA数组</span><br>    <span class="hljs-comment">//CUDA数组是纹理内存的优化存储格式</span><br>    cudaChannelFormatDesc channelDesc = <br>                <span class="hljs-built_in">cudaCreateChannelDesc</span>(<span class="hljs-number">32</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, cudaChannelFormatKindFloat);<br>    <br>    cudaArray* cuArray;<br>    <span class="hljs-built_in">cudaMallocArray</span>(&amp;cuArray, &amp;channelDesc, width, height);<br><br>    <span class="hljs-comment">//copy host memory to device memory located at address h_data</span><br>    <span class="hljs-built_in">cudaMemcpyToArray</span>(cuArray, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, h_data, size, cudaMemcpyHostToDevice);<br><br>    <span class="hljs-comment">//texture</span><br>    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaResourceDesc</span> resDesc;<br>    <span class="hljs-built_in">memset</span>(&amp;resDesc, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(resDesc));<br>    resDesc.resType = cudaResourceTypeArray;<br>    resDesc.res.array.array = cuArray;<br><br>    <span class="hljs-comment">//texture object parameters</span><br>    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaTextureDesc</span> texDesc;<br>    <span class="hljs-built_in">memset</span>(&amp;texDesc, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(texDesc));<br>    texDesc.addressMode[<span class="hljs-number">0</span>] = cudaAddressModeWrap;<br>    texDesc.addressMode[<span class="hljs-number">1</span>] = cudaAddressModeWrap;<br>    texDesc.filterMode = cudaFilterModeLinear;<br>    texDesc.readMode = cudaReadModeElementType;<br>    texDesc.normalizedCoords = <span class="hljs-number">1</span>;<br><br>    <span class="hljs-comment">//creater texture object</span><br>    cudaTextureObject_t texobj = <span class="hljs-number">0</span>;<br>    <span class="hljs-built_in">cudaCreateTextureObject</span>(&amp;texobj, &amp;resDesc, &amp;texDesc, <span class="hljs-literal">NULL</span>);<br><br>    <span class="hljs-type">float</span>* output;<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;output, width * height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>));<br><br>    <span class="hljs-comment">//Invoke kernel</span><br>    <span class="hljs-function">dim3 <span class="hljs-title">dimBlock</span><span class="hljs-params">(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">dimGrid</span><span class="hljs-params">((width + dimBlock.x - <span class="hljs-number">1</span>) / dimBlock.x,</span></span><br><span class="hljs-params"><span class="hljs-function">                (height + dimBlock.y - <span class="hljs-number">1</span>) / dimBlock.y)</span></span>;<br><br>    transformKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt; (output, texobj, width, height, angle);<br><br>    <span class="hljs-comment">//destory texture obj</span><br>    <span class="hljs-built_in">cudaDestroyTextureObject</span>(texobj);<br><br>    <span class="hljs-comment">//free device memory</span><br>    <span class="hljs-built_in">cudaFreeArray</span>(cuArray);<br>    <span class="hljs-built_in">cudaFree</span>(output);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>纹理引用必须在文件作用域声明为静态全局变量，其基本语法为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++">texture&lt;DataType, Type, ReadMode&gt; texRef;<br></code></pre></td></tr></table></figure><ol><li><strong>DataType</strong>：指定纹素(texel)的数据类型</li><li><strong>Type</strong>（可选，默认为<code>cudaTextureType1D</code>）：指定纹理类型：<ul><li><code>cudaTextureType1D</code>：一维纹理</li><li><code>cudaTextureType2D</code>：二维纹理</li><li><code>cudaTextureType3D</code>：三维纹理</li><li><code>cudaTextureType1DLayered</code>：一维分层纹理</li><li><code>cudaTextureType2DLayered</code>：二维分层纹理</li></ul></li><li><strong>ReadMode</strong>（可选，默认为<code>cudaReadModeElementType</code>）：读取模式</li></ol><p>上述属性在编译时确定，不可更改，运行时可以更改的属性通过<code>textureReference</code>结构体定义：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">textureReference</span> &#123;<br>    <span class="hljs-type">int</span> normalized;  <span class="hljs-comment">// 是否使用归一化坐标</span><br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureFilterMode</span> filterMode;  <span class="hljs-comment">// 滤波模式</span><br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureAddressMode</span> addressMode[<span class="hljs-number">3</span>];  <span class="hljs-comment">// 寻址模式</span><br>    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaChannelFormatDesc</span> channelDesc;  <span class="hljs-comment">// 通道格式描述</span><br>    <span class="hljs-type">int</span> sRGB;  <span class="hljs-comment">// 是否使用sRGB色彩空间</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> maxAnisotropy;  <span class="hljs-comment">// 各向异性过滤的最大值</span><br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaTextureFilterMode</span> mipmapFilterMode;  <span class="hljs-comment">// Mipmap滤波模式</span><br>    <span class="hljs-type">float</span> mipmapLevelBias;  <span class="hljs-comment">// Mipmap级别偏置</span><br>    <span class="hljs-type">float</span> minMipmapLevelClamp;  <span class="hljs-comment">// 最小Mipmap级别钳制</span><br>    <span class="hljs-type">float</span> maxMipmapLevelClamp;  <span class="hljs-comment">// 最大Mipmap级别钳制</span><br>&#125;;<br></code></pre></td></tr></table></figure><p>在使用纹理引用前，必须将其绑定到内存或CUDA数组。</p><p>绑定到线性内存，使用低级API：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++">texture&lt;<span class="hljs-type">float</span>, cudaTextureType2D, cudaReadModeElementType&gt; texRef;<br>textureReference* texRefPtr;<br><span class="hljs-built_in">cudaGetTextureReference</span>(&amp;texRefPtr, &amp;texRef);<br>cudaChannelFormatDesc channelDesc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;<span class="hljs-type">float</span>&gt;();<br><span class="hljs-type">size_t</span> offset;<br><span class="hljs-built_in">cudaBindTexture2D</span>(&amp;offset, texRefPtr, devPtr, &amp;channelDesc, width, height, pitch);<br></code></pre></td></tr></table></figure><p>使用高级API：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++">texture&lt;<span class="hljs-type">float</span>, cudaTextureType2D, cudaReadModeElementType&gt; texRef;<br>cudaChannelFormatDesc channelDesc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;<span class="hljs-type">float</span>&gt;();<br><span class="hljs-type">size_t</span> offset;<br><span class="hljs-built_in">cudaBindTexture2D</span>(&amp;offset, texRef, devPtr, channelDesc, width, height, pitch);<br></code></pre></td></tr></table></figure><p>绑定到CUDA数组，使用低级API：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++">texture&lt;<span class="hljs-type">float</span>, cudaTextureType2D, cudaReadModeElementType&gt; texRef;<br>textureReference* texRefPtr;<br><span class="hljs-built_in">cudaGetTextureReference</span>(&amp;texRefPtr, &amp;texRef);<br>cudaChannelFormatDesc channelDesc;<br><span class="hljs-built_in">cudaGetChannelDesc</span>(&amp;channelDesc, cuArray);<br><span class="hljs-built_in">cudaBindTextureToArray</span>(texRef, cuArray, &amp;channelDesc);<br></code></pre></td></tr></table></figure><p>使用高级API：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++">texture&lt;<span class="hljs-type">float</span>, cudaTextureType2D, cudaReadModeElementType&gt; texRef;<br><span class="hljs-built_in">cudaBindTextureToArray</span>(texRef, cuArray);<br></code></pre></td></tr></table></figure><p>使用如下命令解绑纹理：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaUnbindTexture</span>(texRef);<br></code></pre></td></tr></table></figure><p>对于上述旋转变换逻辑，使用纹理引用的代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 2D float texture</span><br>texture&lt;<span class="hljs-type">float</span>, cudaTextureType2D, cudaReadModeElementType&gt; texRef;<br><br><span class="hljs-comment">// Simple transformation kernel</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">transformKernel</span><span class="hljs-params">(<span class="hljs-type">float</span>* output,</span></span><br><span class="hljs-params"><span class="hljs-function">                               <span class="hljs-type">int</span> width, </span></span><br><span class="hljs-params"><span class="hljs-function">                               <span class="hljs-type">int</span> height,</span></span><br><span class="hljs-params"><span class="hljs-function">                               <span class="hljs-type">float</span> theta)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-comment">// Calculate normalized texture coordinates</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;<br>    <span class="hljs-type">float</span> u = x / (<span class="hljs-type">float</span>)width;<br>    <span class="hljs-type">float</span> v = y / (<span class="hljs-type">float</span>)height;<br><br>    <span class="hljs-comment">// Transform coordinates</span><br>    u -= <span class="hljs-number">0.5f</span>;<br>    v -= <span class="hljs-number">0.5f</span>;<br>    <span class="hljs-type">float</span> tu = u * <span class="hljs-built_in">cosf</span>(theta) - v * <span class="hljs-built_in">sinf</span>(theta) + <span class="hljs-number">0.5f</span>;<br>    <span class="hljs-type">float</span> tv = v * <span class="hljs-built_in">cosf</span>(theta) + u * <span class="hljs-built_in">sinf</span>(theta) + <span class="hljs-number">0.5f</span>;<br><br>    <span class="hljs-comment">// Read from texture and write to global memory</span><br>    output[y * width + x] = <span class="hljs-built_in">tex2D</span>(texRef, tu, tv);<br>&#125;<br><br><span class="hljs-comment">// Host code</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-comment">// Allocate CUDA array in device memory</span><br>    cudaChannelFormatDesc channelDesc =<br>        <span class="hljs-built_in">cudaCreateChannelDesc</span>(<span class="hljs-number">32</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,<br>                             cudaChannelFormatKindFloat);<br>    cudaArray* cuArray;<br>    <span class="hljs-built_in">cudaMallocArray</span>(&amp;cuArray, &amp;channelDesc, width, height);<br><br>    <span class="hljs-comment">// Copy to device memory some data located at address h_data</span><br>    <span class="hljs-comment">// in host memory</span><br>    <span class="hljs-built_in">cudaMemcpyToArray</span>(cuArray, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, h_data, size,<br>                     cudaMemcpyHostToDevice);<br><br>    <span class="hljs-comment">// Set texture reference parameters</span><br>    texRef.addressMode[<span class="hljs-number">0</span>] = cudaAddressModeWrap;<br>    texRef.addressMode[<span class="hljs-number">1</span>] = cudaAddressModeWrap;<br>    texRef.filterMode = cudaFilterModeLinear;<br>    texRef.normalized = <span class="hljs-literal">true</span>;<br><br>    <span class="hljs-comment">// Bind the array to the texture reference</span><br>    <span class="hljs-built_in">cudaBindTextureToArray</span>(texRef, cuArray, channelDesc);<br><br>    <span class="hljs-comment">// Allocate result of transformation in device memory</span><br>    <span class="hljs-type">float</span>* output;<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;output, width * height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>));<br><br>    <span class="hljs-comment">// Invoke kernel</span><br>    <span class="hljs-function">dim3 <span class="hljs-title">dimBlock</span><span class="hljs-params">(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">dimGrid</span><span class="hljs-params">((width + dimBlock.x - <span class="hljs-number">1</span>) / dimBlock.x,</span></span><br><span class="hljs-params"><span class="hljs-function">                (height + dimBlock.y - <span class="hljs-number">1</span>) / dimBlock.y)</span></span>;<br>    transformKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(output, width, height,<br>                                         angle);<br><br>    <span class="hljs-comment">// Free device memory</span><br>    <span class="hljs-built_in">cudaFreeArray</span>(cuArray);<br>    <span class="hljs-built_in">cudaFree</span>(output);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>CUDA C++没有直接的16位浮点数据类型，通过<code>unsigned short</code>类型进行转换，对于设备端的转换函数可以使用如下接口：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">unsigned</span> <span class="hljs-type">short</span> __float2half_rn(<span class="hljs-type">float</span> f);  <span class="hljs-comment">// 32位浮点转16位</span><br><span class="hljs-type">float</span> __half2float(<span class="hljs-type">unsigned</span> <span class="hljs-type">short</span> h);    <span class="hljs-comment">// 16位转32位浮点</span><br></code></pre></td></tr></table></figure><p>对于主机端转换使用OpenEXR库中的等效函数。</p><p>以下是一些高级纹理特性的介绍：</p><p>**分层纹理（Layered Textures）**也被称为纹理数组（Texture Array）由多个相同维度、大小和数据类型的常规纹理层组成。</p><p>一维分层纹理使用一个整数索引(层)和一个浮点坐标(层内位置)寻址；二维分层纹理使用一个整数索引(层)和两个浮点坐标(层内位置)寻址。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaMalloc3DArray</span>(&amp;cuArray, &amp;desc, <span class="hljs-built_in">make_cudaExtent</span>(width, height, <span class="hljs-number">0</span>), cudaArrayLayered);<br><br><span class="hljs-built_in">tex1DLayered</span>(texRef, x, layer);        <span class="hljs-comment">// 一维分层</span><br><span class="hljs-built_in">tex2DLayered</span>(texRef, x, y, layer);     <span class="hljs-comment">// 二维分层</span><br></code></pre></td></tr></table></figure><p>**立方体贴图（Cubemap Textures）**是特殊类型的二维分层纹理，有6层代表立方体的面，每层的宽度等于高度，使用三个坐标(x,y,z)作为方向向量进行寻址。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaMalloc3DArray</span>(&amp;cuArray, &amp;desc, <span class="hljs-built_in">make_cudaExtent</span>(size, size, <span class="hljs-number">0</span>), cudaArrayCubemap);<br><br><span class="hljs-built_in">texCubemap</span>(texRef, x, y, z);<br></code></pre></td></tr></table></figure><p>立方体分层纹理（Cubemap layered Textures）由多个立方体贴图组成的序列,使用整数索引(选择立方体)和三个浮点坐标(立方体内位置)寻址.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaMalloc3DArray</span>(&amp;cuArray, &amp;desc, <span class="hljs-built_in">make_cudaExtent</span>(size, size, layers), <br>                 cudaArrayLayered | cudaArrayCubemap);<br>                 <br><span class="hljs-built_in">texCubemapLayered</span>(texRef, x, y, z, layer);<br></code></pre></td></tr></table></figure><h4 id="Surface-Memory">Surface Memory</h4><p>表面内存(Surface Memory)是CUDA中一种特殊的内存访问机制，主要用于计算能力2.0及以上的设备。它提供了对CUDA数组的直接读写能力，与纹理内存相比，表面内存更注重于随机访问和写入操作。</p><p>表面内存通过表面对象(Surface Object)或表面引用(Surface Reference)访问，支持读写操作（纹理内存通常只读），CUDA数组必须使用<code>cudaArraySurfaceLoadStore</code>标志创建。</p><p>表面对象是使用 cudaCreateSurfaceObject（） 从结构 cudaResourceDesc 类型的资源描述中创建，以下是一个表面对象的创建方式：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function">cudaSurfaceObject_t <span class="hljs-title">createSurfaceObject</span><span class="hljs-params">(cudaArray* array)</span> </span>&#123;<br>    cudaResourceDesc resDesc;<br>    <span class="hljs-built_in">memset</span>(&amp;resDesc, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(resDesc));<br>    resDesc.resType = cudaResourceTypeArray;<br>    resDesc.res.array.array = array;<br>    <br>    cudaSurfaceObject_t surfObj;<br>    <span class="hljs-built_in">cudaCreateSurfaceObject</span>(&amp;surfObj, &amp;resDesc);<br>    <span class="hljs-keyword">return</span> surfObj;<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaResourceDesc</span> &#123;<br>    <span class="hljs-keyword">enum</span> <span class="hljs-title class_">cudaResourceType</span> resType;  <span class="hljs-comment">// 资源类型</span><br>    <br>    <span class="hljs-keyword">union</span> &#123;<br>        <span class="hljs-keyword">struct</span> &#123;<br>            cudaArray_t array;      <span class="hljs-comment">// CUDA数组</span><br>        &#125; array;<br>        <br>        <span class="hljs-keyword">struct</span> &#123;<br>            cudaMipmappedArray_t mipmap;  <span class="hljs-comment">// Mipmap数组</span><br>        &#125; mipmap;<br>        <br>        <span class="hljs-keyword">struct</span> &#123;<br>            <span class="hljs-type">void</span>* devPtr;          <span class="hljs-comment">// 设备指针</span><br>            <span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaChannelFormatDesc</span> desc;  <span class="hljs-comment">// 通道格式</span><br>            <span class="hljs-type">size_t</span> sizeInBytes;     <span class="hljs-comment">// 大小（字节）</span><br>        &#125; linear;<br>        <br>        <span class="hljs-keyword">struct</span> &#123;<br>            <span class="hljs-type">void</span>* devPtr;          <span class="hljs-comment">// 设备指针</span><br>            <span class="hljs-keyword">struct</span> <span class="hljs-title class_">cudaChannelFormatDesc</span> desc;  <span class="hljs-comment">// 通道格式</span><br>            <span class="hljs-type">size_t</span> width;         <span class="hljs-comment">// 宽度</span><br>            <span class="hljs-type">size_t</span> height;        <span class="hljs-comment">// 高度</span><br>            <span class="hljs-type">size_t</span> pitchInBytes;  <span class="hljs-comment">// 间距（字节）</span><br>        &#125; pitch2D;<br>    &#125; res;<br>&#125;;<br></code></pre></td></tr></table></figure><p>表面内存通过一组内置函数进行访问:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">//一维表面</span><br><span class="hljs-built_in">surf1Dread</span>(T* data, cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> x);<br><span class="hljs-built_in">surf1Dwrite</span>(T data, cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> x);<br><br><span class="hljs-comment">//二维表面</span><br><span class="hljs-built_in">surf2Dread</span>(T* data, cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y);<br><span class="hljs-built_in">surf2Dwrite</span>(T data, cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y);<br><br><span class="hljs-comment">//三维表面</span><br><span class="hljs-built_in">surf3Dread</span>(T* data, cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y, <span class="hljs-type">int</span> z);<br><span class="hljs-built_in">surf3Dwrite</span>(T data, cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y, <span class="hljs-type">int</span> z);<br><br><span class="hljs-comment">//表面对象销毁</span><br><span class="hljs-built_in">cudaDestroySurfaceObject</span>(cudaSurfaceObject_t surfObj);<br></code></pre></td></tr></table></figure><p>以下是一个使用示例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 创建CUDA数组</span><br>cudaArray* cuArray;<br>cudaChannelFormatDesc channelDesc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;<span class="hljs-type">float</span>&gt;();<br><span class="hljs-built_in">cudaMallocArray</span>(&amp;cuArray, &amp;channelDesc, width, height, cudaArraySurfaceLoadStore);<br><br><span class="hljs-comment">// 创建表面对象</span><br>cudaSurfaceObject_t surfObj = <span class="hljs-built_in">createSurfaceObject</span>(cuArray);<br><br><span class="hljs-comment">// 内核函数：写入表面</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">writeSurface</span><span class="hljs-params">(cudaSurfaceObject_t surfObj, <span class="hljs-type">int</span> width, <span class="hljs-type">int</span> height)</span> </span>&#123;<br>    <span class="hljs-type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;<br>    <br>    <span class="hljs-keyword">if</span> (x &lt; width &amp;&amp; y &lt; height) &#123;<br>        <span class="hljs-type">float</span> value = x + y * <span class="hljs-number">0.1f</span>;<br>        <span class="hljs-built_in">surf2Dwrite</span>(value, surfObj, x, y);<br>    &#125;<br>&#125;<br><br><span class="hljs-comment">// 调用内核</span><br><span class="hljs-function">dim3 <span class="hljs-title">blocks</span><span class="hljs-params">(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>)</span></span>;<br><span class="hljs-function">dim3 <span class="hljs-title">grids</span><span class="hljs-params">((width + blocks.x - <span class="hljs-number">1</span>) / blocks.x, (height + blocks.y - <span class="hljs-number">1</span>) / blocks.y)</span></span>;<br>writeSurface&lt;&lt;&lt;grids, blocks&gt;&gt;&gt;(surfObj, width, height);<br><br><span class="hljs-comment">// 清理</span><br><span class="hljs-built_in">cudaDestroySurfaceObject</span>(surfObj);<br><span class="hljs-built_in">cudaFreeArray</span>(cuArray);<br></code></pre></td></tr></table></figure><h3 id="图像互作性">图像互作性</h3><p>CUDA与OpenGL以及Direct3D等的互操作性允许GPU资源在图形渲染和通用计算之间共享，避免了数据在CPU内存中的来回拷贝，显著提高了异构计算的效率</p><p>操作流程如下：</p><ol><li><strong>资源注册</strong>：将OpenGL资源注册为CUDA可访问资源</li><li><strong>资源映射</strong>：将注册的资源映射到CUDA地址空间</li><li><strong>CUDA访问</strong>：通过CUDA内核读写资源</li><li><strong>资源解映射</strong>：解除CUDA对资源的访问</li><li><strong>OpenGL使用</strong>：OpenGL使用修改后的资源进行渲染</li><li><strong>资源注销</strong>：程序结束时注销资源</li></ol><p>与OpenGL的互作，关键API函数如下：</p><p>1.资源的注册</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaGraphicsGLRegisterBuffer</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    cudaGraphicsResource** resource,</span></span><br><span class="hljs-params"><span class="hljs-function">    GLuint buffer,</span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> flags)</span></span>;<br></code></pre></td></tr></table></figure><ul><li><code>resource</code>：返回的CUDA图形资源指针</li><li><code>buffer</code>：OpenGL缓冲区对象名称</li><li><code>flags</code>：注册标志，常用值：<ul><li><code>cudaGraphicsRegisterFlagsNone</code>：默认</li><li><code>cudaGraphicsRegisterFlagsReadOnly</code>：只读</li><li><code>cudaGraphicsRegisterFlagsWriteDiscard</code>：只写（丢弃原有内容）</li><li><code>cudaGraphicsRegisterFlagsSurfaceLoadStore</code>：允许表面读写</li></ul></li></ul><p>2.资源的映射与解映射</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-built_in">cudaGraphicsMapResources</span>(<span class="hljs-type">int</span> count, cudaGraphicsResource_t* resources, cudaStream_t stream);<br><span class="hljs-built_in">cudaGraphicsUnmapResources</span>(<span class="hljs-type">int</span> count, cudaGraphicsResource_t* resources, cudaStream_t stream);<br></code></pre></td></tr></table></figure><p>3.获取映射指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaGraphicsResourceGetMappedPointer</span>(<span class="hljs-type">void</span>** devPtr, <span class="hljs-type">size_t</span>* size, cudaGraphicsResource_t resource);<br></code></pre></td></tr></table></figure><p>以下是一个完整的示例，显示一个动态正弦波图案，由CUDA计算顶点位置并且通过OpenGL渲染：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;GL/glew.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;GL/freeglut.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cuda_runtime.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cuda_gl_interop.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-type">const</span> <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> WIDTH = <span class="hljs-number">800</span>;<br><span class="hljs-type">const</span> <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> HEIGHT = <span class="hljs-number">600</span>;<br><br>GLuint positionsVBO;<br>cudaGraphicsResource* positionsVBO_CUDA = <span class="hljs-literal">nullptr</span>;<br><span class="hljs-type">float</span> animTime = <span class="hljs-number">0.0f</span>;<br><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> CHECK_CUDA_ERROR(val) check((val), #val, __FILE__, __LINE__)</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">check</span><span class="hljs-params">(cudaError_t result, <span class="hljs-type">const</span> <span class="hljs-type">char</span>* <span class="hljs-type">const</span> func, <span class="hljs-type">const</span> <span class="hljs-type">char</span>* <span class="hljs-type">const</span> file, <span class="hljs-type">int</span> line)</span> </span>&#123;<br>    <span class="hljs-keyword">if</span> (result != cudaSuccess) &#123;<br>        std::cerr &lt;&lt; <span class="hljs-string">&quot;CUDA error at &quot;</span> &lt;&lt; file &lt;&lt; <span class="hljs-string">&quot;:&quot;</span> &lt;&lt; line &lt;&lt; <span class="hljs-string">&quot; code=&quot;</span> &lt;&lt; result <br>                  &lt;&lt; <span class="hljs-string">&quot; \&quot;&quot;</span> &lt;&lt; <span class="hljs-built_in">cudaGetErrorString</span>(result) &lt;&lt; <span class="hljs-string">&quot;\&quot; &quot;</span><br>                  &lt;&lt; func &lt;&lt; std::endl;<br>        <span class="hljs-built_in">exit</span>(EXIT_FAILURE);<br>    &#125;<br>&#125;<br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">createVertices</span><span class="hljs-params">(float4* positions, <span class="hljs-type">float</span> time, </span></span><br><span class="hljs-params"><span class="hljs-function">                             <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> width, <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> height)</span> </span>&#123;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;<br>    <br>    <span class="hljs-keyword">if</span> (x &lt; width &amp;&amp; y &lt; height) &#123;  <span class="hljs-comment">// 添加边界检查</span><br>        <span class="hljs-type">float</span> u = x / (<span class="hljs-type">float</span>)width;<br>        <span class="hljs-type">float</span> v = y / (<span class="hljs-type">float</span>)height;<br>        u = u * <span class="hljs-number">2.0f</span> - <span class="hljs-number">1.0f</span>;<br>        v = v * <span class="hljs-number">2.0f</span> - <span class="hljs-number">1.0f</span>;<br>        <br>        <span class="hljs-type">float</span> freq = <span class="hljs-number">4.0f</span>;<br>        <span class="hljs-type">float</span> w = <span class="hljs-built_in">sinf</span>(u * freq + time) * <span class="hljs-built_in">cosf</span>(v * freq + time) * <span class="hljs-number">0.5f</span>;<br>        <br>        positions[y * width + x] = <span class="hljs-built_in">make_float4</span>(u, w, v, <span class="hljs-number">1.0f</span>);<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">init</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-comment">// 初始化OpenGL缓冲区和CUDA互操作</span><br>    <span class="hljs-built_in">glClearColor</span>(<span class="hljs-number">0.0f</span>, <span class="hljs-number">0.0f</span>, <span class="hljs-number">0.0f</span>, <span class="hljs-number">1.0f</span>);<br>    <span class="hljs-built_in">glPointSize</span>(<span class="hljs-number">1.0f</span>);<br>    <br>    <span class="hljs-built_in">glGenBuffers</span>(<span class="hljs-number">1</span>, &amp;positionsVBO);<br>    <span class="hljs-built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, positionsVBO);<br>    <span class="hljs-built_in">glBufferData</span>(GL_ARRAY_BUFFER, WIDTH * HEIGHT * <span class="hljs-built_in">sizeof</span>(float4), <span class="hljs-literal">nullptr</span>, GL_DYNAMIC_DRAW);<br>    <span class="hljs-built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, <span class="hljs-number">0</span>);<br><br>    <span class="hljs-comment">// 检查设备是否支持CUDA-OpenGL互操作</span><br>    cudaDeviceProp prop;<br>    <span class="hljs-built_in">CHECK_CUDA_ERROR</span>(<span class="hljs-built_in">cudaGetDeviceProperties</span>(&amp;prop, <span class="hljs-number">0</span>));<br>    <span class="hljs-keyword">if</span> (!prop.canMapHostMemory || !prop.unifiedAddressing) &#123;<br>        std::cerr &lt;&lt; <span class="hljs-string">&quot;Device does not support required features for interop&quot;</span> &lt;&lt; std::endl;<br>        <span class="hljs-built_in">exit</span>(EXIT_FAILURE);<br>    &#125;<br><br>    <span class="hljs-built_in">CHECK_CUDA_ERROR</span>(<span class="hljs-built_in">cudaGraphicsGLRegisterBuffer</span>(&amp;positionsVBO_CUDA,<br>                                                positionsVBO,<br>                                                cudaGraphicsMapFlagsWriteDiscard));<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">display</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-comment">// 确保OpenGL上下文正确</span><br>    <span class="hljs-built_in">glutSetWindow</span>(<span class="hljs-built_in">glutGetWindow</span>());<br>    <br>    <span class="hljs-comment">// 解除所有可能影响互操作的OpenGL绑定</span><br>    <span class="hljs-built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, <span class="hljs-number">0</span>);<br>    <br>    <span class="hljs-comment">// 映射资源</span><br>    <span class="hljs-built_in">CHECK_CUDA_ERROR</span>(<span class="hljs-built_in">cudaGraphicsMapResources</span>(<span class="hljs-number">1</span>, &amp;positionsVBO_CUDA, <span class="hljs-number">0</span>));<br>    <br>    float4* d_positions = <span class="hljs-literal">nullptr</span>;<br>    <span class="hljs-type">size_t</span> num_bytes;<br>    <span class="hljs-built_in">CHECK_CUDA_ERROR</span>(<span class="hljs-built_in">cudaGraphicsResourceGetMappedPointer</span>(<br>        (<span class="hljs-type">void</span>**)&amp;d_positions, &amp;num_bytes, positionsVBO_CUDA));<br><br>    <span class="hljs-comment">// 执行内核</span><br>    <span class="hljs-function">dim3 <span class="hljs-title">block</span><span class="hljs-params">(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">grid</span><span class="hljs-params">((WIDTH + block.x - <span class="hljs-number">1</span>) / block.x, </span></span><br><span class="hljs-params"><span class="hljs-function">              (HEIGHT + block.y - <span class="hljs-number">1</span>) / block.y)</span></span>;<br>    createVertices&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_positions, animTime, WIDTH, HEIGHT);<br>    <br>    <span class="hljs-comment">// 确保内核执行完成</span><br>    <span class="hljs-built_in">CHECK_CUDA_ERROR</span>(<span class="hljs-built_in">cudaDeviceSynchronize</span>());<br>    <br>    <span class="hljs-comment">// 解映射资源</span><br>    <span class="hljs-built_in">CHECK_CUDA_ERROR</span>(<span class="hljs-built_in">cudaGraphicsUnmapResources</span>(<span class="hljs-number">1</span>, &amp;positionsVBO_CUDA, <span class="hljs-number">0</span>));<br>    <br>    <span class="hljs-comment">// 渲染</span><br>    <span class="hljs-built_in">glClear</span>(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);<br>    <span class="hljs-built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, positionsVBO);<br>    <span class="hljs-built_in">glVertexPointer</span>(<span class="hljs-number">4</span>, GL_FLOAT, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>);<br>    <span class="hljs-built_in">glEnableClientState</span>(GL_VERTEX_ARRAY);<br>    <span class="hljs-built_in">glDrawArrays</span>(GL_POINTS, <span class="hljs-number">0</span>, WIDTH * HEIGHT);<br>    <span class="hljs-built_in">glDisableClientState</span>(GL_VERTEX_ARRAY);<br>    <span class="hljs-built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, <span class="hljs-number">0</span>);  <span class="hljs-comment">// 解除绑定</span><br>    <br>    <span class="hljs-built_in">glutSwapBuffers</span>();<br>    <span class="hljs-built_in">glutPostRedisplay</span>();<br>    <br>    animTime += <span class="hljs-number">0.01f</span>;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">cleanup</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-keyword">if</span> (positionsVBO_CUDA) &#123;<br>        <span class="hljs-built_in">cudaGraphicsUnregisterResource</span>(positionsVBO_CUDA);<br>        positionsVBO_CUDA = <span class="hljs-literal">nullptr</span>;<br>    &#125;<br>    <br>    <span class="hljs-keyword">if</span> (positionsVBO) &#123;<br>        <span class="hljs-built_in">glDeleteBuffers</span>(<span class="hljs-number">1</span>, &amp;positionsVBO);<br>        positionsVBO = <span class="hljs-number">0</span>;<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">keyboard</span><span class="hljs-params">(<span class="hljs-type">unsigned</span> <span class="hljs-type">char</span> key, <span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y)</span> </span>&#123;<br>    <span class="hljs-keyword">if</span> (key == <span class="hljs-number">27</span>) &#123;  <span class="hljs-comment">// ESC键</span><br>        <span class="hljs-built_in">cleanup</span>();<br>        <span class="hljs-built_in">exit</span>(EXIT_SUCCESS);<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-type">int</span> argc, <span class="hljs-type">char</span>** argv)</span> </span>&#123;<br>    <span class="hljs-built_in">glutInit</span>(&amp;argc, argv);<br>    <span class="hljs-built_in">glutInitDisplayMode</span>(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH);<br>    <span class="hljs-built_in">glutInitWindowSize</span>(WIDTH, HEIGHT);<br>    <span class="hljs-type">int</span> window = <span class="hljs-built_in">glutCreateWindow</span>(<span class="hljs-string">&quot;CUDA-OpenGL Interop Demo&quot;</span>);<br>    <br>    <span class="hljs-comment">// 必须在创建窗口后初始化GLEW</span><br>    glewExperimental = GL_TRUE;<br>    GLenum err = <span class="hljs-built_in">glewInit</span>();<br>    <span class="hljs-keyword">if</span> (err != GLEW_OK) &#123;<br>        std::cerr &lt;&lt; <span class="hljs-string">&quot;GLEW init failed: &quot;</span> &lt;&lt; <span class="hljs-built_in">glewGetErrorString</span>(err) &lt;&lt; std::endl;<br>        <span class="hljs-keyword">return</span> EXIT_FAILURE;<br>    &#125;<br>    <br>    <span class="hljs-comment">// 检查必要扩展</span><br>    <span class="hljs-keyword">if</span> (!<span class="hljs-built_in">glewIsSupported</span>(<span class="hljs-string">&quot;GL_VERSION_2_0&quot;</span>)) &#123;<br>        std::cerr &lt;&lt; <span class="hljs-string">&quot;OpenGL 2.0 not supported&quot;</span> &lt;&lt; std::endl;<br>        <span class="hljs-keyword">return</span> EXIT_FAILURE;<br>    &#125;<br><br>    <span class="hljs-built_in">init</span>();<br>    <br>    <span class="hljs-built_in">glutDisplayFunc</span>(display);<br>    <span class="hljs-built_in">glutKeyboardFunc</span>(keyboard);<br>    <span class="hljs-built_in">glutCloseFunc</span>(cleanup);  <span class="hljs-comment">// 确保退出时清理</span><br>    <br>    <span class="hljs-built_in">glutMainLoop</span>();<br>    <span class="hljs-keyword">return</span> EXIT_SUCCESS;<br>&#125;<br></code></pre></td></tr></table></figure><p>确保安装OpenGL开发库(GLEW和FreeGLUT)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt-get update<br><span class="hljs-built_in">sudo</span> apt-get install -y libglew-dev freeglut3-dev<br></code></pre></td></tr></table></figure><p>使用如下编译命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvcc -o cuda_gl_interop cuda_gl_interop.cu -lGL -lGLEW -lglut<br></code></pre></td></tr></table></figure><p>CUDA支持的Direct3D版本：</p><table><thead><tr><th style="text-align:left">Direct3D版本</th><th style="text-align:left">关键创建参数</th></tr></thead><tbody><tr><td style="text-align:left">D3D9Ex</td><td style="text-align:left"><code>DeviceType = D3DDEVTYPE_HAL</code> <code>BehaviorFlags = D3DCREATE_HARDWARE_VERTEXPROCESSING</code></td></tr><tr><td style="text-align:left">D3D10/D3D11</td><td style="text-align:left"><code>DriverType = D3D_DRIVER_TYPE_HARDWARE</code></td></tr></tbody></table><p>使用如下注册函数：</p><table><thead><tr><th style="text-align:left">Direct3D版本</th><th style="text-align:left">注册函数</th></tr></thead><tbody><tr><td style="text-align:left">D3D9Ex</td><td style="text-align:left"><code>cudaGraphicsD3D9RegisterResource()</code></td></tr><tr><td style="text-align:left">D3D10</td><td style="text-align:left"><code>cudaGraphicsD3D10RegisterResource()</code></td></tr><tr><td style="text-align:left">D3D11</td><td style="text-align:left"><code>cudaGraphicsD3D11RegisterResource()</code></td></tr></tbody></table><p>在配备多个NVIDIA GPU的系统中，CUDA将每个物理GPU视为独立的计算设备（device），每个设备有独立的设备ID。但在SLI（Scalable Link Interface）配置下，GPU间的协同工作会带来特殊考量：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> deviceCount;<br><span class="hljs-built_in">cudaGetDeviceCount</span>(&amp;deviceCount); <span class="hljs-comment">// 获取系统中CUDA设备总数</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; deviceCount; i++) &#123;<br>    cudaDeviceProp prop;<br>    <span class="hljs-built_in">cudaGetDeviceProperties</span>(&amp;prop, i); <span class="hljs-comment">// 获取每个设备的详细属性</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;GPU %d: %s\n&quot;</span>, i, prop.name);<br>&#125;<br></code></pre></td></tr></table></figure><p>在SLI配置中，当通过Direct3D或OpenGL分配资源时，内存会在所有SLI组内的GPU上同步分配：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 典型的内存分配失败场景示例</span><br>cudaError_t err = <span class="hljs-built_in">cudaMalloc</span>(&amp;devPtr, size);<br><span class="hljs-keyword">if</span> (err == cudaErrorMemoryAllocation) &#123;<br>    <span class="hljs-comment">// 在SLI配置下可能比预期更早触发内存不足错误</span><br>&#125;<br></code></pre></td></tr></table></figure><table><thead><tr><th style="text-align:left">场景</th><th style="text-align:left">推荐设备选择方法</th><th style="text-align:left">优点</th><th style="text-align:left">缺点</th></tr></thead><tbody><tr><td style="text-align:left">单帧渲染</td><td style="text-align:left">cudaD3D11DeviceListCurrentFrame</td><td style="text-align:left">最小化数据传输</td><td style="text-align:left">需要频繁切换</td></tr><tr><td style="text-align:left">多帧并行</td><td style="text-align:left">轮询分配</td><td style="text-align:left">负载均衡</td><td style="text-align:left">增加同步复杂度</td></tr><tr><td style="text-align:left">数据并行</td><td style="text-align:left">固定设备分配</td><td style="text-align:left">实现简单</td><td style="text-align:left">可能造成瓶颈</td></tr></tbody></table><p>CUDA外部资源互操作性允许CUDA导入由其他API显式导出的资源，实现跨API的高效资源共享。这种机制避免了数据拷贝，特别适用于以下场景：</p><ul><li>与图形API（如OpenGL/Vulkan/Direct3D）共享纹理和缓冲区</li><li>与视频处理API（如NVDEC/NVENC）共享视频帧</li><li>实现多进程GPU资源共享</li><li>与计算框架（如OpenCL）进行互操作</li></ul><p><img src="%E5%86%85%E5%AD%98%E5%AF%BC%E5%85%A5%E6%B5%81%E7%A8%8B.png" alt="内存导入流程" /></p><p>有如下关键的API：</p><p>1.内存导入：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++">cudaExternalMemoryHandleDesc desc = &#123;&#125;;<br>desc.type = cudaExternalMemoryHandleTypeOpaqueWin32;  <span class="hljs-comment">// Windows句柄类型</span><br>desc.handle.win<span class="hljs-number">32.</span>handle = externalHandle;            <span class="hljs-comment">// 外部句柄</span><br>desc.size = allocationSize;                           <span class="hljs-comment">// 内存大小</span><br><br>cudaExternalMemory_t extMem;<br><span class="hljs-built_in">cudaImportExternalMemory</span>(&amp;extMem, &amp;desc);<br></code></pre></td></tr></table></figure><p>2.获取映射缓冲区</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C++">cudaExternalMemoryBufferDesc bufDesc = &#123;&#125;;<br>bufDesc.offset = <span class="hljs-number">0</span>;      <span class="hljs-comment">// 内存偏移</span><br>bufDesc.size = dataSize;  <span class="hljs-comment">// 映射大小</span><br>bufDesc.flags = <span class="hljs-number">0</span>;        <span class="hljs-comment">// 标志位</span><br><br><span class="hljs-type">void</span>* devPtr;<br><span class="hljs-built_in">cudaExternalMemoryGetMappedBuffer</span>(&amp;devPtr, extMem, &amp;bufDesc);<br></code></pre></td></tr></table></figure><p>3.资源清理</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">cudaFree</span>(devPtr);                   <span class="hljs-comment">// 先释放映射</span><br><span class="hljs-built_in">cudaDestroyExternalMemory</span>(extMem);  <span class="hljs-comment">// 再销毁外部内存</span><br></code></pre></td></tr></table></figure><p>进行同步对象互操作时，支持的类型如下：</p><table><thead><tr><th style="text-align:left">同步类型</th><th style="text-align:left">支持平台</th><th style="text-align:left">特性</th></tr></thead><tbody><tr><td style="text-align:left">Opaque FD</td><td style="text-align:left">Linux</td><td style="text-align:left">文件描述符形式</td></tr><tr><td style="text-align:left">Opaque Win32</td><td style="text-align:left">Windows</td><td style="text-align:left">NT句柄形式</td></tr><tr><td style="text-align:left">NVIDIA SCI</td><td style="text-align:left">跨平台</td><td style="text-align:left">高性能NVIDIA专用接口</td></tr></tbody></table><p>整体的信号量工作流程如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 导入信号量</span><br>cudaExternalSemaphoreHandleDesc semDesc = &#123;&#125;;<br>semDesc.type = cudaExternalSemaphoreHandleTypeOpaqueWin32;<br>semDesc.handle.win<span class="hljs-number">32.</span>handle = semaphoreHandle;<br><br>cudaExternalSemaphore_t extSem;<br><span class="hljs-built_in">cudaImportExternalSemaphore</span>(&amp;extSem, &amp;semDesc);<br><br><span class="hljs-comment">// 信号操作序列</span><br>cudaStream_t stream;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream);<br><br><span class="hljs-comment">// Vulkan完成写入后，CUDA等待信号</span><br>cudaExternalSemaphoreWaitParams waitParams = &#123;&#125;;<br>waitParams.params.fence.value = <span class="hljs-number">0</span>;  <span class="hljs-comment">// 同步值</span><br><span class="hljs-built_in">cudaWaitExternalSemaphoresAsync</span>(&amp;extSem, &amp;waitParams, <span class="hljs-number">1</span>, stream);<br><br><span class="hljs-comment">// CUDA计算任务</span><br>myKernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(...);<br><br><span class="hljs-comment">// CUDA完成后触发信号</span><br>cudaExternalSemaphoreSignalParams signalParams = &#123;&#125;;<br>signalParams.params.fence.value = <span class="hljs-number">1</span>;<br><span class="hljs-built_in">cudaSignalExternalSemaphoresAsync</span>(&amp;extSem, &amp;signalParams, <span class="hljs-number">1</span>, stream);<br></code></pre></td></tr></table></figure><p><img src="%E8%B7%A8API%E5%90%8C%E6%AD%A5%E6%A8%A1%E5%BC%8F.png" alt="跨API同步模式" /></p><p><strong>Windows平台(NT句柄)</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 安全属性设置</span><br>SECURITY_ATTRIBUTES sa = &#123;&#125;;<br>sa.nLength = <span class="hljs-built_in">sizeof</span>(sa);<br>sa.bInheritHandle = TRUE;  <span class="hljs-comment">// 允许继承</span><br><br><span class="hljs-comment">// 创建可导出资源</span><br>HANDLE handle;<br>cudaExternalMemoryHandleDesc desc = &#123;&#125;;<br>desc.type = cudaExternalMemoryHandleTypeOpaqueWin32;<br>desc.handle.win<span class="hljs-number">32.</span>handle = handle;<br>desc.flags = cudaExternalMemoryDedicated;  <span class="hljs-comment">// 专用内存标志</span><br></code></pre></td></tr></table></figure><p><strong>Linux平台(文件描述符)</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 导出FD</span><br><span class="hljs-type">int</span> fd = <span class="hljs-built_in">exportVulkanMemoryToFD</span>();<br><br>cudaExternalMemoryHandleDesc desc = &#123;&#125;;<br>desc.type = cudaExternalMemoryHandleTypeOpaqueFd;<br>desc.handle.fd = fd;<br>desc.size = memSize;<br></code></pre></td></tr></table></figure><p><strong>NVIDIA SCI(高性能互连)</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 使用NVIDIA专用接口</span><br>cudaExternalMemoryHandleDesc desc = &#123;&#125;;<br>desc.type = cudaExternalMemoryHandleTypeNvSciBuf;<br>desc.handle.nvSciBufObject = nvSciBufObj;<br></code></pre></td></tr></table></figure><h2 id="CUDA硬件实现">CUDA硬件实现</h2><p>NVIDIA显卡的核心是由许多&quot;流式多处理器&quot;(SM)组成的阵列。当CPU启动一个CUDA计算任务时：</p><ul><li>计算任务被分成多个&quot;线程块&quot;</li><li>这些块会被分配到空闲的SM上执行</li><li>每个SM可以同时运行多个线程块</li><li>当一个块完成时，新的块会立即补上、</li></ul><p>每个SM需要同时处理数百个线程，为此NVIDIA发明了独特的&quot;SIMT&quot;(单指令多线程)技术，SIMT相较于CPU有如下区别：</p><table><thead><tr><th style="text-align:left">特性</th><th style="text-align:left">CPU</th><th style="text-align:left">GPU(SIMT)</th></tr></thead><tbody><tr><td style="text-align:left">执行方式</td><td style="text-align:left">一次处理1个线程</td><td style="text-align:left">一次处理32个线程(1个warp)</td></tr><tr><td style="text-align:left">分支预测</td><td style="text-align:left">有</td><td style="text-align:left">无</td></tr><tr><td style="text-align:left">线程切换</td><td style="text-align:left">开销大</td><td style="text-align:left">零开销</td></tr><tr><td style="text-align:left">适合场景</td><td style="text-align:left">复杂逻辑</td><td style="text-align:left">大批量简单计算</td></tr></tbody></table><p>Warp，称之为线程束，就像纺织中的&quot;经线束&quot;一样，32个线程捆绑成一组执行，所有线程同时开始执行相同指令，但每个线程有自己的数据和执行路径。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 示例：查看warp大小</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">showWarpSize</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-keyword">if</span>(threadIdx.x == <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;1个warp包含%d个线程\n&quot;</span>, warpSize); <span class="hljs-comment">// 总是32</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>尽量确保一个warp中的32个线程走相同的路径：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 高效访问：32个线程连续读取内存</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">goodMemoryAccess</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, <span class="hljs-type">float</span>* input)</span> </span>&#123;<br>    <span class="hljs-type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;<br>    output[i] = input[i]; <span class="hljs-comment">// 像军训排队一样整齐</span><br>&#125;<br><br><span class="hljs-comment">// 低效访问：线程跳跃式访问</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">badMemoryAccess</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, <span class="hljs-type">float</span>* input)</span> </span>&#123;<br>    <span class="hljs-type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;<br>    output[i*<span class="hljs-number">10</span>] = input[i*<span class="hljs-number">10</span>]; <span class="hljs-comment">// 像散兵游勇</span><br>&#125;<br></code></pre></td></tr></table></figure><p>为了避免降低warp效率，我们可以采用一些策略：</p><p>1.减少分叉</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 优化前：可能造成warp分叉</span><br><span class="hljs-keyword">if</span>(index % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) &#123;<br>    <span class="hljs-comment">// A路径</span><br>&#125; <span class="hljs-keyword">else</span> &#123;<br>    <span class="hljs-comment">// B路径</span><br>&#125;<br><br><span class="hljs-comment">// 优化后：减少分叉</span><br>result = (index % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) ? <span class="hljs-built_in">calcA</span>() : <span class="hljs-built_in">calcB</span>();<br></code></pre></td></tr></table></figure><ol start="2"><li>自动配置资源</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 自动计算最佳配置</span><br><span class="hljs-type">int</span> blockSize;   <span class="hljs-comment">// 每个块的线程数</span><br><span class="hljs-type">int</span> minGridSize; <span class="hljs-comment">// 最少需要多少个块</span><br><span class="hljs-built_in">cudaOccupancyCalculateBestBlockSize</span>(&amp;blockSize, &amp;minGridSize, myKernel);<br></code></pre></td></tr></table></figure><p>可以执行调试命令查看相关性能：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看warp执行效率</span><br>nvprof --metrics achieved_occupancy ./myApp<br><br><span class="hljs-comment"># 检查内存访问模式</span><br>nsight-compute --<span class="hljs-built_in">set</span> default --section MemoryWorkloadAnalysis ./myApp<br></code></pre></td></tr></table></figure><p>GPU的流式多处理器(SM)采用独特的<strong>硬件级上下文管理</strong>：</p><ul><li>每个warp的完整执行状态（程序计数器、寄存器等）始终保存在芯片上</li><li>切换不同warp的执行<strong>不需要保存/恢复上下文</strong></li><li>硬件调度器每个时钟周期选择就绪warp发射指令</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 示例：查看当前设备的warp调度器数量</span><br>cudaDeviceProp prop;<br><span class="hljs-built_in">cudaGetDeviceProperties</span>(&amp;prop, <span class="hljs-number">0</span>);<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;每个SM的warp调度器数量: %d\n&quot;</span>, prop.maxThreadsPerMultiProcessor / <span class="hljs-number">32</span>);<br></code></pre></td></tr></table></figure><p>SM资源分区模型由以下几种类型：</p><table><thead><tr><th style="text-align:left">资源类型</th><th style="text-align:left">分配单位</th><th style="text-align:left">特性</th></tr></thead><tbody><tr><td style="text-align:left">寄存器文件</td><td style="text-align:left">每个线程</td><td style="text-align:left">最快的存储，生命周期随线程</td></tr><tr><td style="text-align:left">共享内存</td><td style="text-align:left">每个线程块</td><td style="text-align:left">块内线程共享，手动管理</td></tr><tr><td style="text-align:left">L1缓存/纹理缓存</td><td style="text-align:left">整个SM</td><td style="text-align:left">自动缓存</td></tr></tbody></table><p>对于特定的资源，有其合理的分配方式：</p><p>每个块的warp数量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">warps_per_block = ceil(threads_per_block, 32) / 32<br></code></pre></td></tr></table></figure><p>寄存器分配：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">总寄存器 = 每个线程寄存器数 × 线程数 × warp数量<br></code></pre></td></tr></table></figure><p>共享内存分配：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">__shared__ <span class="hljs-built_in">float</span> tile[32][32]; // 静态分配<br>extern __shared__ int dynamic[]; // 动态分配<br></code></pre></td></tr></table></figure><h2 id="CUDA性能指南">CUDA性能指南</h2><p>性能优化围绕三个基本策略展开：</p><ul><li>最大化并行执行以实现最大利用率</li><li>优化内存使用以实现最大内存吞吐量</li><li>优化指令使用以实现最大的指令吞吐量</li></ul><h3 id="最大化利用率">最大化利用率</h3><p>为了最大限度地提高利用率，应用程序的结构应尽可能多地公开并行性，并有效地将这种并行性映射到系统的各个组件，以使它们在大部分时间都保持忙碌。</p><p>在<strong>应用层面</strong>的高层设计上，应用程序应通过异步函数调用和流(stream)技术，最大化主机(CPU)、设备(GPU)以及连接总线之间的并行执行。基本原则是：</p><ul><li>CPU处理串行任务</li><li>GPU处理并行任务</li></ul><p>对于需要线程间数据同步的并行任务，存在两种情况：</p><ol><li><p><strong>同一线程块内同步</strong>：使用<code>__syncthreads()</code>和共享内存</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp">__shared__ <span class="hljs-type">float</span> temp[<span class="hljs-number">32</span>];  <span class="hljs-comment">// 共享内存</span><br>temp[threadIdx.x] = data;<br>__syncthreads();           <span class="hljs-comment">// 块内同步</span><br></code></pre></td></tr></table></figure></li><li><p><strong>跨线程块同步</strong>：必须通过全局内存和多次内核调用实现（效率较低）</p></li></ol><p>在<strong>设备内部</strong>，应最大化多处理器(SM)间的并行执行：</p><ul><li>通过流(stream)实现多内核并发执行：</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp">cudaStream_t stream1, stream2;<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream1);<br><span class="hljs-built_in">cudaStreamCreate</span>(&amp;stream2);<br>kernel1&lt;&lt;&lt;..., stream1&gt;&gt;&gt;();<br>kernel2&lt;&lt;&lt;..., stream2&gt;&gt;&gt;();<br></code></pre></td></tr></table></figure><p>在<strong>多处理器层面</strong>，每个SM内部，应最大化各功能单元的并行利用率：</p><ul><li><strong>Warp调度</strong>：每个时钟周期选择就绪的warp执行指令</li><li><strong>延迟隐藏</strong>：通过足够多的活跃warp掩盖指令延迟</li><li><strong>计算能力差异</strong>：<ul><li>5.x/6.1/7.x设备：需要4×延迟周期数的warp</li><li>6.0设备：需要2×延迟周期数的warp</li><li>3.x设备：需要8×延迟周期数的warp</li></ul></li></ul><p>CUDA提供API帮助优化线程块配置：</p><p>占用率计算示例：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 设备代码</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MyKernel</span><span class="hljs-params">(<span class="hljs-type">int</span> *d, <span class="hljs-type">int</span> *a, <span class="hljs-type">int</span> *b)</span> </span>&#123;<br>    <span class="hljs-type">int</span> idx = threadIdx.x + blockIdx.x * blockDim.x;<br>    d[idx] = a[idx] * b[idx];<br>&#125;<br><br><span class="hljs-comment">// 主机代码</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-type">int</span> numBlocks, blockSize = <span class="hljs-number">32</span>;<br>    cudaDeviceProp prop;<br>    <span class="hljs-built_in">cudaGetDeviceProperties</span>(&amp;prop, <span class="hljs-number">0</span>);<br>    <br>    <span class="hljs-comment">// 计算占用率</span><br>    <span class="hljs-built_in">cudaOccupancyMaxActiveBlocksPerMultiprocessor</span>(<br>        &amp;numBlocks, MyKernel, blockSize, <span class="hljs-number">0</span>);<br>    <br>    <span class="hljs-type">float</span> occupancy = (numBlocks * blockSize / prop.warpSize) / <br>                    (<span class="hljs-type">float</span>)(prop.maxThreadsPerMultiProcessor / prop.warpSize);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;占用率: %.1f%%\n&quot;</span>, occupancy * <span class="hljs-number">100</span>);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>自动配置启动参数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">launchKernel</span><span class="hljs-params">(<span class="hljs-type">int</span> *array, <span class="hljs-type">int</span> count)</span> </span>&#123;<br>    <span class="hljs-type">int</span> blockSize, minGridSize, gridSize;<br>    <br>    <span class="hljs-comment">// 获取最优配置</span><br>    <span class="hljs-built_in">cudaOccupancyMaxPotentialBlockSize</span>(<br>        &amp;minGridSize, &amp;blockSize, MyKernel, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>);<br>    <br>    <span class="hljs-comment">// 根据数据量调整网格大小</span><br>    gridSize = (count + blockSize - <span class="hljs-number">1</span>) / blockSize;<br>    MyKernel&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(array, count);<br>    <br>    <span class="hljs-built_in">cudaDeviceSynchronize</span>();<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="最大化内存吞吐量">最大化内存吞吐量</h3><p>一、内存传输优化原则</p><ol><li>最小化主机-设备数据传输</li></ol><ul><li><p><strong>数据传输层级对比</strong>：</p><table><thead><tr><th style="text-align:left">传输类型</th><th style="text-align:left">带宽</th><th style="text-align:left">延迟</th><th style="text-align:left">优化建议</th></tr></thead><tbody><tr><td style="text-align:left">主机↔设备</td><td style="text-align:left">低</td><td style="text-align:left">高</td><td style="text-align:left">尽量减少</td></tr><tr><td style="text-align:left">全局内存访问</td><td style="text-align:left">中</td><td style="text-align:left">中</td><td style="text-align:left">优化访问模式</td></tr><tr><td style="text-align:left">片上内存(共享/L1)</td><td style="text-align:left">高</td><td style="text-align:left">低</td><td style="text-align:left">最大化使用</td></tr></tbody></table></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 坏实践：频繁小数据传输</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;<span class="hljs-number">1000</span>; i++) &#123;<br>    <span class="hljs-built_in">cudaMemcpy</span>(devPtr+i, hostPtr+i, <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice);<br>&#125;<br><br><span class="hljs-comment">// 好实践：批量传输</span><br><span class="hljs-built_in">cudaMemcpy</span>(devPtr, hostPtr, <span class="hljs-number">1000</span>*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice);<br></code></pre></td></tr></table></figure><ol start="2"><li>使用固定内存(pinned memory)</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">float</span>* hostPtr;<br><span class="hljs-built_in">cudaHostAlloc</span>(&amp;hostPtr, size, cudaHostAllocDefault);  <span class="hljs-comment">// 分配固定内存</span><br>kernel&lt;&lt;&lt;...&gt;&gt;&gt;(devPtr, ...);<br><span class="hljs-built_in">cudaMemcpyAsync</span>(hostPtr, devPtr, size, cudaMemcpyDeviceToHost, stream);<br></code></pre></td></tr></table></figure><p>二、全局内存访问优化</p><ol><li>合并访问模式</li></ol><ul><li><strong>理想情况</strong>：一个warp的32个线程访问连续的128字节内存块</li><li><strong>访问模式对比</strong>：</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 非合并访问(低效)</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">badAccess</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, <span class="hljs-type">float</span>* input, <span class="hljs-type">int</span> stride)</span> </span>&#123;<br>    <span class="hljs-type">int</span> tid = threadIdx.x + blockIdx.x * blockDim.x;<br>    output[tid] = input[tid * stride];  <span class="hljs-comment">// 跨步访问</span><br>&#125;<br><br><span class="hljs-comment">// 合并访问(高效)</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">goodAccess</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, <span class="hljs-type">float</span>* input)</span> </span>&#123;<br>    <span class="hljs-type">int</span> tid = threadIdx.x + blockIdx.x * blockDim.x;<br>    output[tid] = input[tid];  <span class="hljs-comment">// 连续访问</span><br>&#125;<br></code></pre></td></tr></table></figure><ol start="2"><li>二维数组的特殊处理</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 使用cudaMallocPitch处理非对齐宽度</span><br><span class="hljs-type">size_t</span> pitch;<br><span class="hljs-type">float</span>* devPtr;<br><span class="hljs-built_in">cudaMallocPitch</span>(&amp;devPtr, &amp;pitch, width*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), height);<br><br><span class="hljs-comment">// 拷贝时指定pitch</span><br><span class="hljs-built_in">cudaMemcpy2D</span>(devPtr, pitch, hostPtr, width*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), <br>             width*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), height, cudaMemcpyHostToDevice);<br></code></pre></td></tr></table></figure><p>三、各类内存特性与优化</p><ol><li><p>共享内存使用技巧</p><p><strong>bank冲突避免</strong>：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp">__shared__ <span class="hljs-type">float</span> tile[<span class="hljs-number">32</span>][<span class="hljs-number">32</span><span class="hljs-number">+1</span>]; <span class="hljs-comment">// 添加padding消除bank冲突</span><br><br><span class="hljs-comment">// 无冲突访问模式</span><br><span class="hljs-type">float</span> val = tile[threadIdx.y][threadIdx.x]; <span class="hljs-comment">// 线程ID与bank分布匹配</span><br></code></pre></td></tr></table></figure><p><strong>典型使用模式</strong>：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">sharedMemExample</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, <span class="hljs-type">float</span>* input)</span> </span>&#123;<br>    __shared__ <span class="hljs-type">float</span> temp[BLOCK_SIZE];<br>    <span class="hljs-type">int</span> tid = threadIdx.x;<br>    <br>    <span class="hljs-comment">// 1. 从全局内存加载到共享内存</span><br>    temp[tid] = input[blockIdx.x * blockDim.x + tid];<br>    <br>    <span class="hljs-comment">// 2. 同步等待所有线程完成加载</span><br>    __syncthreads();<br>    <br>    <span class="hljs-comment">// 3. 处理共享内存数据</span><br>    temp[tid] *= <span class="hljs-number">2.0f</span>;<br>    <br>    <span class="hljs-comment">// 4. 同步确保处理完成</span><br>    __syncthreads();<br>    <br>    <span class="hljs-comment">// 5. 写回全局内存</span><br>    output[blockIdx.x * blockDim.x + tid] = temp[tid];<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>常量内存与纹理内存</p><p><strong>常量内存适用场景</strong>：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp">__constant__ <span class="hljs-type">float</span> constData[<span class="hljs-number">256</span>];<br><br><span class="hljs-comment">// 初始化常量内存</span><br><span class="hljs-built_in">cudaMemcpyToSymbol</span>(constData, hostData, <span class="hljs-built_in">sizeof</span>(hostData));<br></code></pre></td></tr></table></figure><p><strong>纹理内存优势</strong>：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp">texture&lt;<span class="hljs-type">float</span>, <span class="hljs-number">2</span>, cudaReadModeElementType&gt; texRef;<br><br><span class="hljs-comment">// 绑定纹理</span><br><span class="hljs-built_in">cudaBindTexture2D</span>(<span class="hljs-number">0</span>, texRef, devPtr, channelDesc, width, height, pitch);<br><br><span class="hljs-comment">// 内核中访问</span><br><span class="hljs-type">float</span> val = <span class="hljs-built_in">tex2D</span>(texRef, x, y);<br></code></pre></td></tr></table></figure></li></ol><h3 id="最大化指令吞吐量">最大化指令吞吐量</h3><p>一、算术运算优化策略</p><ol><li><p>精度与速度的权衡</p><table><thead><tr><th style="text-align:left">运算类型</th><th style="text-align:left">推荐操作</th><th style="text-align:left">加速比</th><th style="text-align:left">适用场景</th></tr></thead><tbody><tr><td style="text-align:left">单精度除法</td><td style="text-align:left">使用<code>__fdividef(x,y)</code></td><td style="text-align:left">2-5倍</td><td style="text-align:left">精度要求不高时</td></tr><tr><td style="text-align:left">平方根倒数</td><td style="text-align:left">直接调用<code>rsqrtf()</code></td><td style="text-align:left">3倍</td><td style="text-align:left">图形渲染、物理模拟</td></tr><tr><td style="text-align:left">三角函数</td><td style="text-align:left">控制参数范围&lt;105615.0f</td><td style="text-align:left">10倍</td><td style="text-align:left">避免慢速路径</td></tr></tbody></table></li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 快速除法示例</span><br><span class="hljs-type">float</span> result = __fdividef(a, b);  <span class="hljs-comment">// 比常规除法快</span><br><br><span class="hljs-comment">// 优化三角函数调用</span><br><span class="hljs-keyword">if</span>(<span class="hljs-built_in">fabs</span>(x) &lt; <span class="hljs-number">105615.0f</span>) &#123;        <span class="hljs-comment">// 保持使用快速路径</span><br>    <span class="hljs-type">float</span> s = <span class="hljs-built_in">sinf</span>(x); <br>&#125;<br></code></pre></td></tr></table></figure><ol start="2"><li>半精度浮点运算技巧</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 使用half2类型实现双倍吞吐量</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">halfPrecisionKernel</span><span class="hljs-params">(half2* data)</span> </span>&#123;<br>    half2 a = data[threadIdx.x];<br>    half2 b = __hadd2(a, <span class="hljs-built_in">make_half2</span>(<span class="hljs-number">1.0f</span>, <span class="hljs-number">1.0f</span>)); <span class="hljs-comment">// 同时加两个数</span><br>    data[threadIdx.x] = __hmul2(a, b);            <span class="hljs-comment">// 同时乘两个数</span><br>&#125;<br></code></pre></td></tr></table></figure><p>二、控制流优化方法</p><ol><li>避免分支分歧（<strong>Warp对齐条件</strong>：确保同一warp内线程执行相同路径）</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 差实践：基于threadIdx的条件分支</span><br><span class="hljs-keyword">if</span>(threadIdx.x % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) &#123; <span class="hljs-comment">/* 导致warp分歧 */</span> &#125;<br><br><span class="hljs-comment">// 好实践：Warp对齐条件</span><br><span class="hljs-keyword">if</span>((threadIdx.x / warpSize) % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) &#123; <span class="hljs-comment">/* 保持warp统一 */</span> &#125;、<br></code></pre></td></tr></table></figure><ol start="2"><li>循环展开优化</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">pragma</span> unroll 4  <span class="hljs-comment">// 手动指定展开因子</span></span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;N; i++) &#123;<br>    <span class="hljs-comment">// 循环体</span><br>&#125;<br></code></pre></td></tr></table></figure><p>三. 同步指令性能</p><h3 id="各架构同步吞吐量对比">各架构同步吞吐量对比</h3><table><thead><tr><th style="text-align:left">计算能力</th><th style="text-align:left"><code>__syncthreads()</code>吞吐量</th><th style="text-align:left">相当于每周期操作数</th></tr></thead><tbody><tr><td style="text-align:left">3.x</td><td style="text-align:left">高</td><td style="text-align:left">128 ops/cycle</td></tr><tr><td style="text-align:left">5.x/6.1/6.2</td><td style="text-align:left">中</td><td style="text-align:left">64 ops/cycle</td></tr><tr><td style="text-align:left">6.0</td><td style="text-align:left">低</td><td style="text-align:left">32 ops/cycle</td></tr><tr><td style="text-align:left">7.x</td><td style="text-align:left">最低</td><td style="text-align:left">16 ops/cycle</td></tr></tbody></table><p>四. 类型转换优化</p><ol><li>避免隐式转换开销</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 差实践：引发int转换</span><br><span class="hljs-type">short</span> a = b + c;  <span class="hljs-comment">// 转换为int运算</span><br><br><span class="hljs-comment">// 好实践：使用合适类型</span><br><span class="hljs-type">int</span> a = b + c;    <span class="hljs-comment">// 无转换开销</span><br></code></pre></td></tr></table></figure><ol start="2"><li>浮点常量优化</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">float</span> a = <span class="hljs-number">3.141592653589793</span>;   <span class="hljs-comment">// 引发双精度到单精度转换</span><br><span class="hljs-type">float</span> b = <span class="hljs-number">3.141592653589793f</span>;  <span class="hljs-comment">// 直接使用单精度常量</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>高性能计算</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Perf Linux使用教程</title>
    <link href="/Diffcc/Diffcc.github.io/2025/06/10/20250610/"/>
    <url>/Diffcc/Diffcc.github.io/2025/06/10/20250610/</url>
    
    <content type="html"><![CDATA[<p><code>perf</code> 是 Linux 内核提供的一个强大的性能分析工具，基于 <strong>硬件性能计数器（Performance Monitoring Counters, PMC）</strong> 和 <strong>内核事件采样</strong> 来监控系统性能。</p><span id="more"></span><h2 id="基本原理">基本原理</h2><p>Linux性能计数器是一个基于内核的子系统，它提供一个性能分析框架，比如硬件**（CPU、PMU（Performance Monitoring Unit））<strong>功能和软件</strong>（软件计数器、tracepoint）**功能。通过perf，应用程序可以利用PMU、tracepoint和内核中的计数器来进行性能统计。</p><ul><li>现代 CPU（如 Intel、AMD、ARM）都内置了 <strong>性能监控单元（PMU, Performance Monitoring Unit）</strong>，可以统计各种硬件事件：<strong>CPU 周期数（cycles）</strong>，<strong>指令数（instructions）</strong>，<strong>缓存命中/失效（cache-misses, cache-references）</strong>，<strong>分支预测失败（branch-misses）</strong>，<strong>分支预测失败（branch-misses）</strong>，<code>perf</code> 通过 <strong>PMU</strong> 读取这些计数器，计算 <strong>CPI（Cycles Per Instruction）</strong>、<strong>缓存命中率</strong> 等指标，帮助分析程序瓶颈。</li><li>除了硬件事件，<code>perf</code> 还可以监控 <strong>内核/用户态软件事件</strong>：<strong>CPU 调度事件（context-switches, migrations）</strong>，<strong>缺页异常（page-faults）</strong>，<strong>系统调用（syscalls）</strong>，<strong>块设备 I/O（block:block_rq_issue</strong>，这些数据可以帮忙分析系统调用开销、调度延迟、I/O瓶颈等问题。</li><li><code>perf</code>支持<strong>动态探针（Dynamic Probes）</strong>，类似于<code>ftrace</code>和<code>eBPF</code>：<strong>kprobes</strong>：动态追踪内核函数，<strong>uprobes</strong>：动态追踪用户态函数，<strong>tracepoints</strong>：静态内核跟踪点（如 <code>sched:sched_switch</code>）。</li></ul><p>每隔一个固定时间，CPU上产生一个中断，Perf监控当前是哪个进程、哪个函数，然后给对应的进程和函数加一个统计值，这样就知道CPU有多少时间在某个进程或某个函数上。</p><p><img src="Perf%E5%8E%9F%E7%90%86%E5%9B%BE.jpg" alt="Perf原理图" /></p><h2 id="安装">安装</h2><h3 id="1-云端">1.云端</h3><figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs http">https://ui.perfetto.dev/<br></code></pre></td></tr></table></figure><p>Google浏览器访问上述链接，通过导入<code>.json</code>记录文件，生成性能火焰图</p><p><img src="Perf%E7%BD%91%E9%A1%B5%E7%AB%AF%E7%95%8C%E9%9D%A2.png" alt="Perf网页端界面" /></p><h3 id="2-本地">2. 本地</h3><ol><li>运行 <code>perf –version</code> 查看当前需要安装哪些库文件</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">(base) dongnan@server:~/GccTrace_ws/GccTrace/build$ perf --version<br>WARNING: perf not found <span class="hljs-keyword">for</span> kernel 5.4.0-150<br><br>  You may need to install the following packages <span class="hljs-keyword">for</span> this specific kernel:<br>    linux-tools-5.4.0-150-generic<br>    linux-cloud-tools-5.4.0-150-generic<br><br>  You may also want to install one of the following packages to keep up to <span class="hljs-built_in">date</span>:<br>    linux-tools-generic<br>    linux-cloud-tools-generic<br>    <br>    <br>    <br><span class="hljs-built_in">sudo</span> apt-get install linux-tools-5.4.0-150-generic linux-cloud-tools-5.4.0-150-generic linux-tools-generic linux-cloud-tools-generic<br></code></pre></td></tr></table></figure><ol start="2"><li>安装相应库文件 重新运行 perf –version 能够输出对应的版本信息</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">(base) dongnan@server:~/GccTrace_ws/GccTrace/build$ perf --version<br><br>perf version 5.4.233<br></code></pre></td></tr></table></figure><h2 id="使用">使用</h2><h3 id="1-实时分析">1.实时分析</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf top<br></code></pre></td></tr></table></figure><p>只能实时查看目前为止占用cpu较高的进程，不便于事后分析</p><p>-e：指定性能事件</p><p>-a：显示在所有CPU上的性能统计信息</p><p>-C：显示在指定CPU上的性能统计信息</p><p>-p：指定进程PID</p><p>-t：指定线程TID</p><p>-s：指定待解析的符号信息</p><p>-g: 记录函数的调用关系</p><p>执行下面命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> perf top -e task-clock -g -p 21437 -s <span class="hljs-built_in">comm</span>,dso,symbol<br><br><span class="hljs-comment">#comm：触发事件的进程名</span><br><span class="hljs-comment">#dso：Dynamic Shared Object，可以是应用程序、内核、动态链接库、模块</span><br><span class="hljs-comment">#symbol：函数名</span><br></code></pre></td></tr></table></figure><table><thead><tr><th><code>-e task-clock</code></th><th>监控 <strong>任务时钟（Task Clock）</strong> 事件，表示进程占用 CPU 的时间（单位：毫秒）。</th></tr></thead><tbody><tr><td><code>-g</code></td><td>记录 <strong>调用栈（Call Graph）</strong>，便于分析函数调用关系。</td></tr><tr><td><code>-p 21437</code></td><td>仅监控 <strong>进程 ID 为 21437</strong> 的进程。</td></tr><tr><td><code>-s comm,dso,symbol</code></td><td>显示字段： - <code>comm</code>：进程名。 - <code>dso</code>：动态共享对象（如 <code>libc.so</code>、可执行文件）。 - <code>symbol</code>：函数符号名（如 <code>malloc</code>、<code>main</code>）</td></tr></tbody></table><p><img src="Perf_top.png" alt="Perf_top" /></p><p>执行下面命令监控所有进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf top -e task-clock -g<br></code></pre></td></tr></table></figure><p><img src="%E7%9B%91%E6%8E%A7%E6%89%80%E6%9C%89%E8%BF%9B%E7%A8%8B.png" alt="监控所有进程" /></p><h3 id="2-生成数据文件后离线分析"><strong>2.生成数据文件后离线分析</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record<br></code></pre></td></tr></table></figure><p><code>perf record</code>用于 <strong>采集性能数据并保存到文件</strong>（默认生成 <code>perf.data</code>），便于后续离线分析（如 <code>perf report</code>、生成火焰图等</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record [选项] &lt;<span class="hljs-built_in">command</span>&gt;  <span class="hljs-comment"># 监控指定命令的执行</span><br></code></pre></td></tr></table></figure><table><thead><tr><th style="text-align:left">选项</th><th style="text-align:left">作用</th></tr></thead><tbody><tr><td style="text-align:left"><code>-e &lt;事件&gt;</code></td><td style="text-align:left">指定监控的事件（如 <code>cycles</code>, <code>cache-misses</code>, <code>task-clock</code>）。</td></tr><tr><td style="text-align:left"><code>-p &lt;PID&gt;</code></td><td style="text-align:left">监控指定进程（通过进程 ID）。</td></tr><tr><td style="text-align:left"><code>-F &lt;频率&gt;</code></td><td style="text-align:left">采样频率（Hz，如 <code>-F 99</code> 表示每秒采样 99 次）。</td></tr><tr><td style="text-align:left"><code>-g</code></td><td style="text-align:left">记录调用栈（用于生成火焰图）。</td></tr><tr><td style="text-align:left"><code>-a</code></td><td style="text-align:left">监控所有 CPU（系统级分析）。</td></tr><tr><td style="text-align:left"><code>-o &lt;文件&gt;</code></td><td style="text-align:left">指定输出文件（默认 <code>perf.data</code>）。</td></tr><tr><td style="text-align:left"><code>--call-graph &lt;方法&gt;</code></td><td style="text-align:left">调用栈记录方式（<code>fp</code>=帧指针，<code>dwarf</code>=调试信息）。</td></tr><tr><td style="text-align:left"><code>-s</code></td><td style="text-align:left">按线程分离统计。</td></tr><tr><td style="text-align:left"><code>-C &lt;CPU列表&gt;</code></td><td style="text-align:left">仅监控指定 CPU（如 <code>-C 0,1</code>）。</td></tr></tbody></table><h4 id="使用perf启动服务（不需要root权限）">使用perf启动服务（不需要root权限）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -g ./my_program  <span class="hljs-comment"># 监控程序运行，记录调用栈</span><br></code></pre></td></tr></table></figure><p>程序结束后生成 <code>perf.data</code>，用 <code>perf report</code> 分析</p><h4 id="挂接到已启动的进程（需要root权限）">挂接到已启动的进程（需要root权限）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -g -p &lt;PID&gt;      <span class="hljs-comment"># 监控指定进程</span><br></code></pre></td></tr></table></figure><p>按 <code>Ctrl+C</code> 停止采样</p><h4 id="指定采样事件">指定采样事件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -e cycles -g ./my_program   <span class="hljs-comment"># 监控 CPU 周期</span><br>perf record -e cache-misses -g ./my_program  <span class="hljs-comment"># 监控缓存失效</span><br></code></pre></td></tr></table></figure><p>可用事件列表：<code>perf list</code></p><h4 id="设置采样频率">设置采样频率</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -F 99 -g ./my_program  <span class="hljs-comment"># 每秒采样 99 次</span><br></code></pre></td></tr></table></figure><p>频率越高，数据越精确，但开销越大。</p><h4 id="系统级监控（所有-CPU）"><strong>系统级监控（所有 CPU）</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -ag                  <span class="hljs-comment"># 监控所有进程和 CPU</span><br>perf record -ag -e context-switches  <span class="hljs-comment"># 监控上下文切换</span><br></code></pre></td></tr></table></figure><h3 id="3-分析数据">3.分析数据</h3><ol><li>对于生成的.data目录可以直接运行 <code>perf report</code>查看相关报告</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf report                     <span class="hljs-comment"># 交互式查看热点</span><br>perf report --stdio             <span class="hljs-comment"># 命令行模式输出</span><br><br><span class="hljs-comment">#按函数/符号排序，显示 CPU 时间占比。</span><br></code></pre></td></tr></table></figure><ol start="2"><li>通过火焰图分析（需要用到<em><strong>FlameGraph</strong></em>工具，也可以直接使用网页端<a href="https://ui.perfetto.dev/%EF%BC%89">https://ui.perfetto.dev/）</a></li></ol><p>CPU火焰图中的每一个方框是一个函数，方框的长度，代表了它的执行时间，所以越宽的函数，执行越久。火焰图的楼层每高一层，就是更深一级的函数被调用，最顶层的函数，是叶子函数。</p><p><img src="%E7%81%AB%E7%84%B0%E5%9B%BE.png" alt="火焰图" /></p><ul><li>FlameGraph工具使用</li></ul><p>（1）安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> --depth 1 https://github.com/brendangregg/FlameGraph<br></code></pre></td></tr></table></figure><p>（2）使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl &gt; perf.svg<br><br>firefox perf.svg <span class="hljs-comment"># .svg文件可以通过浏览器打开</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Linux系统编程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch简明教程</title>
    <link href="/Diffcc/Diffcc.github.io/2025/06/07/20250607-2/"/>
    <url>/Diffcc/Diffcc.github.io/2025/06/07/20250607-2/</url>
    
    <content type="html"><![CDATA[<p>本文可以作为Pytorch初学者的快速上手文档，参考B站小土堆</p><span id="more"></span><h1>Pytorch</h1><h2 id="1-安装环境">1.安装环境</h2><p>==pyhton 3.6  Anaconda 3.5.2====CUDA+显卡驱动==</p><p><code>create -n pytorch python=3.6</code><code>conda activate pytorch</code><code>pip list</code></p><h2 id="2-Pytorch安装">2. Pytorch安装</h2><p><img src="pytorch%E4%B8%8B%E8%BD%BD.png" alt="pytorch下载" /></p><p>查看nvidia对应的Version</p><p><img src="CUDA%E7%89%88%E6%9C%AC.png" alt="CUDA版本" /></p><p>验证是否安装成功</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">python<br><span class="hljs-keyword">import</span> torch<br>torch.cuda.is_available()  <span class="hljs-comment">##显示是否可用GPU  True</span><br></code></pre></td></tr></table></figure><h2 id="3-Pycharm-Jupyter">3.Pycharm+Jupyter</h2><p><strong>Pycharm 配置pytorch</strong>python console 中运行(便于直接查看变量属性)</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clean"><span class="hljs-keyword">import</span> torch<br>torch.cuda.is_available()  ##显示是否可用GPU  <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><p><strong>Jupyter在pytorch环境中安装</strong></p><p><code>conda install -c conda-forge nb_conda</code><code>jupyter notebook</code><img src="Jupyter.png" alt="Jupyter" /></p><p>Jupyter中  shift+enter  跳转下一行代码</p><h2 id="4-Pytorch中两个重要函数">4.Pytorch中两个重要函数</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stylus">Pytorch就是一个package<br><br><span class="hljs-function"><span class="hljs-title">dir</span><span class="hljs-params">()</span></span>:打开，看见<br><span class="hljs-function"><span class="hljs-title">dir</span><span class="hljs-params">(pytorch)</span></span><br>输出：<span class="hljs-number">1</span>、<span class="hljs-number">2</span>、<span class="hljs-number">3</span>、<span class="hljs-number">4</span><br><span class="hljs-function"><span class="hljs-title">dir</span><span class="hljs-params">(pytorch.<span class="hljs-number">3</span>)</span></span><br>输出：<span class="hljs-selector-tag">a</span>,<span class="hljs-selector-tag">b</span>,c<br><br><br><span class="hljs-function"><span class="hljs-title">help</span><span class="hljs-params">()</span></span>:说明书<br><span class="hljs-function"><span class="hljs-title">help</span><span class="hljs-params">(pytorch.<span class="hljs-number">3</span>.a)</span></span><br>输出：a的具体功能<br></code></pre></td></tr></table></figure><p><img src="%E8%BE%93%E5%87%BA1.png" alt="输出1" /></p><h2 id="5-Pycharm对比Jupyter">5.Pycharm对比Jupyter</h2><p>同时在==python==文件、==Python控制台==、==Jupyter==运行一下代码：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Start！&quot;</span>)<br><span class="hljs-attribute">a</span>=<span class="hljs-string">&quot;hello world&quot;</span><br><span class="hljs-attribute">b</span>=2019<br><span class="hljs-attribute">c</span>=a+b<br><span class="hljs-built_in">print</span>(c)<br></code></pre></td></tr></table></figure><p>代码是以块为一个整体运行</p><p>Python文件：文件的块是所有行的代码，从头开始运行  适用于大型项目Python控制台：以任意行为块运行  但是阅读性不好  可以显示每个变量属性Jupyter：可以以任意代码块运行 利于代码的阅读与修改</p><p><img src="%E5%AF%B9%E6%AF%94.png" alt="对比" /></p><h2 id="6-Pytorch数据加载">6.Pytorch数据加载</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clean">Dataset  ##提供一种方式去获取数据及其label值<br>Dataloader   ##为网络提供不同的数据形式  Batchsize<br></code></pre></td></tr></table></figure><p><strong>Dataset</strong></p><ul><li>如何获取每一个数据及其label</li><li>告诉我们总共有多少数据</li></ul><p><strong>Jupyter可以方便查看Dataset功能</strong></p><p><img src="%E6%9F%A5%E7%9C%8BDataset.png" alt="查看Dataset" /></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ruby">from torch.utils.data import <span class="hljs-title class_">Dataset</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_">Dataset</span>):<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>.idx</span>)<br></code></pre></td></tr></table></figure><p><strong>read_data程序</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> dataset<br>form PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_ inherited__">dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,root_dir,label_dir</span>):<br>        <span class="hljs-variable language_">self</span>.root_dir=root_dir    <span class="hljs-comment">##初始中引入全局变量</span><br>        <span class="hljs-variable language_">self</span>.label_dir=label_dir   <span class="hljs-comment">##全局变量</span><br>        <span class="hljs-variable language_">self</span>.path=os.path.join(<span class="hljs-variable language_">self</span>.root_dir,<span class="hljs-variable language_">self</span>.label_dir)   <span class="hljs-comment">##变量地址相加获取Image地址</span><br>        <span class="hljs-variable language_">self</span>.image_path=os.listdir(<span class="hljs-variable language_">self</span>.path)  <span class="hljs-comment">##获取地址中的列表内容</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self,idx</span>):<br>        image_name=<span class="hljs-variable language_">self</span>.image_path[idx]   <span class="hljs-comment">##获取列表中名字</span><br>        image_item_path=os.path.join(<span class="hljs-variable language_">self</span>.path,image_name)  <span class="hljs-comment">##获取每张图片的地址（包括自身名字）</span><br>        img=Image.<span class="hljs-built_in">open</span>(image_item_path)  <span class="hljs-comment">##打开图片</span><br>        label=<span class="hljs-variable language_">self</span>.label_dir  <span class="hljs-comment">##获取标签</span><br>        <span class="hljs-keyword">return</span> img,label  <span class="hljs-comment">##返回图像与标签</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.image_path)  <span class="hljs-comment">##获取数据长度</span><br>    <br>root_dir=<span class="hljs-string">&quot;&quot;</span><br>ants_label_dir=<span class="hljs-string">&quot;&quot;</span><br>ants_dataset=MyData(root_dir,ants_label_dir)<br>        <br></code></pre></td></tr></table></figure><h2 id="7-TensorBoard-使用">7. TensorBoard 使用</h2><p>==探究模型不同阶段的输出图像效果==</p><style>.ctnxkpzjziac{zoom:80%;}</style><img src="Diffcc/2025/06/07/20250607-2/Tensorboard.png" class="ctnxkpzjziac" alt="Tensorboard"><p><code>writer.add_scalar()</code></p><p><img src="Tensorboard%E8%AE%BE%E7%BD%AE.png" alt="Tensorboard设置" /></p><p><strong>安装tensorboard</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs txt">conda activate pytorch<br>pip install tensorboard<br></code></pre></td></tr></table></figure><p><strong>运行tensorboard</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">tensorboard --logdir=logs --host=<span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span> <span class="hljs-comment">##logdir=logs文件也就是 Summarywriter（“logs”）中的文件地址</span><br>tensorboard --logdir=logs --host=<span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span> --port=<span class="hljs-number">6007</span>  <span class="hljs-comment">##定义端口名字 防止冲突</span><br></code></pre></td></tr></table></figure><p><img src="%E6%9C%AC%E5%9C%B0%E5%9C%B0%E5%9D%80.png" alt="本地地址" /></p><p><img src="%E8%BE%93%E5%87%BA2.png" alt="输出2" /></p><p>==注意：程序运行多次，logs文件中会生成不一样的图像  每向wirter中写入上一个程序 数据会保留====删除 logs==</p><p><strong>读取图像</strong></p><p><code>wirter.add-image()</code></p><p><img src="%E8%AF%BB%E5%8F%96%E5%9B%BE%E5%83%8F.png" alt="读取图像" /></p><p>==其中image读取类型为==</p><p><img src="%E5%9B%BE%E7%89%87%E7%B1%BB%E5%9E%8B.png" alt="图片类型" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">PIL中 Image.<span class="hljs-built_in">open</span>(path)读取的类型为<br>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;PIL.JpegImagePlugin.JpegImageFile&#x27;</span>&gt;<br><br>利用Opencv读取numpy类型<br><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image_path=<span class="hljs-string">&quot;相对路径&quot;</span><br>img=Image.<span class="hljs-built_in">open</span>(image_path)<br>img_narry=np.array(img)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(img_narry))<br>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;numpy.ndarray&#x27;</span>&gt;<br><span class="hljs-built_in">print</span>(img_array.shape)<br>(<span class="hljs-number">512</span>,<span class="hljs-number">768</span>,<span class="hljs-number">3</span>)   (HWC)   <span class="hljs-comment">## add_image默认dataformats为tensorboard的CHW</span><br><span class="hljs-comment">##使用numpy型需指定</span><br><br>writer=SUmmaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>writer.add_image(<span class="hljs-string">&quot;test&quot;</span>,img_array,<span class="hljs-number">1</span>(步骤此处为第一步)，dataformats=<span class="hljs-string">&#x27;HWC&#x27;</span>)<br>writer.close()<br></code></pre></td></tr></table></figure><p><img src="test.png" alt="test" /></p><h2 id="8-Transforms">8.Transforms</h2><h3 id="8-1-ToTensor">8.1 ToTensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchversion <span class="hljs-keyword">import</span> transforms  <span class="hljs-comment">#在transforms中alt+7  查看结构体</span><br></code></pre></td></tr></table></figure><p><strong>transforms结构及用法</strong></p><p>==利用其中的不同功能实现具体效果==</p><p><img src="transforms%E7%94%A8%E6%B3%95.png" alt="transforms用法" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><span class="hljs-keyword">from</span> torchversion <span class="hljs-keyword">import</span> transforms<br><br><span class="hljs-comment"># python的用法 -》tensor数据类型</span><br><span class="hljs-comment"># transforms.toTensor</span><br><span class="hljs-comment">#1、transforms该如何使用(python）</span><br><span class="hljs-comment">#2、为什么需要tensor数据类型</span><br><br>img_path=<span class="hljs-string">&quot;data/train/ants_image/5650366_e22b7e1065.jpg&quot;</span><br>img=Image.<span class="hljs-built_in">open</span>(img_path)<br><br><span class="hljs-comment">#1、transforms该如何使用(python）</span><br>tensor_trans=transforms.ToTensor()  <span class="hljs-comment">#利用classToTensor    创建自己的工具tensor_trans()</span><br>tensor_img=tensor_trans(img(Ctrl+P可以查看具体的所需数据类型))  <span class="hljs-comment">#使用自己的工具tensor_trans()转换成tensor类型图片</span><br><br><span class="hljs-comment">#2、为什么需要tensor数据类型</span><br>tensor 包括了神经网络中的参数类型 例如 梯度 反向传播 等<br><br>ToTensor包含将PIL和OPENCV读取的类型输入<br><br>writer=SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>writer.add_image(<span class="hljs-string">&quot;Tensor_image&quot;</span>,tensor_img)<br>writer.close()<br></code></pre></td></tr></table></figure><p><img src="Tensor_img.png" alt="Tensor_img" /></p><h3 id="8-2-常见的Transforms">8.2 常见的Transforms</h3><ul><li>输入： <em>PIL</em>   <em>Image.open()</em></li><li>输出：<em>tensor</em>      <em>ToTensor()</em></li><li>作用：<em>narrays</em>    <em>cv.imread()</em></li></ul><p><strong>Compose</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">Call的用法， 不需要再用.调用 直接括号调用即可<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Person</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self,name</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;__call__&quot;</span>+<span class="hljs-string">&quot; Hello &quot;</span>+name)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">hello</span>(<span class="hljs-params">self,name</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;hello&quot;</span>+ name)<br><br>person=Person()<br>person(<span class="hljs-string">&quot;zhangsan&quot;</span>)<br>person.hello(<span class="hljs-string">&quot;zhangsan&quot;</span>)<br><br>输出：<br>__call__ Hello zhangsan<br>hellozhangsan<br></code></pre></td></tr></table></figure><p><strong>ToTensor</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">将PIL读取文件 OPENCV读取文件转换成tensor<br><br>trans_totensor=transforms.ToTensor()<br>img_tensor=trans_totensor(img)<br></code></pre></td></tr></table></figure><p><strong>ToPILImage</strong></p><p><code>*&quot;&quot;&quot;Convert a tensor or an ndarray to PIL Image - this does not scale values.*</code></p><p><strong>Normalize</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;Normalize a tensor image with mean and standard deviation.</span><br><span class="hljs-string">This transform does not support PIL Image.</span><br><span class="hljs-string">Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n``</span><br><span class="hljs-string">channels, this transform will normalize each channel of the input</span><br><span class="hljs-string">``torch.*Tensor`` i.e.,</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">``output[channel] = (input[channel] - mean[channel]) / std[channel]``</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">归一化用来消除量纲影响</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">trans_norm=transforms.Normalize([<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>],[<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>])  <br><span class="hljs-comment">#三通道 mean[channel]=[0.5,0.5,0.5]  平均值</span><br><span class="hljs-comment">#std[channel]=[0.5,0.5,0.5]   标准差</span><br><span class="hljs-comment">#output[channel] = (input[channel] - mean[channel]) / std[channel]</span><br><span class="hljs-comment">#此处相当于  2*Input-1 = Output</span><br>img_norm=trans_norm(tensor_img)<br></code></pre></td></tr></table></figure><p><strong>Resize</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;Resize the input image to the given size.</span><br><span class="hljs-string">If the image is torch Tensor, it is expected</span><br><span class="hljs-string">to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions</span><br><span class="hljs-string"></span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">trans_resize=transforms.Resize((<span class="hljs-number">512</span>,<span class="hljs-number">512</span>))<br><span class="hljs-comment"># Resize 输入图片需要为PIL类型 否则会报错</span><br><br>img_resize=trans_resize(img)<br><br><span class="hljs-comment">#将PIL 再次转换成tensor</span><br>img_size=trans_totensor(img_resize)<br><br></code></pre></td></tr></table></figure><p><strong>compose_resize</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">compose利用call()循环其内的命令<br><br>trans_resize_2=transforms.Resize(<span class="hljs-number">512</span>)  <span class="hljs-comment">#单参数将最小的那个边变成此时的值</span><br><span class="hljs-comment">##Compose([a,b,c])  将参数传递给a，将a生成的输出传递给b，将b的输出传递给c，输出此时c的输出</span><br>trans_compose=transforms.Compose([trans_resize_2,trans_tensor])<span class="hljs-comment"># 同时实现了改变大小和将PIL文件转换成tensor文件</span><br>img_resize_2=trans_compose(img)<br></code></pre></td></tr></table></figure><p><strong>RandomCrop</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#随机裁剪</span><br><br>trans_Crop=transforms.RandomCrop(<span class="hljs-number">30</span>)  <span class="hljs-comment">#裁剪30*30的方形</span><br>trans_compose_2=transforms.Compose([trans_Crop,trans_tensor])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    img_randomcrop=trans_compose_2(img)<br>    writer.add_image(<span class="hljs-string">&quot;RandomCrop&quot;</span>,img_randomcrop,i)  <span class="hljs-comment">#随机从原图中裁剪30*30的十张图片</span><br></code></pre></td></tr></table></figure><ul><li><strong>关注输入与输出</strong></li><li><strong>学会查看官方文档</strong></li><li><strong>关注方法需要什么参数</strong></li><li><strong>不知道返回类型  <em>print</em>   <em>print(type)</em>  <em>debug</em></strong></li></ul><p>==在Tensorboard使用一定要转换成tensor数据类型==</p><h2 id="9-torchvision中的数据集的使用">9.torchvision中的数据集的使用</h2><p>torchvision 0.9.0版本数据集地址：<a href="https://pytorch.org/vision/0.9/">https://pytorch.org/vision/0.9/</a>.io  输入输出模块.models  常见的神经网络模块.ops  特殊操作.utils 常见工具(tensorboard).transforms</p><p>例如CIFAR数据集</p><p><img src="CIFAR10%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="CIFAR10数据集" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><br>train_set=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">True</span>,download=<span class="hljs-literal">True</span>)<br>test_set=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,download=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment">##可将下载链接拷贝到迅雷进行下载  下载完的数据集直接拷贝到自己新建的一个dataset文件夹，download自动设置为true，会自动校验下载和解压</span><br><br><span class="hljs-built_in">print</span>(test_set[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(test_set.classes)<br><br>img,target=test_set[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(img)<br><span class="hljs-built_in">print</span>(target)<br><span class="hljs-built_in">print</span>(test_set.classes[target])<br>img.show()<br><br>Files already downloaded <span class="hljs-keyword">and</span> verified<br>Files already downloaded <span class="hljs-keyword">and</span> verified<br>(&lt;PIL.Image.Image image mode=RGB size=32x32 at <span class="hljs-number">0x23C7C9B97E0</span>&gt;, <span class="hljs-number">3</span>)<br>[<span class="hljs-string">&#x27;airplane&#x27;</span>, <span class="hljs-string">&#x27;automobile&#x27;</span>, <span class="hljs-string">&#x27;bird&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;deer&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;frog&#x27;</span>, <span class="hljs-string">&#x27;horse&#x27;</span>, <span class="hljs-string">&#x27;ship&#x27;</span>, <span class="hljs-string">&#x27;truck&#x27;</span>]<br>&lt;PIL.Image.Image image mode=RGB size=32x32 at <span class="hljs-number">0x23C7C9B97E0</span>&gt;<br><span class="hljs-number">3</span><br>cat<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#转换成tensor类型</span><br><br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>dataset_transforms=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])<br><br>train_set=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">True</span>,transform=dataset_transforms,download=<span class="hljs-literal">True</span>)<br>test_set=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=dataset_transforms,download=<span class="hljs-literal">True</span>)<br><br>Writer=SummaryWriter(<span class="hljs-string">&quot;p10&quot;</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    img,target=test_set[i]<br>    Writer.add_image(<span class="hljs-string">&quot;test_set&quot;</span>,img,i)<br></code></pre></td></tr></table></figure><h2 id="10-Dataload的使用">10.Dataload的使用</h2><p>将dataset获取数据加载到神经网络之中</p><p>pytorch 1.8.1  ：<a href="https://pytorch.org/docs/1.8.1/data.html?highlight=dataloader#torch.utils.data.DataLoader">https://pytorch.org/docs/1.8.1/data.html?highlight=dataloader#torch.utils.data.DataLoader</a></p><p><img src="DataLoad.png" alt="DataLoad" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-comment">#数据集</span><br>test_dataset=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><span class="hljs-comment">#载入数据集</span><br>test_loader=DataLoader(dataset=test_dataset,batch_size=<span class="hljs-number">4</span>,shuffle=<span class="hljs-literal">True</span>,num_workers=<span class="hljs-number">0</span>,drop_last=<span class="hljs-literal">False</span>)<br><br>输出：<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>]) <span class="hljs-comment">#4张图片3通道32×32图片</span><br>tensor([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>])  <span class="hljs-comment">#target随机  默认随机策略</span><br><br><br><span class="hljs-comment">#batch_size=4 每次取出4张图片及其target 并打包成新的img，target</span><br><span class="hljs-comment">#shuffle=True  抓取完完整的一轮之后   之后一轮是否和前面一轮一样</span><br><span class="hljs-comment">#num_workers=0  仅仅主线程</span><br><span class="hljs-comment">#drop_last=False 不丢弃最后剩余的图片</span><br><br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">test_loader=DataLoader(dataset=test_dataset,batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-literal">True</span>,num_workers=<span class="hljs-number">0</span>,drop_last=<span class="hljs-literal">False</span>)<br><br>Writer=SummaryWriter(<span class="hljs-string">&quot;dataloader&quot;</span>)<br><br>step=<span class="hljs-number">0</span><br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_loader:<br>    imgs,targets=data<br>    <span class="hljs-comment"># print(imgs.shape)</span><br>    <span class="hljs-comment"># print(targets)</span><br>    Writer.add_images(<span class="hljs-string">&quot;test_data&quot;</span>,imgs,step)  <span class="hljs-comment">#此处为add_images  而非单个图片add_image</span><br>    step+=<span class="hljs-number">1</span><br><br>Writer.close()<br></code></pre></td></tr></table></figure><p><img src="Batch_size_64.png" alt="Batch_size_64" /></p><p><img src="batch_size_64%E4%BD%9916.png" alt="batch_size_64余16" /></p><p>==batch_size=64 每次抓取64张，最后一次剩余16张，由于drop_last设置为flase，即不丢弃最后的16张==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">test_loader=DataLoader(dataset=test_dataset,batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-literal">True</span>,num_workers=<span class="hljs-number">0</span>,drop_last=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):   <span class="hljs-comment">#epoch表示训练两轮  shuffle设置为True  即两轮不一样</span><br>    step = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_loader:<br>        imgs,targets=data<br>    <span class="hljs-comment"># print(imgs.shape)</span><br>    <span class="hljs-comment"># print(targets)</span><br>        Writer.add_images(<span class="hljs-string">&quot;Epoch:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch),imgs,step)  <span class="hljs-comment">#&quot;Epoch:&#123;&#125;&quot;.format(epoch) 表示出输出Epoch：0和Epoch：1</span><br>        <br>        step+=<span class="hljs-number">1</span><br>        <br>Writer.close()<br><br></code></pre></td></tr></table></figure><p><img src="epoch.png" alt="epoch" /></p><p><img src="epoch1.png" alt="epoch1" /></p><h2 id="11-神经网络的基本骨架—nn-module">11.神经网络的基本骨架—nn.module</h2><p><strong>torch.nn</strong>:<a href="https://pytorch.org/docs/1.8.1/nn.html">https://pytorch.org/docs/1.8.1/nn.html</a></p><p><strong>.Containers</strong>   神经网络骨架</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):  //Model为自定义的名字  括号内为调用的父类<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Model, <span class="hljs-variable language_">self</span>).__init__()  //调用nn.Mdoule 的初始化 重写nn.module的方法 可以用Ctrl+O 选择第一个<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)  //卷积<span class="hljs-number">1</span><br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)  //卷积<span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):   //前向传播  pytorch 根据前向传播能自动计算反向传播<br>        x = F.relu(<span class="hljs-variable language_">self</span>.conv1(x))  //conv1卷积<span class="hljs-number">1</span>  relu为非线性处理<br>        <span class="hljs-keyword">return</span> F.relu(<span class="hljs-variable language_">self</span>.conv2(x))//在进行一次卷积<span class="hljs-number">2</span> 非线性处理<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span>  nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NNtest</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,<span class="hljs-built_in">input</span></span>):<br>        output=<span class="hljs-built_in">input</span>+<span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> output<br><br>nntest=NNtest()  <span class="hljs-comment">##调用NNtest中的nn.module的 _init_方法</span><br>x=torch.tensor(<span class="hljs-number">1.0</span>)  <span class="hljs-comment">##x=tensor(1,)</span><br>output=nntest(x)  <span class="hljs-comment">##调用forward方法   nn.module中的__call__会自动把参数传递给forward进行处理</span><br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure><h2 id="12-卷积操作">12.卷积操作</h2><p><img src="conv.png" alt="conv" /></p><p><img src="%E5%8F%82%E6%95%B0.png" alt="参数" /></p><p><strong>stride</strong>：步长<strong>padding</strong>：填充操作（1，1）上下左右依次插入空行空列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span>  torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-built_in">input</span>=torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>],<br>                   [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>],<br>                   [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],<br>                   [<span class="hljs-number">5</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],<br>                   [<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]])<br><br>kernel=torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>],  //对应与weight<br>                    [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],<br>                    [<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]])<br><br><span class="hljs-comment">##conv2d的输入方式（minibatch，in_channel,iH,iW)  使用reshape进行尺寸改变  H代表行  W代表列</span><br><br><span class="hljs-built_in">input</span>=torch.reshape(<span class="hljs-built_in">input</span>,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">5</span>))<br>kernel=torch.reshape(kernel,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br><br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">input</span>.shape)<br><span class="hljs-built_in">print</span>(kernel.shape)<br><br>output=F.conv2d(<span class="hljs-built_in">input</span>,kernel,stride=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(output)<br><br><span class="hljs-comment">##padding操作</span><br>output2=F.conv2d(<span class="hljs-built_in">input</span>,kernel,stride=<span class="hljs-number">1</span>,padding=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br><span class="hljs-built_in">print</span>(output2)<br><br>结果：<br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br>tensor([[[[<span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>],<br>          [<span class="hljs-number">18</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>],<br>          [<span class="hljs-number">13</span>,  <span class="hljs-number">9</span>,  <span class="hljs-number">3</span>]]]])<br><br>tensor([[[[ <span class="hljs-number">1</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>, <span class="hljs-number">10</span>,  <span class="hljs-number">8</span>],<br>          [ <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>,  <span class="hljs-number">6</span>],<br>          [ <span class="hljs-number">7</span>, <span class="hljs-number">18</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>,  <span class="hljs-number">8</span>],<br>          [<span class="hljs-number">11</span>, <span class="hljs-number">13</span>,  <span class="hljs-number">9</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>],<br>          [<span class="hljs-number">14</span>, <span class="hljs-number">13</span>,  <span class="hljs-number">9</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">4</span>]]]])<br></code></pre></td></tr></table></figure><p><img src="%E8%BE%93%E5%87%BA3.png" alt="输出3" /></p><p><img src="%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png" alt="计算公式" /></p><h2 id="13-神经网络—卷积层">13.神经网络—卷积层</h2><p><a href="https://pytorch.org/docs/1.8.1/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">https://pytorch.org/docs/1.8.1/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</a></p><p><img src="conv2d.png" alt="conv2d" /></p><p><img src="conv2d%E5%8F%82%E6%95%B0.png" alt="conv2d参数" /></p><h4 id="参数解释">参数解释</h4><ul><li><strong>in_channels</strong>:(N,C~in~,H~in~,W~in~)  N=batch_size  C~in~为通道数  H~in~为行（高）  W~in~为宽（列）</li><li><strong>out_channels</strong>:(N,C~out~,H~out~,W~out~)  batch_size 始终不变</li><li><strong>kernel_size</strong>:(N,C,H,W)</li><li><strong>stride</strong>: 步长</li><li><strong>padding</strong>:填充（1，1）对输入图像周围填充一行一列</li><li><strong>padding_mode</strong>:’zeros’  填充0</li><li><strong>dilation</strong>：空洞卷积</li></ul><p><img src="%E5%8D%B7%E7%A7%AF%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="卷积示意图" /></p><p><img src="%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF.png" alt="空洞卷积" /></p><p><img src="%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF%E4%BD%9C%E7%94%A8.png" alt="空洞卷积作用" /></p><ul><li><p><strong>groups</strong>: 分组卷积：<a href="https://zhuanlan.zhihu.com/p/490685194">https://zhuanlan.zhihu.com/p/490685194</a></p></li><li><p><strong>bias</strong>：偏置  偏置等于卷积核的个数即kernel_size中（N,C,H,W)中的C</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>data_set=torchvison.dataset.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transforme=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br>dataloader=DataLoader(data_set,batch_size=<span class="hljs-number">64</span>,shuttle=<span class="hljs-literal">True</span>,Drop_last=<span class="hljs-literal">False</span>)<br><br>Class NNconv2d(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NNconv2d,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1=Conv2d(in_channels=<span class="hljs-number">3</span>,out_channels=<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">3</span>,stride=<span class="hljs-number">1</span>,padding=<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x=<span class="hljs-variable language_">self</span>.conv1(x)<br>        <span class="hljs-keyword">return</span> x<br><br>Myconv2d=NNconv2d()<br>step=<span class="hljs-number">0</span><br>writer=SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    outputs=Myconv2d(imgs)<br>    <span class="hljs-comment">##inputs.size(64,3,32,32)</span><br>    <span class="hljs-comment">##outputs.size(64,6,30,30)  由于tensorboard只能输出RGB三通道的图像  因此需要把6通道转换为3通道</span><br>    <span class="hljs-comment">##batch_size 设置为-1  自动根据后面设置的参数设置batch_size</span><br>    writer.add_images(<span class="hljs-string">&quot;Input&quot;</span>,imgs,step)<br>    outputs=torch.reshape(outputs,(-<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">30</span>,<span class="hljs-number">30</span>))<br>    writer.add_images(<span class="hljs-string">&quot;Output&quot;</span>,outputs,step)<br>    step+=<span class="hljs-number">1</span><br>    <br>writer.close()<br><br></code></pre></td></tr></table></figure><h2 id="14-神经网络—最大池化的使用">14.神经网络—最大池化的使用</h2><p>==最大池化取卷积核框内的最大值==</p><p><img src="MaxPool2d.png" alt="MaxPool2d" /></p><p><img src="MaxPool2d%E5%8F%82%E6%95%B0.png" alt="MaxPool2d参数" /></p><p><strong>ceil_mode</strong>:True为cell  即对于卷积核当中有空元素（0）的时候 选择保留其中剩下的最大值     默认为False</p><p>一般只需要设置<strong>kernel_size</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> MaxPool2d<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><span class="hljs-comment"># input=torch.tensor([[1,2,0,3,1],</span><br><span class="hljs-comment">#                     [0,1,2,3,1],</span><br><span class="hljs-comment">#                     [1,2,1,0,0],</span><br><span class="hljs-comment">#                     [5,2,3,1,1],</span><br><span class="hljs-comment">#                     [2,1,0,1,1]],dtype=torch.float32)  #将矩阵中的整形LONG转换成浮点型</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># input=torch.reshape(input,(-1,1,5,5))</span><br><span class="hljs-comment"># print(input.shape)</span><br>data_set=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br>dataloader=DataLoader(data_set,batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">nnMaxpool</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(nnMaxpool,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.maxpool=MaxPool2d(kernel_size=<span class="hljs-number">3</span>,ceil_mode=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x=<span class="hljs-variable language_">self</span>.maxpool(x)<br>        <span class="hljs-keyword">return</span> x<br><br>Max_pool=nnMaxpool()<br><span class="hljs-comment"># output=Max_pool(input)</span><br><span class="hljs-comment"># print(output)</span><br>writer=SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>step=<span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    outputs=Max_pool(imgs)<br>    writer.add_images(<span class="hljs-string">&quot;out_put_maxpoll&quot;</span>,outputs,step)<br>    step+=<span class="hljs-number">1</span><br><br>writer.close()<br><br><br><br>输出：<br>tensor([[[[<span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],<br>          [<span class="hljs-number">5.</span>, <span class="hljs-number">1.</span>]]]])<br><br><span class="hljs-comment">#池化的作用：大大减小训练量，加快训练类似于电影1080P转换成720P  但仍然能满足需求</span><br></code></pre></td></tr></table></figure><p><img src="%E8%BE%93%E5%87%BA4.png" alt="输出4" /></p><h2 id="15-神经网络—非线性激活">15.神经网络—非线性激活</h2><p>Padding Layers  几乎用不到  基本都可以通过卷积层中的padding实现</p><p>==Non—linear  Activations==</p><p><strong>Relu</strong></p><p><img src="ReLu1.png" alt="ReLu1" /></p><p><img src="ReLu2.png" alt="ReLu2" /></p><p><img src="ReLu3.png" alt="ReLu3" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span>  nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> ReLU<br><br><span class="hljs-built_in">input</span>=torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">0.5</span>],<br>                    [<span class="hljs-number">2</span>,-<span class="hljs-number">0.6</span>]])<br><br><span class="hljs-built_in">input</span>=torch.reshape(<span class="hljs-built_in">input</span>,(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NNRelu</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NNRelu,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.Relu1=ReLU()  <span class="hljs-comment">##无需输入</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x=<span class="hljs-variable language_">self</span>.Relu1(x)<br>        <span class="hljs-keyword">return</span> x<br><br>relu_results=NNRelu()<br>outputs=relu_results(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(outputs)<br></code></pre></td></tr></table></figure><p><strong>Sigmod</strong></p><p><img src="Sigmod1.png" alt="Sigmod1" /></p><p><img src="Sigmod2.png" alt="Sigmod2" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span>  torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span>  torch.nn <span class="hljs-keyword">import</span> Sigmoid<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>data_set=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br>dataloader=DataLoader(data_set,batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NNsigmoild</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NNsigmoild,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.Sig=Sigmoid()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,output</span>):<br>        output=<span class="hljs-variable language_">self</span>.Sig(output)<br>        <span class="hljs-keyword">return</span> output<br><br>Mysigmoid=NNsigmoild()<br>writer=SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>step=<span class="hljs-number">0</span><br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    outputs=Mysigmoid(imgs)<br>    writer.add_images(<span class="hljs-string">&quot;Output_sigmoid&quot;</span>,outputs,step)<br>    step+=<span class="hljs-number">1</span><br><br>writer.close()<br><br><br><br></code></pre></td></tr></table></figure><p>==非线性变换的目的：为模型引入非线性特征，提高模型的泛化能力==</p><h2 id="16-神经网络—线性层及其他层的介绍">16.神经网络—线性层及其他层的介绍</h2><p><a href="https://pytorch.org/docs/1.8.1/nn.html">https://pytorch.org/docs/1.8.1/nn.html</a></p><p><img src="%E5%85%B6%E4%BB%96%E5%B1%82.png" alt="其他层" /></p><p><strong>Linear(也就是常说的全连接层)</strong></p><p><img src="%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82.png" alt="全连接层" /></p><p><img src="Linear.png" alt="Linear" /></p><p><img src="Linear%E5%8F%82%E6%95%B0.png" alt="Linear参数" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">##将图片通过liner转换成1x3 </span><br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span>  torch.nn <span class="hljs-keyword">import</span> Linear<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>data_set=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br>dataloader=DataLoader(data_set,batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Haijun</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.Line=Linear(<span class="hljs-number">196608</span>,<span class="hljs-number">10</span>)  <span class="hljs-comment">#分别是输入和输出的样本量</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,<span class="hljs-built_in">input</span></span>):<br>        output=<span class="hljs-variable language_">self</span>.Line(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">return</span> output<br><br>Trans=Haijun()<br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    <span class="hljs-comment"># outputs=torch.reshape(imgs,(1,1,1,-1))</span><br>    outputs=torch.flatten(imgs)  <span class="hljs-comment">##flatten 可以直接展开一行操作  但是reshape可以指定大小</span><br>    <span class="hljs-built_in">print</span>(outputs.shape)<br>    outputs=Trans(outputs)<br>    <span class="hljs-built_in">print</span>(outputs.shape)<br></code></pre></td></tr></table></figure><h2 id="17-神经网络—搭建实战及Sequential序列的使用">17.神经网络—搭建实战及Sequential序列的使用</h2><p><strong>将神经网络各层连接</strong></p><p><img src="%E8%BF%9E%E6%8E%A5%E5%90%84%E5%B1%82.png" alt="连接各层" /></p><p><strong>CIFAR训练模型</strong></p><p><img src="%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.png" alt="训练模型" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Haijun</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># self.cov1=Conv2d(in_channels=3,out_channels=32,kernel_size=5,stride=1,padding=2)</span><br>        <span class="hljs-comment"># self.maxpool1=MaxPool2d(2)</span><br>        <span class="hljs-comment"># self.cov2=Conv2d(32,32,5,1,2)</span><br>        <span class="hljs-comment"># self.maxpool2=MaxPool2d(2)</span><br>        <span class="hljs-comment"># self.cov3=Conv2d(32,64,5,1,2)</span><br>        <span class="hljs-comment"># self.maxpool3=MaxPool2d(2)</span><br>        <span class="hljs-comment"># self.flatten=Flatten()</span><br>        <span class="hljs-comment"># self.Lin1=Linear(1024,64)</span><br>        <span class="hljs-comment"># self.Lin2=Linear(64,10)</span><br><br>        <br>        <span class="hljs-comment">##  使用Sequential可以很有效的简化代码</span><br>        <span class="hljs-variable language_">self</span>.model1=Sequential(<br>            Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Flatten(),<br>            Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>),<br>            Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br>    <span class="hljs-keyword">def</span>  <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,output</span>):<br>        <span class="hljs-comment"># output=self.cov1(output)</span><br>        <span class="hljs-comment"># output=self.maxpool1(output)</span><br>        <span class="hljs-comment"># output=self.cov2(output)</span><br>        <span class="hljs-comment"># output=self.maxpool2(output)</span><br>        <span class="hljs-comment"># output=self.cov3(output)</span><br>        <span class="hljs-comment"># output=self.maxpool3(output)</span><br>        <span class="hljs-comment"># output=self.flatten(output)</span><br>        <span class="hljs-comment"># output=self.Lin1(output)</span><br>        <span class="hljs-comment"># output=self.Lin2(output)</span><br><br>        output=<span class="hljs-variable language_">self</span>.model1(output)<br><br>        <span class="hljs-keyword">return</span> output<br><br>haijun=Haijun()<br><span class="hljs-built_in">input</span>=torch.ones((<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>))  <span class="hljs-comment">##测试用代码  生成64batch 3通道 32x32  其中元素都为1的矩阵</span><br>output=haijun(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(output.shape)<br><br><span class="hljs-comment">##利用TensorBoard可视化</span><br><br>writer=SummaryWriter(<span class="hljs-string">&quot;./logs_seq&quot;</span>)<br>writer.add_graph(haijun,<span class="hljs-built_in">input</span>)<br>writer.close()<br></code></pre></td></tr></table></figure><p><img src="%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt="可视化" /></p><h2 id="18-损失函数与反向传播">18.损失函数与反向传播</h2><p><strong>Loss Function作用：</strong>1、计算实际输出和目标之间的差距2、为我们更新输出提供一定的依据（反向传播）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> L1Loss<br><br>inputs=torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],dtype=torch.float32)<br>targets=torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>],dtype=torch.float32)<br><br>inputs=torch.reshape(inputs,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))<br>outputs=torch.reshape(targets,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))  <span class="hljs-comment">##inputs 和 outputs输入输出大小一定要相同</span><br><br><span class="hljs-comment">##L1 function  绝对值</span><br>loss=L1Loss()<br><span class="hljs-comment">##Mse  平方差</span><br>loss_mse=nn.MSELoss()<br><br>results=loss(inputs,outputs)<br><span class="hljs-built_in">print</span>(results)<br>results2=loss_mse(inputs,outputs)<br><span class="hljs-built_in">print</span>(results2)<br></code></pre></td></tr></table></figure><p><strong>交叉熵：Crossentropyloss   分类问题</strong></p><p><img src="%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.png" alt="交叉熵损失" /></p><p>分类问题输出：output[x1,x2,x3]  分别对应是每一类的概率</p><p>​Target=0（对应与第0类，对应与x1）只有当target完全命中的时候 -x[class]才会比较小  从而Loss比较小</p><p>交叉熵=—x1+ln(exp(x1)+exp(x2)+exp(x3))</p><p><img src="%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B12.png" alt="交叉熵损失2" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">x=torch.tensor([<span class="hljs-number">0.1</span>,<span class="hljs-number">0.2</span>,<span class="hljs-number">0.3</span>])<br>y=torch.tensor([<span class="hljs-number">1</span>])  <span class="hljs-comment">#(batch_size)</span><br>x=torch.reshape(x,(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))  <span class="hljs-comment">#转化为（batch_size,classes)</span><br>loss_cross=nn.CrossEntropyLoss()<br>result=loss_cross(x,y)<br><span class="hljs-built_in">print</span>(result)<br><br>结果：<br>tensor(<span class="hljs-number">1.1019</span>)<br><br><br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    outputs=haijun(imgs)<br>    results=loss(outputs,targets)<br>    <span class="hljs-built_in">print</span>(results)<br>    <br>    results.backword()  <span class="hljs-comment">#反向传播 生成梯度 后续利用优化器</span><br></code></pre></td></tr></table></figure><h2 id="19-优化器">19.优化器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, target <span class="hljs-keyword">in</span> dataset:<br>    optimizer.zero_grad()  <span class="hljs-comment">#梯度清0  防止梯度有初始值对下一轮训练造成影响</span><br>    output = model(<span class="hljs-built_in">input</span>)  <span class="hljs-comment">#模型训练的结果</span><br>    loss = loss_fn(output, target)  <span class="hljs-comment">#计算损失函数</span><br>    loss.backward()   <span class="hljs-comment">#误差反向传播 </span><br>    optimizer.step()  <span class="hljs-comment">#优化器根据梯度调整模型中的各项参数</span><br>    <br>    <br><span class="hljs-comment">##不同的优化器除了params(参数)，lr(学习速率)外还有许多其他要设置的参数，根据不同情况设置</span><br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#利用随机梯度下降</span><br><br>haijun=Haijun()<br>loss=nn.CrossEntropyLoss()<br>optimizer=optim.SGD(haijun.parameters(),lr=<span class="hljs-number">0.01</span>)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):  <span class="hljs-comment">#epoch为训练轮次</span><br>    running_loss=<span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>        imgs,targets=data<br>        outputs=haijun(imgs)<br>        loss_cross=loss(outputs,targets)<br>        optimizer.zero_grad()<br>        loss_cross.backward()<br>        optimizer.step()<br>        running_loss=running_loss+loss_cross  <br>    <span class="hljs-built_in">print</span>(running_loss)  <span class="hljs-comment">#每一轮总的loss和</span><br></code></pre></td></tr></table></figure><h2 id="20-现有模型的使用与修改">20.现有模型的使用与修改</h2><p>各模型地址：<a href="https://pytorch.org/vision/0.9/models.html#id2">https://pytorch.org/vision/0.9/models.html#id2</a></p><p>分类模型VGG举例(其预训练在ImageNet上进行)</p><p><img src="ImageNet.png" alt="ImageNet" /></p><p>ImageNet需要scipy，且目前已不支持download=True下载</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><br><span class="hljs-comment"># dataset=torchvision.datasets.ImageNet(&#x27;./dataset&#x27;,split=&#x27;train&#x27;,transform=torchvision.transforms.ToTensor(),download=True)</span><br><br>vgg16_false=torchvision.models.vgg16(weights=<span class="hljs-literal">None</span>)  <span class="hljs-comment">#只是加载网络模型  目前使用weights 而非download 默认为None 不适用预训练权重</span><br><span class="hljs-comment"># vgg16_true=torchvision.models.vgg16(download=True)    #从ImageNet上下载训练好的模型参数</span><br><br><span class="hljs-built_in">print</span>(vgg16_false)<br><br><br></code></pre></td></tr></table></figure><p><img src="%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.png" alt="网络模型" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#修改现有网络模型 以Vgg16举例</span><br><br><span class="hljs-comment"># 在vgg16中的classifier层中增加一层线性层</span><br>vgg16_false.classifier.add_module(<span class="hljs-string">&#x27;add_linear&#x27;</span>,nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">10</span>))<br><br><br><span class="hljs-comment">#修改vgg16中的calssifier层中最后一层线性层</span><br>vgg16_false.classifier[<span class="hljs-number">6</span>]=nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><h2 id="21-网络模型的保存与读取">21.网络模型的保存与读取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><br><span class="hljs-comment">#保存方式1</span><br>torch.save(vgg16,<span class="hljs-string">&quot;vgg16_model1.pth&quot;</span>)  <span class="hljs-comment">#保存的是模型的结构+参数</span><br>model1=torch.load(<span class="hljs-string">&quot;vgg16_model1.pth&quot;</span>)  <span class="hljs-comment">#加载</span><br><br><span class="hljs-comment">#保存方式2</span><br>torch.save(vgg16.state_dict(),<span class="hljs-string">&quot;vgg16_model2.pth&quot;</span>)  <span class="hljs-comment">#保存的仅仅是模型参数  以字典的形式</span><br>model2=torch.load(<span class="hljs-string">&quot;vgg16_model2.pth&quot;</span>) <span class="hljs-comment">#加载（字典结构显示）</span><br><br><span class="hljs-comment">#将字典加载成模型</span><br>vgg16=torchvision.models.vgg16(weights=<span class="hljs-literal">None</span>)<br>vgg16.load_state_dict(torch.load(<span class="hljs-string">&quot;vgg16_model2.pth&quot;</span>))  <span class="hljs-comment">#此时Vgg16显示的就是结构+参数</span><br><br></code></pre></td></tr></table></figure><h2 id="22-模型训练">22.模型训练</h2><p><strong>以CIFAR10为数据集</strong></p><p>==train.py==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#准备数据集</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> *<br><br>trian_data=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br>test_data=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment">#length长度</span><br>train_data_size=<span class="hljs-built_in">len</span>(trian_data)<br>test_data_size=<span class="hljs-built_in">len</span>(test_data)<br><br><span class="hljs-comment">#数据集长度</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练数据集长度：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(train_data_size))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;测试数据集长度：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(test_data_size))<br><br><span class="hljs-comment">#利用dataloader加载数据集</span><br>trian_dataloader=DataLoader(trian_data,batch_size=<span class="hljs-number">64</span>)<br>test_dataloader=DataLoader(test_data,batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-comment">#创建网络模型</span><br>Haijun=haijun()<br><br><span class="hljs-comment">#创建损失函数</span><br>loss_fn=nn.CrossEntropyLoss()<br><br><span class="hljs-comment">#优化器</span><br>learning_rate=<span class="hljs-number">1e-2</span><br>optim=torch.optim.SGD(Haijun.parameters(),lr=learning_rate)<br><br><span class="hljs-comment">#设置训练网络的一些参数</span><br>total_train_step=<span class="hljs-number">0</span>  <span class="hljs-comment">#训练次数</span><br>total_test_step=<span class="hljs-number">0</span>  <span class="hljs-comment">#测试次数</span><br>epoch=<span class="hljs-number">10</span>  <span class="hljs-comment">#训练轮数</span><br><br><span class="hljs-comment">#添加Tensorboard</span><br>writer=SummaryWriter(<span class="hljs-string">&quot;./Mylogs&quot;</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-------------------------第&#123;&#125;论训练开始----------------------------&quot;</span>.<span class="hljs-built_in">format</span>(i))<br><br>    <span class="hljs-comment">#训练步骤开始</span><br>    <span class="hljs-comment">#Haijun.train()  调用模型的特定结构  例如drop_out层  batchnorm层</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> trian_dataloader:<br>        imgs,targets=data<br>        outputs=Haijun(imgs)<br>        loss=loss_fn(outputs,targets)<br>        <span class="hljs-comment">#优化器优化模型</span><br>        optim.zero_grad()<br>        loss.backward()<br>        optim.step()<br><br>        total_train_step=total_train_step+<span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> total_train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练次数：&#123;&#125;，Loss=&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_train_step,loss.item()))  <span class="hljs-comment">##loss.item() 将tensor(1)转换成数字1显示</span><br>            writer.add_scalar(<span class="hljs-string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)<br><br>    <span class="hljs-comment">#测试步骤开始</span><br>    <span class="hljs-comment">#Haijun.eval()  调用模型结构  例如drop_out  batchnorm层</span><br>    total_test_loss=<span class="hljs-number">0</span><br>    total_accuacy=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment">#去除梯度的影响</span><br>        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_dataloader:<br>            imgs,targets=data<br>            outputs=Haijun(imgs)<br>            loss=loss_fn(outputs,targets)<br>            total_test_loss=total_test_loss+loss.item()<br>            accuacy=(outputs.argmax(<span class="hljs-number">1</span>)==targets).<span class="hljs-built_in">sum</span>()<br>            total_accuacy=total_accuacy+accuacy<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_test_loss))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的Accuacy：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_accuacy/test_data_size))<br>    writer.add_scalar(<span class="hljs-string">&quot;test_loss&quot;</span>,total_test_loss,total_test_step)<br>    writer.add_scalar(<span class="hljs-string">&quot;test_accuacy&quot;</span>,total_accuacy/test_data_size,total_test_step)<br>    total_train_step=total_train_step+<span class="hljs-number">1</span><br><br>    <span class="hljs-comment">#保存文件</span><br>    torch.save(Haijun,<span class="hljs-string">&quot;Haijun_&#123;&#125;.pth&quot;</span>.<span class="hljs-built_in">format</span>(i))<br>    <span class="hljs-comment">#torch.save(Haijun.state_dict(),&quot;Haijun_&#123;&#125;&quot;.format(i))  官方推荐的保存方式</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型已保存&quot;</span>)<br><br>writer.close()<br></code></pre></td></tr></table></figure><p>==model.py==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 搭建神经网络模型</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">haijun</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, output</span>):<br>        output = <span class="hljs-variable language_">self</span>.model(output)<br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    Haijun=haijun()<br>    <span class="hljs-built_in">input</span>=torch.ones((<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>))  <span class="hljs-comment">##64batch_size  3通道 32x32位  全为1的矩阵</span><br>    output=Haijun(<span class="hljs-built_in">input</span>)<br>    <span class="hljs-built_in">print</span>(output.shape)<br></code></pre></td></tr></table></figure><p><img src="train_loss.png" alt="train_loss" /></p><p><img src="testL_loss.png" alt="testL_loss" /></p><p><img src="test_accuacy.png" alt="test_accuacy" /></p><h2 id="23-CUDA加速">23.CUDA加速</h2><p>==第一种cpu加速方式：1、网络模型     2、数据（输入、标注）    3、损失函数    调用.cuda()==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#计算时间代码</span><br><span class="hljs-keyword">import</span> time  <span class="hljs-comment">#记时</span><br><br><span class="hljs-comment">#模型加速</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():  <span class="hljs-comment">#判断cuda()方法是否可用</span><br>    Haijun=Haijun.cuda()  <span class="hljs-comment">#网络模型的cuda方法</span><br>    <br><span class="hljs-comment">#损失函数加速</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    loss_fn=loss_fn.cuda()  <span class="hljs-comment">#损失函数的cuda()方法</span><br>    <br><span class="hljs-comment">#数据加速</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> trian_dataloader:<br>        imgs,targets=data<br>        <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>            imgs=imgs.cuda()  <span class="hljs-comment">#数据的cuda()方法</span><br>            targets=targets.cuda()<br></code></pre></td></tr></table></figure><p>时间对比：</p><p><img src="%E8%BE%93%E5%87%BA5.png" alt="输出5" /></p><p><img src="%E8%BE%93%E5%87%BA6.png" alt="输出6" /></p><p>利用Goole colab云GPU加速</p><p><a href="https://colab.research.google.com/">https://colab.research.google.com/</a></p><p><code>!nvidia-smi</code></p><p><img src="%E8%BE%93%E5%87%BA7.png" alt="输出7" /></p><p><img src="CUDA.png" alt="CUDA" /></p><p>==第二种cpu加速方式：1、网络模型     2、数据（输入、标注）    3、损失函数==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#调用CPU</span><br>device=torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)  <span class="hljs-comment">#定义训练的设备</span><br>device=torch.device(<span class="hljs-string">&quot;cuda&quot;</span><span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span><span class="hljs-string">&quot;cpu&quot;</span>)  <span class="hljs-comment">#有cuda用cuda  无cuda用cpu</span><br><br>device=torch.device(<span class="hljs-string">&quot;cuda&quot;</span>)<br>       torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span>)  <span class="hljs-comment">#指定第一张显卡</span><br>       torch.device(<span class="hljs-string">&quot;cuda:1&quot;</span>)  <span class="hljs-comment">#指定第二张显卡</span><br><br>Haijun.to(device)<br>loss_fn.to(device)<br><span class="hljs-comment">#针对非数据类型 上述写法即可不需要赋值操作  下面的写法也正确</span><br><span class="hljs-comment">#Haijun=Haijun.to(device)</span><br><span class="hljs-comment">#loss_fn=loss_fn.to(device)  </span><br><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> trian_dataloader:<br>        imgs,targets=data<br>        imgs=imgs.to(device)  <span class="hljs-comment">#数据的cuda()方法</span><br>        targets=targets.to(device)<br></code></pre></td></tr></table></figure><h2 id="24-完整的模型验证demo">24.完整的模型验证demo</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br>image_path=<span class="hljs-string">&quot;./data/1.png&quot;</span><br>image=Image.<span class="hljs-built_in">open</span>(image_path)<br><span class="hljs-built_in">print</span>(Image)<br><br>image=image.convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)  <span class="hljs-comment">#png多一个透明通道  转换成三通道</span><br><br>transform=torchvision.transforms.Compose([torchvision.transforms.Resize((<span class="hljs-number">32</span>,<span class="hljs-number">32</span>)),<br>                                          torchvision.transforms.ToTensor()])<br><br><br>image=transform(image)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">haijun</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, output</span>):<br>        output = <span class="hljs-variable language_">self</span>.model(output)<br>        <span class="hljs-keyword">return</span> output<br>Haijun=haijun()<br><br>model=torch.load(<span class="hljs-string">&quot;Haijun_9.pth&quot;</span>)<br><span class="hljs-comment">#model=torch.load(&quot;Haijun_9.pth&quot;,map_location=torch.device(&#x27;cpu&#x27;))  #GPU训练的Haijun_9.pth  yaozai</span><br><span class="hljs-built_in">print</span>(model)<br><br>image=torch.reshape(image,(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>))<br>model.<span class="hljs-built_in">eval</span>()<br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    output=model(image)<br><span class="hljs-built_in">print</span>(output)<br><br><span class="hljs-built_in">print</span>(output.argmax(<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Linux/UNIX文件系统</title>
    <link href="/Diffcc/Diffcc.github.io/2025/06/07/20250607/"/>
    <url>/Diffcc/Diffcc.github.io/2025/06/07/20250607/</url>
    
    <content type="html"><![CDATA[<p><img src="linux.jpg" alt="linux" /></p><p>文件系统给是对文件和目录的组织集合，</p><p>本文探究<strong>Linux</strong>下的设备文件系统，ext系列系统，日志文件系统设计</p><span id="more"></span><h2 id="设备文件系统">设备文件系统</h2><h3 id="设备文件">设备文件</h3><p>在Linux内核中，每种设备类型（真正存在or虚拟抽象）的都有与之对应的设备驱动程序，内核通过<strong>设备驱动程序</strong>的API接口，实现对于设备的操控，虽然每个设备都有差异，但是接口类似，进而能够很好的承接系统调用本身。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">/dev <span class="hljs-comment">#dev目录下设备文件</span><br><br><span class="hljs-comment">#超级用户使用mknod来创建设备文件</span><br></code></pre></td></tr></table></figure><p><img src="dev.png" alt="dev" /></p><p>输入输出设备可分为两大类：<strong>块设备（<em>Block Device</em>）<strong>和</strong>字符设备（<em>Character Device</em>）</strong></p><ul><li>字符型设备基于每个字符来处理数据。终端、键盘、鼠标都属于字符型设备。</li><li>块设备每次处理一块设备。块的大小取决于设备类型，硬盘，USB都属于块设备。</li></ul><h3 id="磁盘设备">磁盘设备</h3><p>磁盘由盘面，磁道，柱面和扇区构成，数据的读/写按柱面进行，而不按盘面进行。也就是说，一个磁道写满数据后，就在同一柱面的下一个盘面来写，<em><strong>*一个柱面写满后，才移到下一个扇区开始写数据*</strong></em>。读数据也按照这种方式进行，这样就提高了硬盘的读/写效率。</p><p><img src="disk.png" alt="disk" /></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">fdisk -l <span class="hljs-comment">#列出所有磁盘的分区</span><br></code></pre></td></tr></table></figure><p>磁盘分区可以容纳任何类型的信息，但通常只包含<strong>下面之一</strong>：</p><ul><li>文件系统：存放常规文件。</li><li>数据区域： 充当裸设备。</li><li>交换区域：内存管理之用 特权进程通过***swapon()<em><strong>和</strong></em>swapoff()***启动和停止磁盘分区操作。</li></ul><p>详情可参考：<a href="https://blog.csdn.net/hguisu/article/details/7408047">https://blog.csdn.net/hguisu/article/details/7408047</a></p><h3 id="ext系列文件系统">ext系列文件系统</h3><p>**ext2（second extended file system）**扩展文件系统二世是Linux上使用最广泛的文件系统</p><p>ext2文件系统以<em><strong>block</strong></em>为基本单位，包括引导块，超级块，i节点表和数据块部分：</p><ul><li>引导块可以理解为Linux系统的<strong>init()</strong>，用来引导操作系统的信息。</li><li><strong>超级块</strong>则是标记了i节点表的大小，逻辑块的大小信息。</li><li><em><strong>i-list</strong></em>维护了文件类型、属主、硬链接数、<strong>指向文件数据块</strong>的指针等信息，在ext2中，每个i节点包括了15个指针，前12个指针用于直接索引文件数据库的位置，保证直接访问一击必中，后四个文件块通过指向一个<strong>指针块</strong>，分化为指向更多的数据块，以保证容纳大体量的文件，同时可以将未指向数据块的指针块中的指针标记未0，则无需未<strong>文件黑洞</strong>分配空字节数据块。</li><li><strong>数据块</strong>构成了文件和目录，用于存放数据。</li></ul><p><img src="inode.png" alt="inode" /></p><p><img src="ext2.png" alt="ext2" /></p><p><strong>ext3</strong>文件系统允许<code>journaling</code>日志，<code>journaling</code>日志是在文件系单独的区域存储，每当文件系统意外崩溃，采用<code>journaling</code>日志可以进行恢复，<strong>和ext2文件系统不同的是多出了<code>journaling</code>日志的功能</strong>。</p><ul><li>ext3提供三种journal日志模式，分别是<code>writeback</code>、<code>ordered</code>、<code>journal</code>.<code>writeback</code>仅仅会记录元数据的日志，数据可以直接写到磁盘，但是不保证数据比元数据先落盘，这也是性能最好的，但是<a href="https://cloud.tencent.com/product/dsgc?from_column=20065&amp;from=20065">数据安全</a>性最低；<code>ordered</code>也是仅仅是记录元数据块的日志，这种模式是将文件的数据相关的元数据和数据在一个事务中，当元数据写入到磁盘时候，把对应的数据也写到磁盘，这样是先数据刷盘，再做元数据日志。<code>journal</code>提供数据和元数据的日志，所有的文件数据都先写到日志，然后在写到磁盘，数据需要写2次，性能是最差的。</li><li>ext2中在目录项中查找文件时间的复杂度是<code>O(n)</code>，ext3中采用了<code>h-trees</code>查找效率提高了很多</li></ul><p><img src="ext3.png" alt="ext3" /></p><p><strong>ext4</strong>是从ext3 fork而来,针对ext4最大的feature就是ext4采用<code>extents</code>机制，替代了<code>indirect block</code>寻址的方式。ext4尽量会把<a href="https://cloud.tencent.com/product/cos?from_column=20065&amp;from=20065">数据存储</a>在连续的block区域内，为了实现这个ext4需要知道三个信息，第一是文件的初始化block.其次是块的数量，最后是磁盘上初始化块的数据，这些信息统一抽象以<code>struct ext4_extent</code>呈现。</p><p><img src="ext4.png" alt="ext4" /></p><p>详情可参考：<a href="https://cloud.tencent.com/developer/article/2074590">https://cloud.tencent.com/developer/article/2074590</a></p><h3 id="虚拟文件系统">虚拟文件系统</h3><p>除了ext系列文件系统，还有诸如Reiserfs，VFAT，NFS等文件系统，为了能和各种文件系统打交道，应用程序通过<strong>虚拟文件系统（VFS</strong>）这一层抽象层中定义的通用接口（<em><strong>open()、read()、write()、lseek()</strong></em>….），来实现对不同文件系统的访问。</p><p>不同的文件系统对比可参考：<a href="https://zhuanlan.zhihu.com/p/689551298">https://zhuanlan.zhihu.com/p/689551298</a></p><p><img src="%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94.png" alt="文件系统性能对比" /></p><h3 id="日志文件系统">日志文件系统</h3><p><strong>日志文件系统</strong>的作用在于避免了系统崩溃之后，为了确保文件系统的完整，需要遍历整个文件系统，来检查一致性（<em><strong>fcsk</strong></em>）。</p><p>日志文件系统会将更新操作记录在专门的磁盘日志文件中，可利用日志重做（<em><strong>redo</strong></em>）任何不完整更新（日志系统总能够保证将<strong>文件元数据事务</strong>作为一个完整单元提交至磁盘）。</p><h3 id="文件挂载">文件挂载</h3><p>文件挂载（Mount）的作用是将存储设备（如硬盘分区、光盘、网络存储等）或虚拟文件系统（如<code>proc</code>、<code>sysfs</code>等）关联到目录树的某个位置（挂载点），使得用户和程序可以<strong>通过文件系统接口访问这些资源</strong>。</p><p><strong>Linux/UNIX</strong>所有的文件系统中的文件都位于单根目录树下（<em><strong>”/“</strong></em>），其他的文件系统都挂载在根目录下，被视为整个目录层级的子树（<em><strong>subtree</strong></em>）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 将device文件系统挂载在directory所指定的目录下</span><br>mount device directory<br><br><span class="hljs-comment"># mount用于列出当前已挂载的文件系统，unmount用于卸载</span><br>mount<br></code></pre></td></tr></table></figure><p><img src="mount.png" alt="mount" /></p><p>用于查看当前已挂载和可挂载的文件信息：<code>/proc/mounts</code>，<code>/etc/mtab</code>，<code>/etc/fstab</code></p><p><img src="proc_mounts.png" alt="proc_mounts" /></p><ul><li><strong>文件系统类型</strong>: <code>sysfs</code>（虚拟文件系统，用于导出内核对象信息）</li><li><strong>挂载点</strong>: <code>/sys</code></li><li><strong>文件系统类型</strong>: <code>sysfs</code></li><li><strong>挂载标志</strong>:<ul><li><code>rw</code>: 以读写方式挂载</li><li><code>nosuid</code>: 忽略文件系统中的SUID/SGID权限位（禁用setuid/setgid效果）</li><li><code>nodev</code>: 不允许解释文件系统上的设备文件（禁用设备文件）</li><li><code>noexec</code>: 禁止直接执行该文件系统上的程序</li><li><code>relatime</code>: 仅在访问时间早于修改时间时更新访问时间（优化性能）</li></ul></li><li><strong>数字字段</strong>:<ul><li><code>0</code>: dump备份标志（0表示不备份）</li><li><code>0</code>: fsck检查顺序（0表示不检查）</li></ul></li></ul><h4 id="高级挂载特性">高级挂载特性</h4><p><strong>多个挂载点挂在文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount /dev/sda12 /test1<br>mount /dev/sda12 /test2<br><br><span class="hljs-comment">#在test1挂载点下操作，test2下完全可见</span><br></code></pre></td></tr></table></figure><p><strong>多次挂载同一挂载点</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">每次新挂载会隐藏之前可见于挂载点下的目录子树<br>mount /dev/sda12 /testfs<br>mount /dev/sda12 /testfs<br><br><span class="hljs-comment">#卸载最后一次挂载，上次挂载内容可见</span><br></code></pre></td></tr></table></figure><p><strong>基于每次挂载的挂载标志</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#mountflag可以基于每次挂载来设置</span><br></code></pre></td></tr></table></figure><p><strong>绑定挂载</strong></p><p><strong>两处同时可见，类似于硬链接，但可以针对目录</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">MS_BIND<br>mount --<span class="hljs-built_in">bind</span><br></code></pre></td></tr></table></figure><p><strong>递归绑定挂载</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">MS_BIND OR MS_REC<br></code></pre></td></tr></table></figure><h3 id="虚拟内存文件系统-tmpfs">虚拟内存文件系统: tmpfs</h3><p>tmpfs 是 Linux 内核中的一个虚拟文件系统，它将数据<strong>存储在内存中</strong>而不是硬盘上。使用 tmpfs 可以快速访问数据，因为数据存储在 <em><strong>RAM</strong></em> 中，而不需要进行实际的磁盘 <em><strong>I/O</strong></em> 操作。</p><p>可使用如下命令挂载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Bash">mount -t tmpfs <span class="hljs-built_in">source</span> target <span class="hljs-comment">#source:名称 target:挂载点</span><br></code></pre></td></tr></table></figure><p>除了用于用户应用程序之外，tmpfs文件系统还有下面两个用途:</p><ul><li>由内核内部挂载隐形tmpfs文件系统，用于实现<em><strong>System V</strong></em>共享内存和共享匿名内存映射</li><li>挂在于***/dev/shm*** 的tmpfs文件系统 ，为<em><strong>glibc</strong></em>用以实现POSIX共享内存和POSIX信号量</li></ul><p>系统可通过<code>statvfs()</code> 和 <code>fstatvfs()</code>库函数获取已挂载文件系统的有关信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#include &lt;sys/statvfs.h&gt;</span><br><br>int statvfs(const char*pathname, struct statvfs* statvfsbuf);<br>int fstatvfs(int fd, struct statvfs* statvfsbuf);<br><br><span class="hljs-comment">#statvfsbuf为指向statvfs结构体的缓冲区，内含文件系统的信息</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Linux系统编程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
