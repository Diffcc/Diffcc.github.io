<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Perf Linux使用教程</title>
    <link href="/Diffcc/Diffcc.github.io/2025/06/10/20250610/"/>
    <url>/Diffcc/Diffcc.github.io/2025/06/10/20250610/</url>
    
    <content type="html"><![CDATA[<p><code>perf</code> 是 Linux 内核提供的一个强大的性能分析工具，基于 <strong>硬件性能计数器（Performance Monitoring Counters, PMC）</strong> 和 <strong>内核事件采样</strong> 来监控系统性能。</p><span id="more"></span><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>Linux性能计数器是一个基于内核的子系统，它提供一个性能分析框架，比如硬件**（CPU、PMU（Performance Monitoring Unit））<strong>功能和软件</strong>（软件计数器、tracepoint）**功能。通过perf，应用程序可以利用PMU、tracepoint和内核中的计数器来进行性能统计。</p><ul><li>现代 CPU（如 Intel、AMD、ARM）都内置了 <strong>性能监控单元（PMU, Performance Monitoring Unit）</strong>，可以统计各种硬件事件：<strong>CPU 周期数（cycles）</strong>，<strong>指令数（instructions）</strong>，<strong>缓存命中&#x2F;失效（cache-misses, cache-references）</strong>，<strong>分支预测失败（branch-misses）</strong>，<strong>分支预测失败（branch-misses）</strong>，<code>perf</code> 通过 <strong>PMU</strong> 读取这些计数器，计算 <strong>CPI（Cycles Per Instruction）</strong>、<strong>缓存命中率</strong> 等指标，帮助分析程序瓶颈。</li><li>除了硬件事件，<code>perf</code> 还可以监控 <strong>内核&#x2F;用户态软件事件</strong>：<strong>CPU 调度事件（context-switches, migrations）</strong>，<strong>缺页异常（page-faults）</strong>，<strong>系统调用（syscalls）</strong>，<strong>块设备 I&#x2F;O（block:block_rq_issue</strong>，这些数据可以帮忙分析系统调用开销、调度延迟、I&#x2F;O瓶颈等问题。</li><li><code>perf</code>支持<strong>动态探针（Dynamic Probes）</strong>，类似于<code>ftrace</code>和<code>eBPF</code>：<strong>kprobes</strong>：动态追踪内核函数，<strong>uprobes</strong>：动态追踪用户态函数，<strong>tracepoints</strong>：静态内核跟踪点（如 <code>sched:sched_switch</code>）。</li></ul><p>每隔一个固定时间，CPU上产生一个中断，Perf监控当前是哪个进程、哪个函数，然后给对应的进程和函数加一个统计值，这样就知道CPU有多少时间在某个进程或某个函数上。</p><p><img src="Perf%E5%8E%9F%E7%90%86%E5%9B%BE.jpg" alt="Perf原理图"></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="1-云端"><a href="#1-云端" class="headerlink" title="1.云端"></a>1.云端</h3><figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs http">https://ui.perfetto.dev/<br></code></pre></td></tr></table></figure><p>Google浏览器访问上述链接，通过导入<code>.json</code>记录文件，生成性能火焰图</p><p><img src="Perf%E7%BD%91%E9%A1%B5%E7%AB%AF%E7%95%8C%E9%9D%A2.png" alt="Perf网页端界面"></p><h3 id="2-本地"><a href="#2-本地" class="headerlink" title="2. 本地"></a>2. 本地</h3><ol><li>运行 <code>perf –version</code> 查看当前需要安装哪些库文件</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">(base) dongnan@server:~/GccTrace_ws/GccTrace/build$ perf --version<br>WARNING: perf not found <span class="hljs-keyword">for</span> kernel 5.4.0-150<br><br>  You may need to install the following packages <span class="hljs-keyword">for</span> this specific kernel:<br>    linux-tools-5.4.0-150-generic<br>    linux-cloud-tools-5.4.0-150-generic<br><br>  You may also want to install one of the following packages to keep up to <span class="hljs-built_in">date</span>:<br>    linux-tools-generic<br>    linux-cloud-tools-generic<br>    <br>    <br>    <br><span class="hljs-built_in">sudo</span> apt-get install linux-tools-5.4.0-150-generic linux-cloud-tools-5.4.0-150-generic linux-tools-generic linux-cloud-tools-generic<br></code></pre></td></tr></table></figure><ol start="2"><li>安装相应库文件 重新运行 perf –version 能够输出对应的版本信息</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">(base) dongnan@server:~/GccTrace_ws/GccTrace/build$ perf --version<br><br>perf version 5.4.233<br></code></pre></td></tr></table></figure><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="1-实时分析"><a href="#1-实时分析" class="headerlink" title="1.实时分析"></a>1.实时分析</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf top<br></code></pre></td></tr></table></figure><p>只能实时查看目前为止占用cpu较高的进程，不便于事后分析</p><p>-e：指定性能事件</p><p>-a：显示在所有CPU上的性能统计信息</p><p>-C：显示在指定CPU上的性能统计信息</p><p>-p：指定进程PID</p><p>-t：指定线程TID</p><p>-s：指定待解析的符号信息</p><p>-g: 记录函数的调用关系</p><p>执行下面命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> perf top -e task-clock -g -p 21437 -s <span class="hljs-built_in">comm</span>,dso,symbol<br><br><span class="hljs-comment">#comm：触发事件的进程名</span><br><span class="hljs-comment">#dso：Dynamic Shared Object，可以是应用程序、内核、动态链接库、模块</span><br><span class="hljs-comment">#symbol：函数名</span><br></code></pre></td></tr></table></figure><table><thead><tr><th><code>-e task-clock</code></th><th>监控 <strong>任务时钟（Task Clock）</strong> 事件，表示进程占用 CPU 的时间（单位：毫秒）。</th></tr></thead><tbody><tr><td><code>-g</code></td><td>记录 <strong>调用栈（Call Graph）</strong>，便于分析函数调用关系。</td></tr><tr><td><code>-p 21437</code></td><td>仅监控 <strong>进程 ID 为 21437</strong> 的进程。</td></tr><tr><td><code>-s comm,dso,symbol</code></td><td>显示字段： - <code>comm</code>：进程名。 - <code>dso</code>：动态共享对象（如 <code>libc.so</code>、可执行文件）。 - <code>symbol</code>：函数符号名（如 <code>malloc</code>、<code>main</code>）</td></tr></tbody></table><p><img src="Perf_top.png" alt="Perf_top"></p><p>执行下面命令监控所有进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf top -e task-clock -g<br></code></pre></td></tr></table></figure><p><img src="%E7%9B%91%E6%8E%A7%E6%89%80%E6%9C%89%E8%BF%9B%E7%A8%8B.png" alt="监控所有进程"></p><h3 id="2-生成数据文件后离线分析"><a href="#2-生成数据文件后离线分析" class="headerlink" title="2.生成数据文件后离线分析"></a><strong>2.生成数据文件后离线分析</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record<br></code></pre></td></tr></table></figure><p><code>perf record</code>用于 <strong>采集性能数据并保存到文件</strong>（默认生成 <code>perf.data</code>），便于后续离线分析（如 <code>perf report</code>、生成火焰图等</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record [选项] &lt;<span class="hljs-built_in">command</span>&gt;  <span class="hljs-comment"># 监控指定命令的执行</span><br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">选项</th><th align="left">作用</th></tr></thead><tbody><tr><td align="left"><code>-e &lt;事件&gt;</code></td><td align="left">指定监控的事件（如 <code>cycles</code>, <code>cache-misses</code>, <code>task-clock</code>）。</td></tr><tr><td align="left"><code>-p &lt;PID&gt;</code></td><td align="left">监控指定进程（通过进程 ID）。</td></tr><tr><td align="left"><code>-F &lt;频率&gt;</code></td><td align="left">采样频率（Hz，如 <code>-F 99</code> 表示每秒采样 99 次）。</td></tr><tr><td align="left"><code>-g</code></td><td align="left">记录调用栈（用于生成火焰图）。</td></tr><tr><td align="left"><code>-a</code></td><td align="left">监控所有 CPU（系统级分析）。</td></tr><tr><td align="left"><code>-o &lt;文件&gt;</code></td><td align="left">指定输出文件（默认 <code>perf.data</code>）。</td></tr><tr><td align="left"><code>--call-graph &lt;方法&gt;</code></td><td align="left">调用栈记录方式（<code>fp</code>&#x3D;帧指针，<code>dwarf</code>&#x3D;调试信息）。</td></tr><tr><td align="left"><code>-s</code></td><td align="left">按线程分离统计。</td></tr><tr><td align="left"><code>-C &lt;CPU列表&gt;</code></td><td align="left">仅监控指定 CPU（如 <code>-C 0,1</code>）。</td></tr></tbody></table><h4 id="使用perf启动服务（不需要root权限）"><a href="#使用perf启动服务（不需要root权限）" class="headerlink" title="使用perf启动服务（不需要root权限）"></a>使用perf启动服务（不需要root权限）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -g ./my_program  <span class="hljs-comment"># 监控程序运行，记录调用栈</span><br></code></pre></td></tr></table></figure><p>程序结束后生成 <code>perf.data</code>，用 <code>perf report</code> 分析</p><h4 id="挂接到已启动的进程（需要root权限）"><a href="#挂接到已启动的进程（需要root权限）" class="headerlink" title="挂接到已启动的进程（需要root权限）"></a>挂接到已启动的进程（需要root权限）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -g -p &lt;PID&gt;      <span class="hljs-comment"># 监控指定进程</span><br></code></pre></td></tr></table></figure><p>按 <code>Ctrl+C</code> 停止采样</p><h4 id="指定采样事件"><a href="#指定采样事件" class="headerlink" title="指定采样事件"></a>指定采样事件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -e cycles -g ./my_program   <span class="hljs-comment"># 监控 CPU 周期</span><br>perf record -e cache-misses -g ./my_program  <span class="hljs-comment"># 监控缓存失效</span><br></code></pre></td></tr></table></figure><p>可用事件列表：<code>perf list</code></p><h4 id="设置采样频率"><a href="#设置采样频率" class="headerlink" title="设置采样频率"></a>设置采样频率</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -F 99 -g ./my_program  <span class="hljs-comment"># 每秒采样 99 次</span><br></code></pre></td></tr></table></figure><p>频率越高，数据越精确，但开销越大。</p><h4 id="系统级监控（所有-CPU）"><a href="#系统级监控（所有-CPU）" class="headerlink" title="系统级监控（所有 CPU）"></a><strong>系统级监控（所有 CPU）</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf record -ag                  <span class="hljs-comment"># 监控所有进程和 CPU</span><br>perf record -ag -e context-switches  <span class="hljs-comment"># 监控上下文切换</span><br></code></pre></td></tr></table></figure><h3 id="3-分析数据"><a href="#3-分析数据" class="headerlink" title="3.分析数据"></a>3.分析数据</h3><ol><li>对于生成的.data目录可以直接运行 <code>perf report</code>查看相关报告</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf report                     <span class="hljs-comment"># 交互式查看热点</span><br>perf report --stdio             <span class="hljs-comment"># 命令行模式输出</span><br><br><span class="hljs-comment">#按函数/符号排序，显示 CPU 时间占比。</span><br></code></pre></td></tr></table></figure><ol start="2"><li>通过火焰图分析（需要用到<em><strong>FlameGraph</strong></em>工具，也可以直接使用网页端<a href="https://ui.perfetto.dev/%EF%BC%89">https://ui.perfetto.dev/）</a></li></ol><p>CPU火焰图中的每一个方框是一个函数，方框的长度，代表了它的执行时间，所以越宽的函数，执行越久。火焰图的楼层每高一层，就是更深一级的函数被调用，最顶层的函数，是叶子函数。</p><p><img src="%E7%81%AB%E7%84%B0%E5%9B%BE.png" alt="火焰图"></p><ul><li>FlameGraph工具使用</li></ul><p>（1）安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> --depth 1 https://github.com/brendangregg/FlameGraph<br></code></pre></td></tr></table></figure><p>（2）使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl &gt; perf.svg<br><br>firefox perf.svg <span class="hljs-comment"># .svg文件可以通过浏览器打开</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Linux系统编程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch简明教程</title>
    <link href="/Diffcc/Diffcc.github.io/2025/06/07/20250607-2/"/>
    <url>/Diffcc/Diffcc.github.io/2025/06/07/20250607-2/</url>
    
    <content type="html"><![CDATA[<p>本文可以作为Pytorch初学者的快速上手文档，参考B站小土堆</p><span id="more"></span><h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><h2 id="1-安装环境"><a href="#1-安装环境" class="headerlink" title="1.安装环境"></a>1.安装环境</h2><p>&#x3D;&#x3D;pyhton 3.6  Anaconda 3.5.2&#x3D;&#x3D;<br>&#x3D;&#x3D;CUDA+显卡驱动&#x3D;&#x3D;</p><p><code>create -n pytorch python=3.6</code><br><code>conda activate pytorch</code><br><code>pip list</code></p><h2 id="2-Pytorch安装"><a href="#2-Pytorch安装" class="headerlink" title="2. Pytorch安装"></a>2. Pytorch安装</h2><p><img src="pytorch%E4%B8%8B%E8%BD%BD.png" alt="pytorch下载"></p><p>查看nvidia对应的Version</p><p><img src="CUDA%E7%89%88%E6%9C%AC.png" alt="CUDA版本"></p><p>验证是否安装成功</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">python<br><span class="hljs-keyword">import</span> torch<br>torch.cuda.is_available()  <span class="hljs-comment">##显示是否可用GPU  True</span><br></code></pre></td></tr></table></figure><h2 id="3-Pycharm-Jupyter"><a href="#3-Pycharm-Jupyter" class="headerlink" title="3.Pycharm+Jupyter"></a>3.Pycharm+Jupyter</h2><p><strong>Pycharm 配置pytorch</strong><br>python console 中运行(便于直接查看变量属性)</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clean"><span class="hljs-keyword">import</span> torch<br>torch.cuda.is_available()  ##显示是否可用GPU  <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><p><strong>Jupyter在pytorch环境中安装</strong></p><p><code>conda install -c conda-forge nb_conda</code><br><code>jupyter notebook</code><br><img src="Jupyter.png" alt="Jupyter"></p><p>Jupyter中  shift+enter  跳转下一行代码</p><h2 id="4-Pytorch中两个重要函数"><a href="#4-Pytorch中两个重要函数" class="headerlink" title="4.Pytorch中两个重要函数"></a>4.Pytorch中两个重要函数</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stylus">Pytorch就是一个package<br><br><span class="hljs-function"><span class="hljs-title">dir</span><span class="hljs-params">()</span></span>:打开，看见<br><span class="hljs-function"><span class="hljs-title">dir</span><span class="hljs-params">(pytorch)</span></span><br>输出：<span class="hljs-number">1</span>、<span class="hljs-number">2</span>、<span class="hljs-number">3</span>、<span class="hljs-number">4</span><br><span class="hljs-function"><span class="hljs-title">dir</span><span class="hljs-params">(pytorch.<span class="hljs-number">3</span>)</span></span><br>输出：<span class="hljs-selector-tag">a</span>,<span class="hljs-selector-tag">b</span>,c<br><br><br><span class="hljs-function"><span class="hljs-title">help</span><span class="hljs-params">()</span></span>:说明书<br><span class="hljs-function"><span class="hljs-title">help</span><span class="hljs-params">(pytorch.<span class="hljs-number">3</span>.a)</span></span><br>输出：a的具体功能<br></code></pre></td></tr></table></figure><p><img src="%E8%BE%93%E5%87%BA1.png" alt="输出1"></p><h2 id="5-Pycharm对比Jupyter"><a href="#5-Pycharm对比Jupyter" class="headerlink" title="5.Pycharm对比Jupyter"></a>5.Pycharm对比Jupyter</h2><p>同时在&#x3D;&#x3D;python&#x3D;&#x3D;文件、&#x3D;&#x3D;Python控制台&#x3D;&#x3D;、&#x3D;&#x3D;Jupyter&#x3D;&#x3D;运行一下代码：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Start！&quot;</span>)<br><span class="hljs-attribute">a</span>=<span class="hljs-string">&quot;hello world&quot;</span><br><span class="hljs-attribute">b</span>=2019<br><span class="hljs-attribute">c</span>=a+b<br><span class="hljs-built_in">print</span>(c)<br></code></pre></td></tr></table></figure><p>代码是以块为一个整体运行</p><p>Python文件：文件的块是所有行的代码，从头开始运行  适用于大型项目<br>Python控制台：以任意行为块运行  但是阅读性不好  可以显示每个变量属性<br>Jupyter：可以以任意代码块运行 利于代码的阅读与修改  </p><p><img src="%E5%AF%B9%E6%AF%94.png" alt="对比"></p><h2 id="6-Pytorch数据加载"><a href="#6-Pytorch数据加载" class="headerlink" title="6.Pytorch数据加载"></a>6.Pytorch数据加载</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clean">Dataset  ##提供一种方式去获取数据及其label值<br>Dataloader   ##为网络提供不同的数据形式  Batchsize<br></code></pre></td></tr></table></figure><p><strong>Dataset</strong></p><ul><li>如何获取每一个数据及其label</li><li>告诉我们总共有多少数据</li></ul><p><strong>Jupyter可以方便查看Dataset功能</strong></p><p><img src="%E6%9F%A5%E7%9C%8BDataset.png" alt="查看Dataset"></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ruby">from torch.utils.data import <span class="hljs-title class_">Dataset</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_">Dataset</span>):<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>.idx</span>)<br></code></pre></td></tr></table></figure><p><strong>read_data程序</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> dataset<br>form PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_ inherited__">dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,root_dir,label_dir</span>):<br>        <span class="hljs-variable language_">self</span>.root_dir=root_dir    <span class="hljs-comment">##初始中引入全局变量</span><br>        <span class="hljs-variable language_">self</span>.label_dir=label_dir   <span class="hljs-comment">##全局变量</span><br>        <span class="hljs-variable language_">self</span>.path=os.path.join(<span class="hljs-variable language_">self</span>.root_dir,<span class="hljs-variable language_">self</span>.label_dir)   <span class="hljs-comment">##变量地址相加获取Image地址</span><br>        <span class="hljs-variable language_">self</span>.image_path=os.listdir(<span class="hljs-variable language_">self</span>.path)  <span class="hljs-comment">##获取地址中的列表内容</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self,idx</span>):<br>        image_name=<span class="hljs-variable language_">self</span>.image_path[idx]   <span class="hljs-comment">##获取列表中名字</span><br>        image_item_path=os.path.join(<span class="hljs-variable language_">self</span>.path,image_name)  <span class="hljs-comment">##获取每张图片的地址（包括自身名字）</span><br>        img=Image.<span class="hljs-built_in">open</span>(image_item_path)  <span class="hljs-comment">##打开图片</span><br>        label=<span class="hljs-variable language_">self</span>.label_dir  <span class="hljs-comment">##获取标签</span><br>        <span class="hljs-keyword">return</span> img,label  <span class="hljs-comment">##返回图像与标签</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.image_path)  <span class="hljs-comment">##获取数据长度</span><br>    <br>root_dir=<span class="hljs-string">&quot;&quot;</span><br>ants_label_dir=<span class="hljs-string">&quot;&quot;</span><br>ants_dataset=MyData(root_dir,ants_label_dir)<br>        <br></code></pre></td></tr></table></figure><h2 id="7-TensorBoard-使用"><a href="#7-TensorBoard-使用" class="headerlink" title="7. TensorBoard 使用"></a>7. TensorBoard 使用</h2><p>&#x3D;&#x3D;探究模型不同阶段的输出图像效果&#x3D;&#x3D;</p><style>.ibtnmljpxovd{zoom:80%;}</style><img src="Diffcc/2025/06/07/20250607-2/Tensorboard.png" class="ibtnmljpxovd" alt="Tensorboard"><p><code>writer.add_scalar()</code></p><p><img src="Tensorboard%E8%AE%BE%E7%BD%AE.png" alt="Tensorboard设置"></p><p><strong>安装tensorboard</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs txt">conda activate pytorch<br>pip install tensorboard<br></code></pre></td></tr></table></figure><p><strong>运行tensorboard</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">tensorboard --logdir=logs --host=<span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span> <span class="hljs-comment">##logdir=logs文件也就是 Summarywriter（“logs”）中的文件地址</span><br>tensorboard --logdir=logs --host=<span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span> --port=<span class="hljs-number">6007</span>  <span class="hljs-comment">##定义端口名字 防止冲突</span><br></code></pre></td></tr></table></figure><p><img src="%E6%9C%AC%E5%9C%B0%E5%9C%B0%E5%9D%80.png" alt="本地地址"></p><p><img src="%E8%BE%93%E5%87%BA2.png" alt="输出2"></p><p>&#x3D;&#x3D;注意：程序运行多次，logs文件中会生成不一样的图像  每向wirter中写入上一个程序 数据会保留&#x3D;&#x3D;<br>&#x3D;&#x3D;删除 logs&#x3D;&#x3D;</p><p><strong>读取图像</strong></p><p><code>wirter.add-image()</code></p><p><img src="%E8%AF%BB%E5%8F%96%E5%9B%BE%E5%83%8F.png" alt="读取图像"></p><p>&#x3D;&#x3D;其中image读取类型为&#x3D;&#x3D;</p><p><img src="%E5%9B%BE%E7%89%87%E7%B1%BB%E5%9E%8B.png" alt="图片类型"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">PIL中 Image.<span class="hljs-built_in">open</span>(path)读取的类型为<br>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;PIL.JpegImagePlugin.JpegImageFile&#x27;</span>&gt;<br><br>利用Opencv读取numpy类型<br><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image_path=<span class="hljs-string">&quot;相对路径&quot;</span><br>img=Image.<span class="hljs-built_in">open</span>(image_path)<br>img_narry=np.array(img)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(img_narry))<br>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;numpy.ndarray&#x27;</span>&gt;<br><span class="hljs-built_in">print</span>(img_array.shape)<br>(<span class="hljs-number">512</span>,<span class="hljs-number">768</span>,<span class="hljs-number">3</span>)   (HWC)   <span class="hljs-comment">## add_image默认dataformats为tensorboard的CHW</span><br><span class="hljs-comment">##使用numpy型需指定</span><br><br>writer=SUmmaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>writer.add_image(<span class="hljs-string">&quot;test&quot;</span>,img_array,<span class="hljs-number">1</span>(步骤此处为第一步)，dataformats=<span class="hljs-string">&#x27;HWC&#x27;</span>)<br>writer.close()<br></code></pre></td></tr></table></figure><p><img src="test.png" alt="test"></p><h2 id="8-Transforms"><a href="#8-Transforms" class="headerlink" title="8.Transforms"></a>8.Transforms</h2><h3 id="8-1-ToTensor"><a href="#8-1-ToTensor" class="headerlink" title="8.1 ToTensor"></a>8.1 ToTensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchversion <span class="hljs-keyword">import</span> transforms  <span class="hljs-comment">#在transforms中alt+7  查看结构体</span><br></code></pre></td></tr></table></figure><p><strong>transforms结构及用法</strong></p><p>&#x3D;&#x3D;利用其中的不同功能实现具体效果&#x3D;&#x3D;</p><p><img src="transforms%E7%94%A8%E6%B3%95.png" alt="transforms用法"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><span class="hljs-keyword">from</span> torchversion <span class="hljs-keyword">import</span> transforms<br><br><span class="hljs-comment"># python的用法 -》tensor数据类型</span><br><span class="hljs-comment"># transforms.toTensor</span><br><span class="hljs-comment">#1、transforms该如何使用(python）</span><br><span class="hljs-comment">#2、为什么需要tensor数据类型</span><br><br>img_path=<span class="hljs-string">&quot;data/train/ants_image/5650366_e22b7e1065.jpg&quot;</span><br>img=Image.<span class="hljs-built_in">open</span>(img_path)<br><br><span class="hljs-comment">#1、transforms该如何使用(python）</span><br>tensor_trans=transforms.ToTensor()  <span class="hljs-comment">#利用classToTensor    创建自己的工具tensor_trans()</span><br>tensor_img=tensor_trans(img(Ctrl+P可以查看具体的所需数据类型))  <span class="hljs-comment">#使用自己的工具tensor_trans()转换成tensor类型图片</span><br><br><span class="hljs-comment">#2、为什么需要tensor数据类型</span><br>tensor 包括了神经网络中的参数类型 例如 梯度 反向传播 等<br><br>ToTensor包含将PIL和OPENCV读取的类型输入<br><br>writer=SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>writer.add_image(<span class="hljs-string">&quot;Tensor_image&quot;</span>,tensor_img)<br>writer.close()<br></code></pre></td></tr></table></figure><p><img src="Tensor_img.png" alt="Tensor_img"></p><h3 id="8-2-常见的Transforms"><a href="#8-2-常见的Transforms" class="headerlink" title="8.2 常见的Transforms"></a>8.2 常见的Transforms</h3><ul><li>输入： <em>PIL</em>   <em>Image.open()</em></li><li>输出：<em>tensor</em>      <em>ToTensor()</em></li><li>作用：<em>narrays</em>    <em>cv.imread()</em></li></ul><p><strong>Compose</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">Call的用法， 不需要再用.调用 直接括号调用即可<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Person</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self,name</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;__call__&quot;</span>+<span class="hljs-string">&quot; Hello &quot;</span>+name)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">hello</span>(<span class="hljs-params">self,name</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;hello&quot;</span>+ name)<br><br>person=Person()<br>person(<span class="hljs-string">&quot;zhangsan&quot;</span>)<br>person.hello(<span class="hljs-string">&quot;zhangsan&quot;</span>)<br><br>输出：<br>__call__ Hello zhangsan<br>hellozhangsan<br></code></pre></td></tr></table></figure><p><strong>ToTensor</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">将PIL读取文件 OPENCV读取文件转换成tensor<br><br>trans_totensor=transforms.ToTensor()<br>img_tensor=trans_totensor(img)<br></code></pre></td></tr></table></figure><p><strong>ToPILImage</strong></p><p><code>*&quot;&quot;&quot;Convert a tensor or an ndarray to PIL Image - this does not scale values.*</code></p><p><strong>Normalize</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;Normalize a tensor image with mean and standard deviation.</span><br><span class="hljs-string">This transform does not support PIL Image.</span><br><span class="hljs-string">Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n``</span><br><span class="hljs-string">channels, this transform will normalize each channel of the input</span><br><span class="hljs-string">``torch.*Tensor`` i.e.,</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">``output[channel] = (input[channel] - mean[channel]) / std[channel]``</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">归一化用来消除量纲影响</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">trans_norm=transforms.Normalize([<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>],[<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>])  <br><span class="hljs-comment">#三通道 mean[channel]=[0.5,0.5,0.5]  平均值</span><br><span class="hljs-comment">#std[channel]=[0.5,0.5,0.5]   标准差</span><br><span class="hljs-comment">#output[channel] = (input[channel] - mean[channel]) / std[channel]</span><br><span class="hljs-comment">#此处相当于  2*Input-1 = Output</span><br>img_norm=trans_norm(tensor_img)<br></code></pre></td></tr></table></figure><p><strong>Resize</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;Resize the input image to the given size.</span><br><span class="hljs-string">If the image is torch Tensor, it is expected</span><br><span class="hljs-string">to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions</span><br><span class="hljs-string"></span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">trans_resize=transforms.Resize((<span class="hljs-number">512</span>,<span class="hljs-number">512</span>))<br><span class="hljs-comment"># Resize 输入图片需要为PIL类型 否则会报错</span><br><br>img_resize=trans_resize(img)<br><br><span class="hljs-comment">#将PIL 再次转换成tensor</span><br>img_size=trans_totensor(img_resize)<br><br></code></pre></td></tr></table></figure><p><strong>compose_resize</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">compose利用call()循环其内的命令<br><br>trans_resize_2=transforms.Resize(<span class="hljs-number">512</span>)  <span class="hljs-comment">#单参数将最小的那个边变成此时的值</span><br><span class="hljs-comment">##Compose([a,b,c])  将参数传递给a，将a生成的输出传递给b，将b的输出传递给c，输出此时c的输出</span><br>trans_compose=transforms.Compose([trans_resize_2,trans_tensor])<span class="hljs-comment"># 同时实现了改变大小和将PIL文件转换成tensor文件</span><br>img_resize_2=trans_compose(img)<br></code></pre></td></tr></table></figure><p><strong>RandomCrop</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#随机裁剪</span><br><br>trans_Crop=transforms.RandomCrop(<span class="hljs-number">30</span>)  <span class="hljs-comment">#裁剪30*30的方形</span><br>trans_compose_2=transforms.Compose([trans_Crop,trans_tensor])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    img_randomcrop=trans_compose_2(img)<br>    writer.add_image(<span class="hljs-string">&quot;RandomCrop&quot;</span>,img_randomcrop,i)  <span class="hljs-comment">#随机从原图中裁剪30*30的十张图片</span><br></code></pre></td></tr></table></figure><ul><li><strong>关注输入与输出</strong></li><li><strong>学会查看官方文档</strong></li><li><strong>关注方法需要什么参数</strong></li><li><strong>不知道返回类型  <em>print</em>   <em>print(type)</em>  <em>debug</em></strong></li></ul><p>&#x3D;&#x3D;在Tensorboard使用一定要转换成tensor数据类型&#x3D;&#x3D;</p><h2 id="9-torchvision中的数据集的使用"><a href="#9-torchvision中的数据集的使用" class="headerlink" title="9.torchvision中的数据集的使用"></a>9.torchvision中的数据集的使用</h2><p>torchvision 0.9.0版本数据集地址：<a href="https://pytorch.org/vision/0.9/">https://pytorch.org/vision/0.9/</a><br>.io  输入输出模块<br>.models  常见的神经网络模块<br>.ops  特殊操作<br>.utils 常见工具(tensorboard)<br>.transforms</p><p>例如CIFAR数据集</p><p><img src="CIFAR10%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="CIFAR10数据集"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><br>train_set=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">True</span>,download=<span class="hljs-literal">True</span>)<br>test_set=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,download=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment">##可将下载链接拷贝到迅雷进行下载  下载完的数据集直接拷贝到自己新建的一个dataset文件夹，download自动设置为true，会自动校验下载和解压</span><br><br><span class="hljs-built_in">print</span>(test_set[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(test_set.classes)<br><br>img,target=test_set[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(img)<br><span class="hljs-built_in">print</span>(target)<br><span class="hljs-built_in">print</span>(test_set.classes[target])<br>img.show()<br><br>Files already downloaded <span class="hljs-keyword">and</span> verified<br>Files already downloaded <span class="hljs-keyword">and</span> verified<br>(&lt;PIL.Image.Image image mode=RGB size=32x32 at <span class="hljs-number">0x23C7C9B97E0</span>&gt;, <span class="hljs-number">3</span>)<br>[<span class="hljs-string">&#x27;airplane&#x27;</span>, <span class="hljs-string">&#x27;automobile&#x27;</span>, <span class="hljs-string">&#x27;bird&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;deer&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;frog&#x27;</span>, <span class="hljs-string">&#x27;horse&#x27;</span>, <span class="hljs-string">&#x27;ship&#x27;</span>, <span class="hljs-string">&#x27;truck&#x27;</span>]<br>&lt;PIL.Image.Image image mode=RGB size=32x32 at <span class="hljs-number">0x23C7C9B97E0</span>&gt;<br><span class="hljs-number">3</span><br>cat<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#转换成tensor类型</span><br><br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>dataset_transforms=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])<br><br>train_set=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">True</span>,transform=dataset_transforms,download=<span class="hljs-literal">True</span>)<br>test_set=torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=dataset_transforms,download=<span class="hljs-literal">True</span>)<br><br>Writer=SummaryWriter(<span class="hljs-string">&quot;p10&quot;</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    img,target=test_set[i]<br>    Writer.add_image(<span class="hljs-string">&quot;test_set&quot;</span>,img,i)<br></code></pre></td></tr></table></figure><h2 id="10-Dataload的使用"><a href="#10-Dataload的使用" class="headerlink" title="10.Dataload的使用"></a>10.Dataload的使用</h2><p>将dataset获取数据加载到神经网络之中</p><p>pytorch 1.8.1  ：<a href="https://pytorch.org/docs/1.8.1/data.html?highlight=dataloader#torch.utils.data.DataLoader">https://pytorch.org/docs/1.8.1/data.html?highlight=dataloader#torch.utils.data.DataLoader</a></p><p><img src="DataLoad.png" alt="DataLoad"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-comment">#数据集</span><br>test_dataset=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><span class="hljs-comment">#载入数据集</span><br>test_loader=DataLoader(dataset=test_dataset,batch_size=<span class="hljs-number">4</span>,shuffle=<span class="hljs-literal">True</span>,num_workers=<span class="hljs-number">0</span>,drop_last=<span class="hljs-literal">False</span>)<br><br>输出：<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>]) <span class="hljs-comment">#4张图片3通道32×32图片</span><br>tensor([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>])  <span class="hljs-comment">#target随机  默认随机策略</span><br><br><br><span class="hljs-comment">#batch_size=4 每次取出4张图片及其target 并打包成新的img，target</span><br><span class="hljs-comment">#shuffle=True  抓取完完整的一轮之后   之后一轮是否和前面一轮一样</span><br><span class="hljs-comment">#num_workers=0  仅仅主线程</span><br><span class="hljs-comment">#drop_last=False 不丢弃最后剩余的图片</span><br><br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">test_loader=DataLoader(dataset=test_dataset,batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-literal">True</span>,num_workers=<span class="hljs-number">0</span>,drop_last=<span class="hljs-literal">False</span>)<br><br>Writer=SummaryWriter(<span class="hljs-string">&quot;dataloader&quot;</span>)<br><br>step=<span class="hljs-number">0</span><br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_loader:<br>    imgs,targets=data<br>    <span class="hljs-comment"># print(imgs.shape)</span><br>    <span class="hljs-comment"># print(targets)</span><br>    Writer.add_images(<span class="hljs-string">&quot;test_data&quot;</span>,imgs,step)  <span class="hljs-comment">#此处为add_images  而非单个图片add_image</span><br>    step+=<span class="hljs-number">1</span><br><br>Writer.close()<br></code></pre></td></tr></table></figure><p><img src="Batch_size_64.png" alt="Batch_size_64"></p><p><img src="batch_size_64%E4%BD%9916.png" alt="batch_size_64余16"></p><p>&#x3D;&#x3D;batch_size&#x3D;64 每次抓取64张，最后一次剩余16张，由于drop_last设置为flase，即不丢弃最后的16张&#x3D;&#x3D;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">test_loader=DataLoader(dataset=test_dataset,batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-literal">True</span>,num_workers=<span class="hljs-number">0</span>,drop_last=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):   <span class="hljs-comment">#epoch表示训练两轮  shuffle设置为True  即两轮不一样</span><br>    step = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_loader:<br>        imgs,targets=data<br>    <span class="hljs-comment"># print(imgs.shape)</span><br>    <span class="hljs-comment"># print(targets)</span><br>        Writer.add_images(<span class="hljs-string">&quot;Epoch:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch),imgs,step)  <span class="hljs-comment">#&quot;Epoch:&#123;&#125;&quot;.format(epoch) 表示出输出Epoch：0和Epoch：1</span><br>        <br>        step+=<span class="hljs-number">1</span><br>        <br>Writer.close()<br><br></code></pre></td></tr></table></figure><p><img src="epoch.png" alt="epoch"></p><p><img src="epoch1.png" alt="epoch1"></p><h2 id="11-神经网络的基本骨架—nn-module"><a href="#11-神经网络的基本骨架—nn-module" class="headerlink" title="11.神经网络的基本骨架—nn.module"></a>11.神经网络的基本骨架—nn.module</h2><p><strong>torch.nn</strong>:<a href="https://pytorch.org/docs/1.8.1/nn.html">https://pytorch.org/docs/1.8.1/nn.html</a></p><p><strong>.Containers</strong>   神经网络骨架</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):  //Model为自定义的名字  括号内为调用的父类<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Model, <span class="hljs-variable language_">self</span>).__init__()  //调用nn.Mdoule 的初始化 重写nn.module的方法 可以用Ctrl+O 选择第一个<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)  //卷积<span class="hljs-number">1</span><br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)  //卷积<span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):   //前向传播  pytorch 根据前向传播能自动计算反向传播<br>        x = F.relu(<span class="hljs-variable language_">self</span>.conv1(x))  //conv1卷积<span class="hljs-number">1</span>  relu为非线性处理<br>        <span class="hljs-keyword">return</span> F.relu(<span class="hljs-variable language_">self</span>.conv2(x))//在进行一次卷积<span class="hljs-number">2</span> 非线性处理<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span>  nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NNtest</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,<span class="hljs-built_in">input</span></span>):<br>        output=<span class="hljs-built_in">input</span>+<span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> output<br><br>nntest=NNtest()  <span class="hljs-comment">##调用NNtest中的nn.module的 _init_方法</span><br>x=torch.tensor(<span class="hljs-number">1.0</span>)  <span class="hljs-comment">##x=tensor(1,)</span><br>output=nntest(x)  <span class="hljs-comment">##调用forward方法   nn.module中的__call__会自动把参数传递给forward进行处理</span><br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure><h2 id="12-卷积操作"><a href="#12-卷积操作" class="headerlink" title="12.卷积操作"></a>12.卷积操作</h2><p><img src="conv.png" alt="conv"></p><p><img src="%E5%8F%82%E6%95%B0.png" alt="参数"></p><p><strong>stride</strong>：步长<br><strong>padding</strong>：填充操作（1，1）上下左右依次插入空行空列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span>  torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-built_in">input</span>=torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>],<br>                   [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>],<br>                   [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],<br>                   [<span class="hljs-number">5</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],<br>                   [<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]])<br><br>kernel=torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>],  //对应与weight<br>                    [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],<br>                    [<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]])<br><br><span class="hljs-comment">##conv2d的输入方式（minibatch，in_channel,iH,iW)  使用reshape进行尺寸改变  H代表行  W代表列</span><br><br><span class="hljs-built_in">input</span>=torch.reshape(<span class="hljs-built_in">input</span>,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">5</span>))<br>kernel=torch.reshape(kernel,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br><br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">input</span>.shape)<br><span class="hljs-built_in">print</span>(kernel.shape)<br><br>output=F.conv2d(<span class="hljs-built_in">input</span>,kernel,stride=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(output)<br><br><span class="hljs-comment">##padding操作</span><br>output2=F.conv2d(<span class="hljs-built_in">input</span>,kernel,stride=<span class="hljs-number">1</span>,padding=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br><span class="hljs-built_in">print</span>(output2)<br><br>结果：<br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br>tensor([[[[<span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>],<br>          [<span class="hljs-number">18</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>],<br>          [<span class="hljs-number">13</span>,  <span class="hljs-number">9</span>,  <span class="hljs-number">3</span>]]]])<br><br>tensor([[[[ <span class="hljs-number">1</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>, <span class="hljs-number">10</span>,  <span class="hljs-number">8</span>],<br>          [ <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>,  <span class="hljs-number">6</span>],<br>          [ <span class="hljs-number">7</span>, <span class="hljs-number">18</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>,  <span class="hljs-number">8</span>],<br>          [<span class="hljs-number">11</span>, <span class="hljs-number">13</span>,  <span class="hljs-number">9</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>],<br>          [<span class="hljs-number">14</span>, <span class="hljs-number">13</span>,  <span class="hljs-number">9</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">4</span>]]]])<br></code></pre></td></tr></table></figure><p><img src="%E8%BE%93%E5%87%BA3.png" alt="输出3"></p><p><img src="%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png" alt="计算公式"></p><h2 id="13-神经网络—卷积层"><a href="#13-神经网络—卷积层" class="headerlink" title="13.神经网络—卷积层"></a>13.神经网络—卷积层</h2><p><a href="https://pytorch.org/docs/1.8.1/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">https://pytorch.org/docs/1.8.1/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</a></p><p><img src="conv2d.png" alt="conv2d"></p><p><img src="conv2d%E5%8F%82%E6%95%B0.png" alt="conv2d参数"></p><h4 id="参数解释"><a href="#参数解释" class="headerlink" title="参数解释"></a>参数解释</h4><ul><li><strong>in_channels</strong>:(N,C<del>in</del>,H<del>in</del>,W<del>in</del>)  N&#x3D;batch_size  C<del>in</del>为通道数  H<del>in</del>为行（高）  W<del>in</del>为宽（列）</li><li><strong>out_channels</strong>:(N,C<del>out</del>,H<del>out</del>,W<del>out</del>)  batch_size 始终不变</li><li><strong>kernel_size</strong>:(N,C,H,W) </li><li><strong>stride</strong>: 步长</li><li><strong>padding</strong>:填充（1，1）对输入图像周围填充一行一列</li><li><strong>padding_mode</strong>:’zeros’  填充0</li><li><strong>dilation</strong>：空洞卷积</li></ul><p><img src="%E5%8D%B7%E7%A7%AF%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="卷积示意图"></p><p><img src="%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF.png" alt="空洞卷积"></p><p><img src="%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF%E4%BD%9C%E7%94%A8.png" alt="空洞卷积作用"></p><ul><li><p><strong>groups</strong>: 分组卷积：<a href="https://zhuanlan.zhihu.com/p/490685194">https://zhuanlan.zhihu.com/p/490685194</a></p></li><li><p><strong>bias</strong>：偏置  偏置等于卷积核的个数即kernel_size中（N,C,H,W)中的C</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>data_set=torchvison.dataset.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transforme=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br>dataloader=DataLoader(data_set,batch_size=<span class="hljs-number">64</span>,shuttle=<span class="hljs-literal">True</span>,Drop_last=<span class="hljs-literal">False</span>)<br><br>Class NNconv2d(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NNconv2d,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1=Conv2d(in_channels=<span class="hljs-number">3</span>,out_channels=<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">3</span>,stride=<span class="hljs-number">1</span>,padding=<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x=<span class="hljs-variable language_">self</span>.conv1(x)<br>        <span class="hljs-keyword">return</span> x<br><br>Myconv2d=NNconv2d()<br>step=<span class="hljs-number">0</span><br>writer=SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    outputs=Myconv2d(imgs)<br>    <span class="hljs-comment">##inputs.size(64,3,32,32)</span><br>    <span class="hljs-comment">##outputs.size(64,6,30,30)  由于tensorboard只能输出RGB三通道的图像  因此需要把6通道转换为3通道</span><br>    <span class="hljs-comment">##batch_size 设置为-1  自动根据后面设置的参数设置batch_size</span><br>    writer.add_images(<span class="hljs-string">&quot;Input&quot;</span>,imgs,step)<br>    outputs=torch.reshape(outputs,(-<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">30</span>,<span class="hljs-number">30</span>))<br>    writer.add_images(<span class="hljs-string">&quot;Output&quot;</span>,outputs,step)<br>    step+=<span class="hljs-number">1</span><br>    <br>writer.close()<br><br></code></pre></td></tr></table></figure><h2 id="14-神经网络—最大池化的使用"><a href="#14-神经网络—最大池化的使用" class="headerlink" title="14.神经网络—最大池化的使用"></a>14.神经网络—最大池化的使用</h2><p>&#x3D;&#x3D;最大池化取卷积核框内的最大值&#x3D;&#x3D;</p><p><img src="MaxPool2d.png" alt="MaxPool2d"></p><p><img src="MaxPool2d%E5%8F%82%E6%95%B0.png" alt="MaxPool2d参数"></p><p><strong>ceil_mode</strong>:True为cell  即对于卷积核当中有空元素（0）的时候 选择保留其中剩下的最大值     默认为False</p><p>一般只需要设置<strong>kernel_size</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> MaxPool2d<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><span class="hljs-comment"># input=torch.tensor([[1,2,0,3,1],</span><br><span class="hljs-comment">#                     [0,1,2,3,1],</span><br><span class="hljs-comment">#                     [1,2,1,0,0],</span><br><span class="hljs-comment">#                     [5,2,3,1,1],</span><br><span class="hljs-comment">#                     [2,1,0,1,1]],dtype=torch.float32)  #将矩阵中的整形LONG转换成浮点型</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># input=torch.reshape(input,(-1,1,5,5))</span><br><span class="hljs-comment"># print(input.shape)</span><br>data_set=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br>dataloader=DataLoader(data_set,batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">nnMaxpool</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(nnMaxpool,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.maxpool=MaxPool2d(kernel_size=<span class="hljs-number">3</span>,ceil_mode=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x=<span class="hljs-variable language_">self</span>.maxpool(x)<br>        <span class="hljs-keyword">return</span> x<br><br>Max_pool=nnMaxpool()<br><span class="hljs-comment"># output=Max_pool(input)</span><br><span class="hljs-comment"># print(output)</span><br>writer=SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>step=<span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    outputs=Max_pool(imgs)<br>    writer.add_images(<span class="hljs-string">&quot;out_put_maxpoll&quot;</span>,outputs,step)<br>    step+=<span class="hljs-number">1</span><br><br>writer.close()<br><br><br><br>输出：<br>tensor([[[[<span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],<br>          [<span class="hljs-number">5.</span>, <span class="hljs-number">1.</span>]]]])<br><br><span class="hljs-comment">#池化的作用：大大减小训练量，加快训练类似于电影1080P转换成720P  但仍然能满足需求</span><br></code></pre></td></tr></table></figure><p><img src="%E8%BE%93%E5%87%BA4.png" alt="输出4"></p><h2 id="15-神经网络—非线性激活"><a href="#15-神经网络—非线性激活" class="headerlink" title="15.神经网络—非线性激活"></a>15.神经网络—非线性激活</h2><p>Padding Layers  几乎用不到  基本都可以通过卷积层中的padding实现</p><p>&#x3D;&#x3D;Non—linear  Activations&#x3D;&#x3D;</p><p><strong>Relu</strong></p><p><img src="ReLu1.png" alt="ReLu1"></p><p><img src="ReLu2.png" alt="ReLu2"></p><p><img src="ReLu3.png" alt="ReLu3"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span>  nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> ReLU<br><br><span class="hljs-built_in">input</span>=torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">0.5</span>],<br>                    [<span class="hljs-number">2</span>,-<span class="hljs-number">0.6</span>]])<br><br><span class="hljs-built_in">input</span>=torch.reshape(<span class="hljs-built_in">input</span>,(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NNRelu</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NNRelu,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.Relu1=ReLU()  <span class="hljs-comment">##无需输入</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x=<span class="hljs-variable language_">self</span>.Relu1(x)<br>        <span class="hljs-keyword">return</span> x<br><br>relu_results=NNRelu()<br>outputs=relu_results(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(outputs)<br></code></pre></td></tr></table></figure><p><strong>Sigmod</strong></p><p><img src="Sigmod1.png" alt="Sigmod1"></p><p><img src="Sigmod2.png" alt="Sigmod2"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span>  torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span>  torch.nn <span class="hljs-keyword">import</span> Sigmoid<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>data_set=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br>dataloader=DataLoader(data_set,batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NNsigmoild</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NNsigmoild,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.Sig=Sigmoid()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,output</span>):<br>        output=<span class="hljs-variable language_">self</span>.Sig(output)<br>        <span class="hljs-keyword">return</span> output<br><br>Mysigmoid=NNsigmoild()<br>writer=SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>step=<span class="hljs-number">0</span><br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    outputs=Mysigmoid(imgs)<br>    writer.add_images(<span class="hljs-string">&quot;Output_sigmoid&quot;</span>,outputs,step)<br>    step+=<span class="hljs-number">1</span><br><br>writer.close()<br><br><br><br></code></pre></td></tr></table></figure><p>&#x3D;&#x3D;非线性变换的目的：为模型引入非线性特征，提高模型的泛化能力&#x3D;&#x3D;</p><h2 id="16-神经网络—线性层及其他层的介绍"><a href="#16-神经网络—线性层及其他层的介绍" class="headerlink" title="16.神经网络—线性层及其他层的介绍"></a>16.神经网络—线性层及其他层的介绍</h2><p><a href="https://pytorch.org/docs/1.8.1/nn.html">https://pytorch.org/docs/1.8.1/nn.html</a></p><p><img src="%E5%85%B6%E4%BB%96%E5%B1%82.png" alt="其他层"></p><p><strong>Linear(也就是常说的全连接层)</strong></p><p><img src="%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82.png" alt="全连接层"></p><p><img src="Linear.png" alt="Linear"></p><p><img src="Linear%E5%8F%82%E6%95%B0.png" alt="Linear参数"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">##将图片通过liner转换成1x3 </span><br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span>  torch.nn <span class="hljs-keyword">import</span> Linear<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>data_set=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br>dataloader=DataLoader(data_set,batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Haijun</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.Line=Linear(<span class="hljs-number">196608</span>,<span class="hljs-number">10</span>)  <span class="hljs-comment">#分别是输入和输出的样本量</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,<span class="hljs-built_in">input</span></span>):<br>        output=<span class="hljs-variable language_">self</span>.Line(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">return</span> output<br><br>Trans=Haijun()<br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    <span class="hljs-comment"># outputs=torch.reshape(imgs,(1,1,1,-1))</span><br>    outputs=torch.flatten(imgs)  <span class="hljs-comment">##flatten 可以直接展开一行操作  但是reshape可以指定大小</span><br>    <span class="hljs-built_in">print</span>(outputs.shape)<br>    outputs=Trans(outputs)<br>    <span class="hljs-built_in">print</span>(outputs.shape)<br></code></pre></td></tr></table></figure><h2 id="17-神经网络—搭建实战及Sequential序列的使用"><a href="#17-神经网络—搭建实战及Sequential序列的使用" class="headerlink" title="17.神经网络—搭建实战及Sequential序列的使用"></a>17.神经网络—搭建实战及Sequential序列的使用</h2><p><strong>将神经网络各层连接</strong></p><p><img src="%E8%BF%9E%E6%8E%A5%E5%90%84%E5%B1%82.png" alt="连接各层"></p><p><strong>CIFAR训练模型</strong></p><p><img src="%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.png" alt="训练模型"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Haijun</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># self.cov1=Conv2d(in_channels=3,out_channels=32,kernel_size=5,stride=1,padding=2)</span><br>        <span class="hljs-comment"># self.maxpool1=MaxPool2d(2)</span><br>        <span class="hljs-comment"># self.cov2=Conv2d(32,32,5,1,2)</span><br>        <span class="hljs-comment"># self.maxpool2=MaxPool2d(2)</span><br>        <span class="hljs-comment"># self.cov3=Conv2d(32,64,5,1,2)</span><br>        <span class="hljs-comment"># self.maxpool3=MaxPool2d(2)</span><br>        <span class="hljs-comment"># self.flatten=Flatten()</span><br>        <span class="hljs-comment"># self.Lin1=Linear(1024,64)</span><br>        <span class="hljs-comment"># self.Lin2=Linear(64,10)</span><br><br>        <br>        <span class="hljs-comment">##  使用Sequential可以很有效的简化代码</span><br>        <span class="hljs-variable language_">self</span>.model1=Sequential(<br>            Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Flatten(),<br>            Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>),<br>            Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br>    <span class="hljs-keyword">def</span>  <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,output</span>):<br>        <span class="hljs-comment"># output=self.cov1(output)</span><br>        <span class="hljs-comment"># output=self.maxpool1(output)</span><br>        <span class="hljs-comment"># output=self.cov2(output)</span><br>        <span class="hljs-comment"># output=self.maxpool2(output)</span><br>        <span class="hljs-comment"># output=self.cov3(output)</span><br>        <span class="hljs-comment"># output=self.maxpool3(output)</span><br>        <span class="hljs-comment"># output=self.flatten(output)</span><br>        <span class="hljs-comment"># output=self.Lin1(output)</span><br>        <span class="hljs-comment"># output=self.Lin2(output)</span><br><br>        output=<span class="hljs-variable language_">self</span>.model1(output)<br><br>        <span class="hljs-keyword">return</span> output<br><br>haijun=Haijun()<br><span class="hljs-built_in">input</span>=torch.ones((<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>))  <span class="hljs-comment">##测试用代码  生成64batch 3通道 32x32  其中元素都为1的矩阵</span><br>output=haijun(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(output.shape)<br><br><span class="hljs-comment">##利用TensorBoard可视化</span><br><br>writer=SummaryWriter(<span class="hljs-string">&quot;./logs_seq&quot;</span>)<br>writer.add_graph(haijun,<span class="hljs-built_in">input</span>)<br>writer.close()<br></code></pre></td></tr></table></figure><p><img src="%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt="可视化"></p><h2 id="18-损失函数与反向传播"><a href="#18-损失函数与反向传播" class="headerlink" title="18.损失函数与反向传播"></a>18.损失函数与反向传播</h2><p><strong>Loss Function作用：</strong><br>1、计算实际输出和目标之间的差距<br>2、为我们更新输出提供一定的依据（反向传播）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> L1Loss<br><br>inputs=torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],dtype=torch.float32)<br>targets=torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>],dtype=torch.float32)<br><br>inputs=torch.reshape(inputs,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))<br>outputs=torch.reshape(targets,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))  <span class="hljs-comment">##inputs 和 outputs输入输出大小一定要相同</span><br><br><span class="hljs-comment">##L1 function  绝对值</span><br>loss=L1Loss()<br><span class="hljs-comment">##Mse  平方差</span><br>loss_mse=nn.MSELoss()<br><br>results=loss(inputs,outputs)<br><span class="hljs-built_in">print</span>(results)<br>results2=loss_mse(inputs,outputs)<br><span class="hljs-built_in">print</span>(results2)<br></code></pre></td></tr></table></figure><p><strong>交叉熵：Crossentropyloss   分类问题</strong></p><p><img src="%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.png" alt="交叉熵损失"></p><p>分类问题输出：output[x1,x2,x3]  分别对应是每一类的概率</p><p>​Target&#x3D;0（对应与第0类，对应与x1）只有当target完全命中的时候 -x[class]才会比较小  从而Loss比较小</p><p>交叉熵&#x3D;—x1+ln(exp(x1)+exp(x2)+exp(x3))</p><p><img src="%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B12.png" alt="交叉熵损失2"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">x=torch.tensor([<span class="hljs-number">0.1</span>,<span class="hljs-number">0.2</span>,<span class="hljs-number">0.3</span>])<br>y=torch.tensor([<span class="hljs-number">1</span>])  <span class="hljs-comment">#(batch_size)</span><br>x=torch.reshape(x,(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))  <span class="hljs-comment">#转化为（batch_size,classes)</span><br>loss_cross=nn.CrossEntropyLoss()<br>result=loss_cross(x,y)<br><span class="hljs-built_in">print</span>(result)<br><br>结果：<br>tensor(<span class="hljs-number">1.1019</span>)<br><br><br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs,targets=data<br>    outputs=haijun(imgs)<br>    results=loss(outputs,targets)<br>    <span class="hljs-built_in">print</span>(results)<br>    <br>    results.backword()  <span class="hljs-comment">#反向传播 生成梯度 后续利用优化器</span><br></code></pre></td></tr></table></figure><h2 id="19-优化器"><a href="#19-优化器" class="headerlink" title="19.优化器"></a>19.优化器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, target <span class="hljs-keyword">in</span> dataset:<br>    optimizer.zero_grad()  <span class="hljs-comment">#梯度清0  防止梯度有初始值对下一轮训练造成影响</span><br>    output = model(<span class="hljs-built_in">input</span>)  <span class="hljs-comment">#模型训练的结果</span><br>    loss = loss_fn(output, target)  <span class="hljs-comment">#计算损失函数</span><br>    loss.backward()   <span class="hljs-comment">#误差反向传播 </span><br>    optimizer.step()  <span class="hljs-comment">#优化器根据梯度调整模型中的各项参数</span><br>    <br>    <br><span class="hljs-comment">##不同的优化器除了params(参数)，lr(学习速率)外还有许多其他要设置的参数，根据不同情况设置</span><br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#利用随机梯度下降</span><br><br>haijun=Haijun()<br>loss=nn.CrossEntropyLoss()<br>optimizer=optim.SGD(haijun.parameters(),lr=<span class="hljs-number">0.01</span>)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):  <span class="hljs-comment">#epoch为训练轮次</span><br>    running_loss=<span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>        imgs,targets=data<br>        outputs=haijun(imgs)<br>        loss_cross=loss(outputs,targets)<br>        optimizer.zero_grad()<br>        loss_cross.backward()<br>        optimizer.step()<br>        running_loss=running_loss+loss_cross  <br>    <span class="hljs-built_in">print</span>(running_loss)  <span class="hljs-comment">#每一轮总的loss和</span><br></code></pre></td></tr></table></figure><h2 id="20-现有模型的使用与修改"><a href="#20-现有模型的使用与修改" class="headerlink" title="20.现有模型的使用与修改"></a>20.现有模型的使用与修改</h2><p>各模型地址：<a href="https://pytorch.org/vision/0.9/models.html#id2">https://pytorch.org/vision/0.9/models.html#id2</a></p><p>分类模型VGG举例(其预训练在ImageNet上进行)</p><p><img src="ImageNet.png" alt="ImageNet"></p><p>ImageNet需要scipy，且目前已不支持download&#x3D;True下载</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><br><span class="hljs-comment"># dataset=torchvision.datasets.ImageNet(&#x27;./dataset&#x27;,split=&#x27;train&#x27;,transform=torchvision.transforms.ToTensor(),download=True)</span><br><br>vgg16_false=torchvision.models.vgg16(weights=<span class="hljs-literal">None</span>)  <span class="hljs-comment">#只是加载网络模型  目前使用weights 而非download 默认为None 不适用预训练权重</span><br><span class="hljs-comment"># vgg16_true=torchvision.models.vgg16(download=True)    #从ImageNet上下载训练好的模型参数</span><br><br><span class="hljs-built_in">print</span>(vgg16_false)<br><br><br></code></pre></td></tr></table></figure><p><img src="%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.png" alt="网络模型"> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#修改现有网络模型 以Vgg16举例</span><br><br><span class="hljs-comment"># 在vgg16中的classifier层中增加一层线性层</span><br>vgg16_false.classifier.add_module(<span class="hljs-string">&#x27;add_linear&#x27;</span>,nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">10</span>))<br><br><br><span class="hljs-comment">#修改vgg16中的calssifier层中最后一层线性层</span><br>vgg16_false.classifier[<span class="hljs-number">6</span>]=nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><h2 id="21-网络模型的保存与读取"><a href="#21-网络模型的保存与读取" class="headerlink" title="21.网络模型的保存与读取"></a>21.网络模型的保存与读取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><br><span class="hljs-comment">#保存方式1</span><br>torch.save(vgg16,<span class="hljs-string">&quot;vgg16_model1.pth&quot;</span>)  <span class="hljs-comment">#保存的是模型的结构+参数</span><br>model1=torch.load(<span class="hljs-string">&quot;vgg16_model1.pth&quot;</span>)  <span class="hljs-comment">#加载</span><br><br><span class="hljs-comment">#保存方式2</span><br>torch.save(vgg16.state_dict(),<span class="hljs-string">&quot;vgg16_model2.pth&quot;</span>)  <span class="hljs-comment">#保存的仅仅是模型参数  以字典的形式</span><br>model2=torch.load(<span class="hljs-string">&quot;vgg16_model2.pth&quot;</span>) <span class="hljs-comment">#加载（字典结构显示）</span><br><br><span class="hljs-comment">#将字典加载成模型</span><br>vgg16=torchvision.models.vgg16(weights=<span class="hljs-literal">None</span>)<br>vgg16.load_state_dict(torch.load(<span class="hljs-string">&quot;vgg16_model2.pth&quot;</span>))  <span class="hljs-comment">#此时Vgg16显示的就是结构+参数</span><br><br></code></pre></td></tr></table></figure><h2 id="22-模型训练"><a href="#22-模型训练" class="headerlink" title="22.模型训练"></a>22.模型训练</h2><p><strong>以CIFAR10为数据集</strong></p><p>&#x3D;&#x3D;train.py&#x3D;&#x3D;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#准备数据集</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> *<br><br>trian_data=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br>test_data=torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>,train=<span class="hljs-literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment">#length长度</span><br>train_data_size=<span class="hljs-built_in">len</span>(trian_data)<br>test_data_size=<span class="hljs-built_in">len</span>(test_data)<br><br><span class="hljs-comment">#数据集长度</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练数据集长度：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(train_data_size))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;测试数据集长度：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(test_data_size))<br><br><span class="hljs-comment">#利用dataloader加载数据集</span><br>trian_dataloader=DataLoader(trian_data,batch_size=<span class="hljs-number">64</span>)<br>test_dataloader=DataLoader(test_data,batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-comment">#创建网络模型</span><br>Haijun=haijun()<br><br><span class="hljs-comment">#创建损失函数</span><br>loss_fn=nn.CrossEntropyLoss()<br><br><span class="hljs-comment">#优化器</span><br>learning_rate=<span class="hljs-number">1e-2</span><br>optim=torch.optim.SGD(Haijun.parameters(),lr=learning_rate)<br><br><span class="hljs-comment">#设置训练网络的一些参数</span><br>total_train_step=<span class="hljs-number">0</span>  <span class="hljs-comment">#训练次数</span><br>total_test_step=<span class="hljs-number">0</span>  <span class="hljs-comment">#测试次数</span><br>epoch=<span class="hljs-number">10</span>  <span class="hljs-comment">#训练轮数</span><br><br><span class="hljs-comment">#添加Tensorboard</span><br>writer=SummaryWriter(<span class="hljs-string">&quot;./Mylogs&quot;</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-------------------------第&#123;&#125;论训练开始----------------------------&quot;</span>.<span class="hljs-built_in">format</span>(i))<br><br>    <span class="hljs-comment">#训练步骤开始</span><br>    <span class="hljs-comment">#Haijun.train()  调用模型的特定结构  例如drop_out层  batchnorm层</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> trian_dataloader:<br>        imgs,targets=data<br>        outputs=Haijun(imgs)<br>        loss=loss_fn(outputs,targets)<br>        <span class="hljs-comment">#优化器优化模型</span><br>        optim.zero_grad()<br>        loss.backward()<br>        optim.step()<br><br>        total_train_step=total_train_step+<span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> total_train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练次数：&#123;&#125;，Loss=&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_train_step,loss.item()))  <span class="hljs-comment">##loss.item() 将tensor(1)转换成数字1显示</span><br>            writer.add_scalar(<span class="hljs-string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)<br><br>    <span class="hljs-comment">#测试步骤开始</span><br>    <span class="hljs-comment">#Haijun.eval()  调用模型结构  例如drop_out  batchnorm层</span><br>    total_test_loss=<span class="hljs-number">0</span><br>    total_accuacy=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment">#去除梯度的影响</span><br>        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_dataloader:<br>            imgs,targets=data<br>            outputs=Haijun(imgs)<br>            loss=loss_fn(outputs,targets)<br>            total_test_loss=total_test_loss+loss.item()<br>            accuacy=(outputs.argmax(<span class="hljs-number">1</span>)==targets).<span class="hljs-built_in">sum</span>()<br>            total_accuacy=total_accuacy+accuacy<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_test_loss))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的Accuacy：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_accuacy/test_data_size))<br>    writer.add_scalar(<span class="hljs-string">&quot;test_loss&quot;</span>,total_test_loss,total_test_step)<br>    writer.add_scalar(<span class="hljs-string">&quot;test_accuacy&quot;</span>,total_accuacy/test_data_size,total_test_step)<br>    total_train_step=total_train_step+<span class="hljs-number">1</span><br><br>    <span class="hljs-comment">#保存文件</span><br>    torch.save(Haijun,<span class="hljs-string">&quot;Haijun_&#123;&#125;.pth&quot;</span>.<span class="hljs-built_in">format</span>(i))<br>    <span class="hljs-comment">#torch.save(Haijun.state_dict(),&quot;Haijun_&#123;&#125;&quot;.format(i))  官方推荐的保存方式</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型已保存&quot;</span>)<br><br>writer.close()<br></code></pre></td></tr></table></figure><p>&#x3D;&#x3D;model.py&#x3D;&#x3D;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 搭建神经网络模型</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">haijun</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, output</span>):<br>        output = <span class="hljs-variable language_">self</span>.model(output)<br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    Haijun=haijun()<br>    <span class="hljs-built_in">input</span>=torch.ones((<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>))  <span class="hljs-comment">##64batch_size  3通道 32x32位  全为1的矩阵</span><br>    output=Haijun(<span class="hljs-built_in">input</span>)<br>    <span class="hljs-built_in">print</span>(output.shape)<br></code></pre></td></tr></table></figure><p><img src="train_loss.png" alt="train_loss"></p><p><img src="testL_loss.png" alt="testL_loss"></p><p><img src="test_accuacy.png" alt="test_accuacy"></p><h2 id="23-CUDA加速"><a href="#23-CUDA加速" class="headerlink" title="23.CUDA加速"></a>23.CUDA加速</h2><p>&#x3D;&#x3D;第一种cpu加速方式：1、网络模型     2、数据（输入、标注）    3、损失函数    调用.cuda()&#x3D;&#x3D;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#计算时间代码</span><br><span class="hljs-keyword">import</span> time  <span class="hljs-comment">#记时</span><br><br><span class="hljs-comment">#模型加速</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():  <span class="hljs-comment">#判断cuda()方法是否可用</span><br>    Haijun=Haijun.cuda()  <span class="hljs-comment">#网络模型的cuda方法</span><br>    <br><span class="hljs-comment">#损失函数加速</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    loss_fn=loss_fn.cuda()  <span class="hljs-comment">#损失函数的cuda()方法</span><br>    <br><span class="hljs-comment">#数据加速</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> trian_dataloader:<br>        imgs,targets=data<br>        <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>            imgs=imgs.cuda()  <span class="hljs-comment">#数据的cuda()方法</span><br>            targets=targets.cuda()<br></code></pre></td></tr></table></figure><p>时间对比：</p><p><img src="%E8%BE%93%E5%87%BA5.png" alt="输出5"></p><p><img src="%E8%BE%93%E5%87%BA6.png" alt="输出6"></p><p>利用Goole colab云GPU加速</p><p><a href="https://colab.research.google.com/">https://colab.research.google.com/</a></p><p><code>!nvidia-smi</code></p><p><img src="%E8%BE%93%E5%87%BA7.png" alt="输出7"></p><p><img src="CUDA.png" alt="CUDA"></p><p>&#x3D;&#x3D;第二种cpu加速方式：1、网络模型     2、数据（输入、标注）    3、损失函数&#x3D;&#x3D;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#调用CPU</span><br>device=torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)  <span class="hljs-comment">#定义训练的设备</span><br>device=torch.device(<span class="hljs-string">&quot;cuda&quot;</span><span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span><span class="hljs-string">&quot;cpu&quot;</span>)  <span class="hljs-comment">#有cuda用cuda  无cuda用cpu</span><br><br>device=torch.device(<span class="hljs-string">&quot;cuda&quot;</span>)<br>       torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span>)  <span class="hljs-comment">#指定第一张显卡</span><br>       torch.device(<span class="hljs-string">&quot;cuda:1&quot;</span>)  <span class="hljs-comment">#指定第二张显卡</span><br><br>Haijun.to(device)<br>loss_fn.to(device)<br><span class="hljs-comment">#针对非数据类型 上述写法即可不需要赋值操作  下面的写法也正确</span><br><span class="hljs-comment">#Haijun=Haijun.to(device)</span><br><span class="hljs-comment">#loss_fn=loss_fn.to(device)  </span><br><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> trian_dataloader:<br>        imgs,targets=data<br>        imgs=imgs.to(device)  <span class="hljs-comment">#数据的cuda()方法</span><br>        targets=targets.to(device)<br></code></pre></td></tr></table></figure><h2 id="24-完整的模型验证demo"><a href="#24-完整的模型验证demo" class="headerlink" title="24.完整的模型验证demo"></a>24.完整的模型验证demo</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br>image_path=<span class="hljs-string">&quot;./data/1.png&quot;</span><br>image=Image.<span class="hljs-built_in">open</span>(image_path)<br><span class="hljs-built_in">print</span>(Image)<br><br>image=image.convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)  <span class="hljs-comment">#png多一个透明通道  转换成三通道</span><br><br>transform=torchvision.transforms.Compose([torchvision.transforms.Resize((<span class="hljs-number">32</span>,<span class="hljs-number">32</span>)),<br>                                          torchvision.transforms.ToTensor()])<br><br><br>image=transform(image)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">haijun</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, output</span>):<br>        output = <span class="hljs-variable language_">self</span>.model(output)<br>        <span class="hljs-keyword">return</span> output<br>Haijun=haijun()<br><br>model=torch.load(<span class="hljs-string">&quot;Haijun_9.pth&quot;</span>)<br><span class="hljs-comment">#model=torch.load(&quot;Haijun_9.pth&quot;,map_location=torch.device(&#x27;cpu&#x27;))  #GPU训练的Haijun_9.pth  yaozai</span><br><span class="hljs-built_in">print</span>(model)<br><br>image=torch.reshape(image,(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>))<br>model.<span class="hljs-built_in">eval</span>()<br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    output=model(image)<br><span class="hljs-built_in">print</span>(output)<br><br><span class="hljs-built_in">print</span>(output.argmax(<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Linux/UNIX文件系统</title>
    <link href="/Diffcc/Diffcc.github.io/2025/06/07/20250607/"/>
    <url>/Diffcc/Diffcc.github.io/2025/06/07/20250607/</url>
    
    <content type="html"><![CDATA[<p><img src="linux.jpg" alt="linux"></p><p>文件系统给是对文件和目录的组织集合，</p><p>本文探究<strong>Linux</strong>下的设备文件系统，ext系列系统，日志文件系统设计</p><span id="more"></span><h2 id="设备文件系统"><a href="#设备文件系统" class="headerlink" title="设备文件系统"></a>设备文件系统</h2><h3 id="设备文件"><a href="#设备文件" class="headerlink" title="设备文件"></a>设备文件</h3><p>在Linux内核中，每种设备类型（真正存在or虚拟抽象）的都有与之对应的设备驱动程序，内核通过<strong>设备驱动程序</strong>的API接口，实现对于设备的操控，虽然每个设备都有差异，但是接口类似，进而能够很好的承接系统调用本身。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">/dev <span class="hljs-comment">#dev目录下设备文件</span><br><br><span class="hljs-comment">#超级用户使用mknod来创建设备文件</span><br></code></pre></td></tr></table></figure><p><img src="dev.png" alt="dev"></p><p>输入输出设备可分为两大类：<strong>块设备（<em>Block Device</em>）<strong>和</strong>字符设备（<em>Character Device</em>）</strong></p><ul><li>字符型设备基于每个字符来处理数据。终端、键盘、鼠标都属于字符型设备。</li><li>块设备每次处理一块设备。块的大小取决于设备类型，硬盘，USB都属于块设备。</li></ul><h3 id="磁盘设备"><a href="#磁盘设备" class="headerlink" title="磁盘设备"></a>磁盘设备</h3><p>磁盘由盘面，磁道，柱面和扇区构成，数据的读&#x2F;写按柱面进行，而不按盘面进行。也就是说，一个磁道写满数据后，就在同一柱面的下一个盘面来写，<em><strong>*一个柱面写满后，才移到下一个扇区开始写数据*</strong></em>。读数据也按照这种方式进行，这样就提高了硬盘的读&#x2F;写效率。</p><p><img src="disk.png" alt="disk"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">fdisk -l <span class="hljs-comment">#列出所有磁盘的分区</span><br></code></pre></td></tr></table></figure><p>磁盘分区可以容纳任何类型的信息，但通常只包含<strong>下面之一</strong>：</p><ul><li>文件系统：存放常规文件。</li><li>数据区域： 充当裸设备。</li><li>交换区域：内存管理之用 特权进程通过***swapon()<em><strong>和</strong></em>swapoff()***启动和停止磁盘分区操作。</li></ul><p>详情可参考：<a href="https://blog.csdn.net/hguisu/article/details/7408047">https://blog.csdn.net/hguisu/article/details/7408047</a></p><h3 id="ext系列文件系统"><a href="#ext系列文件系统" class="headerlink" title="ext系列文件系统"></a>ext系列文件系统</h3><p>**ext2（second extended file system）**扩展文件系统二世是Linux上使用最广泛的文件系统</p><p>ext2文件系统以<em><strong>block</strong></em>为基本单位，包括引导块，超级块，i节点表和数据块部分：</p><ul><li>引导块可以理解为Linux系统的<strong>init()</strong>，用来引导操作系统的信息。</li><li><strong>超级块</strong>则是标记了i节点表的大小，逻辑块的大小信息。</li><li><em><strong>i-list</strong></em>维护了文件类型、属主、硬链接数、<strong>指向文件数据块</strong>的指针等信息，在ext2中，每个i节点包括了15个指针，前12个指针用于直接索引文件数据库的位置，保证直接访问一击必中，后四个文件块通过指向一个<strong>指针块</strong>，分化为指向更多的数据块，以保证容纳大体量的文件，同时可以将未指向数据块的指针块中的指针标记未0，则无需未<strong>文件黑洞</strong>分配空字节数据块。</li><li><strong>数据块</strong>构成了文件和目录，用于存放数据。</li></ul><p><img src="inode.png" alt="inode"></p><p><img src="ext2.png" alt="ext2"></p><p><strong>ext3</strong>文件系统允许<code>journaling</code>日志，<code>journaling</code>日志是在文件系单独的区域存储，每当文件系统意外崩溃，采用<code>journaling</code>日志可以进行恢复，<strong>和ext2文件系统不同的是多出了<code>journaling</code>日志的功能</strong>。</p><ul><li>ext3提供三种journal日志模式，分别是<code>writeback</code>、<code>ordered</code>、<code>journal</code>.<code>writeback</code>仅仅会记录元数据的日志，数据可以直接写到磁盘，但是不保证数据比元数据先落盘，这也是性能最好的，但是<a href="https://cloud.tencent.com/product/dsgc?from_column=20065&from=20065">数据安全</a>性最低；<code>ordered</code>也是仅仅是记录元数据块的日志，这种模式是将文件的数据相关的元数据和数据在一个事务中，当元数据写入到磁盘时候，把对应的数据也写到磁盘，这样是先数据刷盘，再做元数据日志。<code>journal</code>提供数据和元数据的日志，所有的文件数据都先写到日志，然后在写到磁盘，数据需要写2次，性能是最差的。</li><li>ext2中在目录项中查找文件时间的复杂度是<code>O(n)</code>，ext3中采用了<code>h-trees</code>查找效率提高了很多</li></ul><p><img src="ext3.png" alt="ext3"></p><p><strong>ext4</strong>是从ext3 fork而来,针对ext4最大的feature就是ext4采用<code>extents</code>机制，替代了<code>indirect block</code>寻址的方式。ext4尽量会把<a href="https://cloud.tencent.com/product/cos?from_column=20065&from=20065">数据存储</a>在连续的block区域内，为了实现这个ext4需要知道三个信息，第一是文件的初始化block.其次是块的数量，最后是磁盘上初始化块的数据，这些信息统一抽象以<code>struct ext4_extent</code>呈现。</p><p><img src="ext4.png" alt="ext4"></p><p>详情可参考：<a href="https://cloud.tencent.com/developer/article/2074590">https://cloud.tencent.com/developer/article/2074590</a></p><h3 id="虚拟文件系统"><a href="#虚拟文件系统" class="headerlink" title="虚拟文件系统"></a>虚拟文件系统</h3><p>除了ext系列文件系统，还有诸如Reiserfs，VFAT，NFS等文件系统，为了能和各种文件系统打交道，应用程序通过<strong>虚拟文件系统（VFS</strong>）这一层抽象层中定义的通用接口（<em><strong>open()、read()、write()、lseek()</strong></em>….），来实现对不同文件系统的访问。</p><p>不同的文件系统对比可参考：<a href="https://zhuanlan.zhihu.com/p/689551298">https://zhuanlan.zhihu.com/p/689551298</a></p><p><img src="%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94.png" alt="文件系统性能对比"></p><h3 id="日志文件系统"><a href="#日志文件系统" class="headerlink" title="日志文件系统"></a>日志文件系统</h3><p><strong>日志文件系统</strong>的作用在于避免了系统崩溃之后，为了确保文件系统的完整，需要遍历整个文件系统，来检查一致性（<em><strong>fcsk</strong></em>）。</p><p>日志文件系统会将更新操作记录在专门的磁盘日志文件中，可利用日志重做（<em><strong>redo</strong></em>）任何不完整更新（日志系统总能够保证将<strong>文件元数据事务</strong>作为一个完整单元提交至磁盘）。</p><h3 id="文件挂载"><a href="#文件挂载" class="headerlink" title="文件挂载"></a>文件挂载</h3><p>文件挂载（Mount）的作用是将存储设备（如硬盘分区、光盘、网络存储等）或虚拟文件系统（如<code>proc</code>、<code>sysfs</code>等）关联到目录树的某个位置（挂载点），使得用户和程序可以<strong>通过文件系统接口访问这些资源</strong>。</p><p><strong>Linux&#x2F;UNIX</strong>所有的文件系统中的文件都位于单根目录树下（<em><strong>”&#x2F;“</strong></em>），其他的文件系统都挂载在根目录下，被视为整个目录层级的子树（<em><strong>subtree</strong></em>）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 将device文件系统挂载在directory所指定的目录下</span><br>mount device directory<br><br><span class="hljs-comment"># mount用于列出当前已挂载的文件系统，unmount用于卸载</span><br>mount<br></code></pre></td></tr></table></figure><p><img src="mount.png" alt="mount"></p><p>用于查看当前已挂载和可挂载的文件信息：<code>/proc/mounts</code>，<code>/etc/mtab</code>，<code>/etc/fstab</code></p><p><img src="proc_mounts.png" alt="proc_mounts"></p><ul><li><strong>文件系统类型</strong>: <code>sysfs</code>（虚拟文件系统，用于导出内核对象信息）</li><li><strong>挂载点</strong>: <code>/sys</code></li><li><strong>文件系统类型</strong>: <code>sysfs</code></li><li><strong>挂载标志</strong>:<ul><li><code>rw</code>: 以读写方式挂载</li><li><code>nosuid</code>: 忽略文件系统中的SUID&#x2F;SGID权限位（禁用setuid&#x2F;setgid效果）</li><li><code>nodev</code>: 不允许解释文件系统上的设备文件（禁用设备文件）</li><li><code>noexec</code>: 禁止直接执行该文件系统上的程序</li><li><code>relatime</code>: 仅在访问时间早于修改时间时更新访问时间（优化性能）</li></ul></li><li><strong>数字字段</strong>:<ul><li><code>0</code>: dump备份标志（0表示不备份）</li><li><code>0</code>: fsck检查顺序（0表示不检查）</li></ul></li></ul><h4 id="高级挂载特性"><a href="#高级挂载特性" class="headerlink" title="高级挂载特性"></a>高级挂载特性</h4><p><strong>多个挂载点挂在文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">mount /dev/sda12 /test1<br>mount /dev/sda12 /test2<br><br><span class="hljs-comment">#在test1挂载点下操作，test2下完全可见</span><br></code></pre></td></tr></table></figure><p><strong>多次挂载同一挂载点</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">每次新挂载会隐藏之前可见于挂载点下的目录子树<br>mount /dev/sda12 /testfs<br>mount /dev/sda12 /testfs<br><br><span class="hljs-comment">#卸载最后一次挂载，上次挂载内容可见</span><br></code></pre></td></tr></table></figure><p><strong>基于每次挂载的挂载标志</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#mountflag可以基于每次挂载来设置</span><br></code></pre></td></tr></table></figure><p><strong>绑定挂载</strong></p><p><strong>两处同时可见，类似于硬链接，但可以针对目录</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">MS_BIND<br>mount --<span class="hljs-built_in">bind</span><br></code></pre></td></tr></table></figure><p><strong>递归绑定挂载</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">MS_BIND OR MS_REC<br></code></pre></td></tr></table></figure><h3 id="虚拟内存文件系统-tmpfs"><a href="#虚拟内存文件系统-tmpfs" class="headerlink" title="虚拟内存文件系统: tmpfs"></a>虚拟内存文件系统: tmpfs</h3><p>tmpfs 是 Linux 内核中的一个虚拟文件系统，它将数据<strong>存储在内存中</strong>而不是硬盘上。使用 tmpfs 可以快速访问数据，因为数据存储在 <em><strong>RAM</strong></em> 中，而不需要进行实际的磁盘 <em><strong>I&#x2F;O</strong></em> 操作。</p><p>可使用如下命令挂载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Bash">mount -t tmpfs <span class="hljs-built_in">source</span> target <span class="hljs-comment">#source:名称 target:挂载点</span><br></code></pre></td></tr></table></figure><p>除了用于用户应用程序之外，tmpfs文件系统还有下面两个用途:</p><ul><li>由内核内部挂载隐形tmpfs文件系统，用于实现<em><strong>System V</strong></em>共享内存和共享匿名内存映射</li><li>挂在于***&#x2F;dev&#x2F;shm*** 的tmpfs文件系统 ，为<em><strong>glibc</strong></em>用以实现POSIX共享内存和POSIX信号量</li></ul><p>系统可通过<code>statvfs()</code> 和 <code>fstatvfs()</code>库函数获取已挂载文件系统的有关信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#include &lt;sys/statvfs.h&gt;</span><br><br>int statvfs(const char*pathname, struct statvfs* statvfsbuf);<br>int fstatvfs(int fd, struct statvfs* statvfsbuf);<br><br><span class="hljs-comment">#statvfsbuf为指向statvfs结构体的缓冲区，内含文件系统的信息</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Linux系统编程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
